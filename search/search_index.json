{"config":{"lang":["en"],"separator":"[\\s\\-\\_]","pipeline":["stopWordFilter"],"fields":{"title":{"boost":1000.0},"text":{"boost":1.0},"tags":{"boost":1000000.0}}},"docs":[{"location":"","title":"Home","text":"<p>With the growing number of containerized Network Operating Systems grows the demand to easily run them in the user-defined, versatile lab topologies.</p> <p>Unfortunately, container orchestration tools like docker-compose are not a good fit for that purpose, as they do not allow a user to easily create connections between the containers which define a topology.</p> <p>Containerlab provides a CLI for orchestrating and managing container-based networking labs. It starts the containers, builds a virtual wiring between them to create lab topologies of users choice and manages labs lifecycle.</p> <p>Containerlab focuses on the containerized Network Operating Systems which are typically used to test network features and designs, such as:</p> <ul> <li>Nokia SR Linux</li> <li>Arista cEOS</li> <li>Cisco XRd</li> <li>Azure SONiC</li> <li>Juniper cRPD</li> <li>Cumulus VX</li> <li>Keysight IXIA-C</li> </ul> <p>In addition to native containerized NOSes, containerlab can launch traditional virtual machine based routers using vrnetlab or boxen integration:</p> <ul> <li>Nokia virtual SR OS (vSim/VSR)</li> <li>Juniper vMX</li> <li>Juniper vQFX</li> <li>Cisco IOS XRv9k</li> <li>Cisco Nexus 9000v</li> <li>Dell FTOS10v</li> <li>Cisco CSR 1000v</li> <li>Arista vEOS</li> <li>Palo Alto PAN</li> <li>IPInfusion OcNOS</li> <li>Check Point Cloudguard</li> </ul> <p>And, of course, containerlab is perfectly capable of wiring up arbitrary linux containers which can host your network applications, virtual functions or simply be a test client. With all that, containerlab provides a single IaaC interface to manage labs which can span all the needed variants of nodes:</p> <p>This short clip briefly demonstrates containerlab features and explains its purpose:</p>"},{"location":"#features","title":"Features","text":"<ul> <li>IaaC approach     Declarative way of defining the labs by means of the topology definition <code>clab</code> files.</li> <li>Network Operating Systems centric     Focus on containerized Network Operating Systems. The sophisticated startup requirements of various NOS containers are abstracted with kinds which allows the user to focus on the use cases, rather than infrastructure hurdles.</li> <li>VM based nodes friendly     With the vrnetlab integration it is possible to get the best of two worlds - running virtualized and containerized nodes alike with the same IaaC approach and workflows.</li> <li>Multi-vendor and open     Although being kick-started by Nokia engineers, containerlab doesn't take sides and supports NOSes from other vendors and opensource projects.</li> <li>Lab orchestration     Starting the containers and interconnecting them alone is already good, but containerlab packages even more features like managing lab lifecycle: deploy, destroy, save, inspect, graph operations.</li> <li>Scaled labs generator     With <code>generate</code> capabilities of containerlab it possible to define/launch CLOS-based topologies of arbitrary scale. Just say how many tiers you need and how big each tier is, the rest will be done in a split second.</li> <li>Simplicity and convenience     Starting from frictionless installation and upgrade capabilities and ranging to the behind-the-scenes link wiring machinery, containerlab does its best for you to enjoy the tool.</li> <li>Fast     Blazing fast way to create container based labs on any Linux system with Docker.</li> <li>Automated TLS certificates provisioning     The nodes which require TLS certs will get them automatically on boot.</li> <li>Documentation is a first-class citizen     We do not let our users guess by making a complete, concise and clean documentation.</li> <li>Lab catalog    The \"most-wanted\" lab topologies are documented and included with containerlab installation. Based on this cherry-picked selection you can start crafting the labs answering your needs.</li> </ul>"},{"location":"#use-cases","title":"Use cases","text":"<ul> <li>Labs and demos     Containerlab was meant to be a tool for provisioning networking labs built with containers. It is free, open and ubiquitous. No software apart from Docker is required!     As with any lab environment it allows the users to validate features, topologies, perform interop testing, datapath testing, etc.     It is also a perfect companion for your next demo. Deploy the lab fast, with all its configuration stored as a code -&gt; destroy when done. Easily and securely share lab access if needed.</li> <li>Testing and CI     Because of the containerlab's single-binary packaging and code-based lab definition files, it was never that easy to spin up a test bed for CI. Gitlab CI, Github Actions and virtually any CI system will be able to spin up containerlab topologies in a single simple command.</li> <li>Telemetry validation     Coupling modern telemetry stacks with containerlab labs make a perfect fit for Telemetry use cases validation. Spin up a lab with containerized network functions with a telemetry on the side, and run comprehensive telemetry use cases.</li> </ul>"},{"location":"#join-us","title":"Join us","text":"<p>Have questions, ideas, bug reports or just want to chat? Come join our discord server.</p>"},{"location":"community/","title":"Community","text":"<p>Containerlab openness and focus on multivendor labs were key to its success and adoption. With more than a dozen Network Operating Systems spread across several networking vendors and opensource teams, it is a tool that can answer the needs of a broad network engineers community.</p>"},{"location":"community/#chat-servers","title":"Chat servers","text":""},{"location":"community/#discord","title":"Discord","text":"<p>Growing the number of supported NOSes is a task that can't be done by a single person, and there the community role is adamant. To support and cherish the growing containerlab community and provide better feedback and discussions platform, we launched containerlab's official Discord server where maintainers and users hang out.</p> <p>Everybody is welcome to join and chat with our community members about all things containerlab!</p> <p> Join Containerlab Discord Server</p>"},{"location":"community/#irc","title":"IRC","text":"<p>For those who cherish IRC in their hearts, a community-led IRC channel <code>#containerlab</code> is available at IRC Libera server.</p>"},{"location":"community/#in-the-media","title":"In The Media","text":"<p>We are always happy to showcase containerlab and demonstrate its powers. Luckily, the network engineering community has lots of events worldwide, and we participated in some. Below you will find recordings of containerlab talks in different formats and on various venues listed in reverse chronological order<sup>1</sup>.</p>"},{"location":"community/#telco-podcast","title":"Telco podcast","text":"<p> Networking Topology As Code \u00b7  2022-04-21</p> <p>A year in the making; containerlab made lots of waves in the community threading its way to engineers hearts through simple and lightweight abstractions tailored to networking labs problem space. In this podcast Maciej and Anton from The Telco podcast interviewed Roman Dodin on containerlab' current state and many more.</p> <p>Participants:  Roman Dodin</p>"},{"location":"community/#packet-pushers-tech-bytes","title":"Packet Pushers Tech Bytes","text":"<p> Containerlab Makes Container And VM Networking Labs Easy \u00b7  2021-11-15</p> <p>A short, 14 minutes long introductory talk about Containerlab. If you wanted to know what containerlab is, but all you have is 15 minutes break - go check it out.</p> <p>Participants:  Roman Dodin</p>"},{"location":"community/#nanog-83","title":"NANOG 83","text":"<p> Containerlab - running networking labs with Docker UX \u00b7  2021-11-03</p> <p>Our very first NANOG appearance and we went full-steam there. This talk is the most comprehensive containerlab tutorial captured to that date. It starts with the basics and escalates to the advanced DC fabric deployment with HA telemetry cluster created. All driven by a single containerlab topology file.</p> <p>Participants:</p> <ul> <li> Roman Dodin</li> <li> Karim Radhouani</li> </ul>"},{"location":"community/#open-networking-edge-summit-2021","title":"Open Networking &amp; Edge Summit 2021","text":"<p> Containerlab - a Modern way to Deploy Networking Topologies for Labs, CI, and Testing \u00b7  2021-10-11</p> <p>This 30mins screencast introduces containerlab by going through a multivendor lab example consisting of Nokia SR Linux, Arista cEOS, and GoBGP containers participating in a route reflection scenario.</p> <p>The talk starts with the reasoning as to why containerlab development was warranted and what features of container-based labs we wanted to have. Then we start building the lab step by step, explaining how the containerlab topology file is structured.</p> <p>Participants:  Roman Dodin</p>"},{"location":"community/#nlnog-2021","title":"NLNOG 2021","text":"<p> Running networking labs with Docker User Experience \u00b7  2021-09-05</p> <p>The first public talk around containerlab happened in the Netherlands at an in-person (sic!) networking event NLNOG 2021.</p> <p>Participants:  Roman Dodin</p>"},{"location":"community/#modem-podcast-s01e10","title":"Modem Podcast s01e10","text":"<p> Containerlab: Declarative network labbing \u00b7  2021-06-06</p> <p>Building large-scale network labs can be tedious and error-prone \u2014 More importantly, they can be notoriously hard to spin up automatically. Containerlab is a new tool that promises to \u201credefine the way you run networking labs\u201d, and I really think it hits that target. On this episode of the Modulate Demodulate podcast, Nick and Chris C. are joined by Roman Dodin, one of the brains behind Containerlab.</p> <p>Participants:  Roman Dodin</p>"},{"location":"community/#blogs","title":"Blogs","text":"<p>The power of the community is in its members. We are delighted to have containerlab users who share their experience working with the tool, unveiling new use cases, and providing a personal touch to the workflows.</p> <p>This section logs the most notable blogs, streams, and and demos delivered by containerlab users worldwide.</p>"},{"location":"community/#cilium-bgp-tested-with-containerlab","title":"Cilium BGP tested with Containerlab","text":"<p> screencast by Nico Vibert \u00b7  2022-07-20</p> <p>In this video, Senior Technical Marketing Engineer Nico Vibert walks through BGP enhancements in Cilium 1.12, with the integration with GoBGP. Nico uses a spicey mix of kind and containerlab to stand up a lab that simulates a datacenter POD with Cilium-powered k8s nodes talking TOR switches and interconnected via the core layer.</p> <p></p> <p>Clever use of containerlab with a focus on the workload capabilities and integration with kind clusters.</p>"},{"location":"community/#containerlab-with-inmanta","title":"Containerlab with Inmanta","text":"<p> screencast by Inmanta \u00b7  2022-03-20</p> <p>Inmanta is a network orchestration and automation platform. In these YouTube series, Inmanta's engineer explains how this platform can help you manage network infrastructure provided by Containerlab. Make sure to check subsequent videos in their channel where multiple angles of containerlab are shown.</p>"},{"location":"community/#containerlab-based-ddos-testbed","title":"Containerlab-based DDOS testbed","text":"<p> Blog by Peter Phaal \u00b7  2022-03-16</p> <p>Real-time telemetry from a 5 stage Clos fabric describes lightweight emulation of realistic data center switch topologies using Containerlab. This article extends the testbed to experiment with distributed denial of service (DDoS) detection and mitigation techniques described in Real-time DDoS mitigation using BGP RTBH and FlowSpec.</p>"},{"location":"community/#multi-vendor-evpn-vxlan-setup-with-containerlab","title":"Multi-vendor EVPN VXLAN setup with Containerlab","text":"<p> Blog by @aninchat \u00b7  2022-03-12</p> <p>In this post, we deploy a multivendor EVPN L2 overlay fabric, with BGP in the underlay as well. The entire fabric deployment is automated with Ansible, and Containerlab is used to define and deploy the actual topology.</p>"},{"location":"community/#network-simulation-tools-and-containerlab","title":"Network Simulation Tools and Containerlab","text":"<p> Blog by @JulioPDX \u00b7  2022-02-13</p> <p>I\u2019ve been progressing through a series of technical books, some of which I\u2019ve shared on other blogs. A few of them focus on BGP. BGP being so broad, I decided to create a challenge lab. Creating the challenge/troubleshooting labs has really made more concepts stick. I\u2019m trying to use the principle of teaching someone to make the learning last. These have been incredibly fun to create and the community interaction has been amazing. One brave soul(Jeroen van Bemmel) shared his solution. I was fascinated on his solution and how he created his topology with Containerlab and net-sim tools.</p>"},{"location":"community/#juniper-vqfx-and-containerlab","title":"Juniper vQFX and containerlab","text":"<p> Blog by @aninchat \u00b7  2022-02-06</p> <p>In this post, we look at how Containerlab can be used to quickly spin up vQFX topologies for network validation and testing. We\u2019ll walk through the entire process - how to build docker images from vQFX images, what happens behind the scenes when bringing these containers up and how to build/verify your topology.</p>"},{"location":"community/#multipoint-redistribution-and-sr-linux","title":"Multipoint Redistribution and SR Linux","text":"<p> Blog by @JulioPDX \u00b7  2022-01-22</p> <p>I\u2019ve been working my way through Optimal Routing Design by Russ White, Don Slice, and Alvaro Retana. It has been a great read so far and I highly recommend it. When I work through a book I usually try and lab up any concept I can, this helps me make it stick as much as possible. Early on in the book the authors mention redistribution and possible issues that can come with doing it at more than one point in the network. So here we are reading this post. I hope you enjoy and possibly learn something along the way.</p> <p>Discussions: </p>"},{"location":"community/#real-time-telemetry-from-a-5-stage-clos-fabric","title":"Real-time telemetry from a 5 stage Clos fabric","text":"<p> Blog by Peter Phaal \u00b7  2022-02-21</p> <p>As Peter wrote, sFlow is a companion to the Streaming Telemetry applications. It's UDP based transport claims to be more suitable for situations where networks are congested. In this blog Peter explains how containerlab can deploy a Clos topolgy with sFlow collector and endpoints generating traffic to see the whole machinery in action.</p> <p>Discussions: </p>"},{"location":"community/#network-modeling-automating-mikrotik-routeros-chr-containerlab-images","title":"Network Modeling: Automating Mikrotik RouterOS CHR Containerlab images","text":"<p> Blog by @nlgotz \u00b7  2021-12-20</p> <p>A blogpost showing how to build containerlab images for the Mikrotik CHR, and how you can avoid having to build them yourself using Docker.</p> <p>Discussions: </p>"},{"location":"community/#my-journey-and-experience-with-containerlab","title":"My Journey and Experience with Containerlab","text":"<p> Blog by @JulioPDX \u00b7  2021-12-10</p> <p>In this blog Julio took containerlab for a spin and shares his experience with it. His lab consists of a few Arista cEOS nodes which he then provisions with Nornir, using Ansible inventory generated by containerlab.</p> <p>Discussions:  \u00b7 </p>"},{"location":"community/#network-modeling-segmented-lab-access-with-containerlab-and-zerotier","title":"Network Modeling: Segmented Lab access with Containerlab and ZeroTier","text":"<p> Blog by @nlgotz \u00b7  2021-11-23</p> <p>When building out network labs, often multiple people will need access to the lab. The main way right now is to use something like EVE-NG or GNS3 to provide access.</p> <p>There are 2 downsides to this method. The first is that your server is exposed to the internet and if your usernames/passwords aren\u2019t strong enough, your server can become compromised. The second is that sometimes you may not want everyone to be able to add or edit to the lab topology.</p> <p>The solution to this is using Containerlab and ZeroTier. This setup is great for things like testing new hires, training classes, or for providing lab access to others on a limited basis.</p> <p>Discussions: </p>"},{"location":"community/#building-your-own-data-center-fabric-with-containerlab","title":"Building Your Own Data Center Fabric with Containerlab","text":"<p> Blog and a screencast by Alperen Akpinar \u00b7  2021-08-24</p> <p>Alperen did a great job explaining how to build a DC fabric topology using containerlab. This was the first post in the series, making it an excellent intro to containerlab, especially when you follow the screencast and watch the topology buildup live.</p> <p>In a subsequent post, Alperen explains how to configure the SR Linux fabric he just built.</p>"},{"location":"community/#how-to-consistently-run-a-temporary-vm-on-aws-to-run-containerlab","title":"How to consistently run a temporary VM on AWS to run Containerlab","text":"<p> Blog by @_nleiva \u00b7  2021-07-12</p> <p>Create and then destroy a cloud environment ready to Containerlab with pre-loaded topology files, which just work if you use FRR and SR Linux. If you need to run Arista's cEOS, see: Getting cEOS image.</p> <p>This post describes the benefits of running any temporary workload in the cloud to then focus on Containerlab, so skip to the \u201cNetwork testing challenges\u201d section if you want to get to the meat of it. More details on the labs included at Network Labs.</p> <ol> <li> <p>most recent talks appear first.\u00a0\u21a9</p> </li> </ol>"},{"location":"install/","title":"Installation","text":"<p>Containerlab is distributed as a Linux deb/rpm package and can be installed on any Debian- or RHEL-like distributive in a matter of a few seconds.</p>"},{"location":"install/#pre-requisites","title":"Pre-requisites","text":"<p>The following requirements must be satisfied to let containerlab tool run successfully:</p> <ul> <li>A user should have <code>sudo</code> privileges to run containerlab.</li> <li>A Linux server/VM<sup>2</sup> and Docker installed.</li> <li>Load container images (e.g. Nokia SR Linux, Arista cEOS) that are not downloadable from a container registry. Containerlab will try to pull images at runtime if they do not exist locally.</li> </ul>"},{"location":"install/#install-script","title":"Install script","text":"<p>Containerlab can be installed using the installation script which detects the operating system type and installs the relevant package:</p> <p>Note</p> <p>Containerlab is distributed via deb/rpm packages, thus only Debian- and RHEL-like distributives can leverage package installation. Other systems can follow the manual installation procedure.</p> <pre><code># download and install the latest release (may require sudo)\nbash -c \"$(curl -sL https://get.containerlab.dev)\"\n# download a specific version - 0.10.3 (may require sudo)\nbash -c \"$(curl -sL https://get.containerlab.dev)\" -- -v 0.10.3\n\n# with wget\nbash -c \"$(wget -qO - https://get.containerlab.dev)\"\n</code></pre>"},{"location":"install/#package-managers","title":"Package managers","text":"<p>It is possible to install official containerlab releases via public APT/YUM repository.</p> APTYUMAPKAUR <pre><code>echo \"deb [trusted=yes] https://apt.fury.io/netdevops/ /\" | \\\nsudo tee -a /etc/apt/sources.list.d/netdevops.list\n\napt update &amp;&amp; apt install containerlab\n</code></pre> <pre><code>yum-config-manager --add-repo=https://yum.fury.io/netdevops/ &amp;&amp; \\\necho \"gpgcheck=0\" | sudo tee -a /etc/yum.repos.d/yum.fury.io_netdevops_.repo\n\nyum install containerlab\n</code></pre> <p>Download <code>.apk</code> package from Github releases.</p> <p>Arch Linux users can download a package from this AUR repository.</p> Manual package installation <p>Alternatively, users can manually download the deb/rpm package from the Github releases page.</p> <p>example: <pre><code># manually install latest release with package managers\nLATEST=$(curl -s https://github.com/srl-labs/containerlab/releases/latest | sed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/')\n# with yum\nyum install \"https://github.com/srl-labs/containerlab/releases/download/v${LATEST}/containerlab_${LATEST}_linux_amd64.rpm\"\n# with dpkg\ncurl -sL -o /tmp/clab.deb \"https://github.com/srl-labs/containerlab/releases/download/v${LATEST}/containerlab_${LATEST}_linux_amd64.deb\" &amp;&amp; dpkg -i /tmp/clab.deb\n\n# install specific release with yum\nyum install https://github.com/srl-labs/containerlab/releases/download/v0.7.0/containerlab_0.7.0_linux_386.rpm\n</code></pre></p> <p>The package installer will put the <code>containerlab</code> binary in the <code>/usr/bin</code> directory as well as create the <code>/usr/bin/clab -&gt; /usr/bin/containerlab</code> symlink. The symlink allows the users to save on typing when they use containerlab: <code>clab &lt;command&gt;</code>.</p>"},{"location":"install/#container","title":"Container","text":"<p>Containerlab is also available in a container packaging. The latest containerlab release can be pulled with:</p> <pre><code>docker pull ghcr.io/srl-labs/clab\n</code></pre> <p>To pick any of the released versions starting from release 0.19.0, use the version number as a tag, for example, <code>docker pull ghcr.io/srl-labs/clab:0.19.0</code></p> <p>Since containerlab itself deploys containers and creates veth pairs, its run instructions are a bit more complex, but still, it is a copy-paste-able command.</p> <p>For example, if your lab files are contained within the current working directory - <code>$(pwd)</code> - then you can launch containerlab container as follows:</p> <pre><code>docker run --rm -it --privileged \\\n--network host \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /var/run/netns:/var/run/netns \\\n-v /etc/hosts:/etc/hosts \\\n-v /var/lib/docker/containers:/var/lib/docker/containers \\\n--pid=\"host\" \\\n-v $(pwd):$(pwd) \\\n-w $(pwd) \\\nghcr.io/srl-labs/clab bash\n</code></pre> <p>Within the started container you can use the same <code>containerlab deploy/destroy/inspect</code> commands to manage your labs.</p> <p>Note</p> <p>Containerlab' container command is itself <code>containerlab</code>, so you can deploy a lab without invoking a shell, for example: <pre><code>docker run --rm -it --privileged \\\n# &lt;run options omitted&gt;\n-w $(pwd) \\\nghcr.io/srl-labs/clab deploy -t somelab.clab.yml\n</code></pre></p>"},{"location":"install/#manual-installation","title":"Manual installation","text":"<p>If the linux distributive can't install deb/rpm packages, containerlab can be installed from the archive:</p> <pre><code># get the latest available tag\nLATEST=$(curl -s https://github.com/srl-labs/containerlab/releases/latest | \\\nsed -e 's/.*tag\\/v\\(.*\\)\\\".*/\\1/')\n# download tar.gz archive\ncurl -L -o /tmp/clab.tar.gz \"https://github.com/srl-labs/containerlab/releases/download/v${LATEST}/containerlab_${LATEST}_Linux_amd64.tar.gz\"\n# create containerlab directory\nmkdir -p /etc/containerlab\n\n# extract downloaded archive into the containerlab directory\ntar -zxvf /tmp/clab.tar.gz -C /etc/containerlab\n\n# (optional) move containerlab binary somewhere in the $PATH\nmv /etc/containerlab/containerlab /usr/bin &amp;&amp; chmod a+x /usr/bin/containerlab\n</code></pre>"},{"location":"install/#windows-subsystem-linux-wsl","title":"Windows Subsystem Linux (WSL)","text":"<p>Containerlab runs on WSL, but you need to install docker-ce inside the WSL2 linux system instead of using Docker Desktop<sup>3</sup>.</p> <p>If you are running Ubuntu 20.04 as your WSL2 machine, you can run this script to install docker-ce.</p> <pre><code>curl -L https://gist.githubusercontent.com/hellt/e8095c1719a3ea0051165ff282d2b62a/raw/1dffb71d0495bb2be953c489cd06a25656d974a4/docker-install.sh | \\\nbash\n</code></pre> <p>Once installed, issue <code>sudo service docker start</code> to start the docker service inside WSL2 machine.</p> Running VM-based routers inside WSL <p>At the moment of this writing, KVM support was not available out-of-the box with WSL2 VMs. There are ways to enable KVM support, but they were not tested with containerlab. This means that running traditional VM based routers via vrnetlab integration is not readily available.</p> <p>It appears to be that next versions of WSL2 kernels will support KVM.</p>"},{"location":"install/#mac-os","title":"Mac OS","text":"<p>Running containerlab on Mac OS is possible<sup>4</sup> by means of a separate docker image with containerlab inside.</p> <p>Warning</p> <p>ARM-based Macs (M1/2) are not supported, and no binaries are generated for this platform. This is mainly due to the lack of network images built for arm64 architecture as of now.</p> <p>To use this container use the following command:</p> <pre><code>CLAB_WORKDIR=~/clab\n\ndocker run --rm -it --privileged \\\n--network host \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /run/netns:/run/netns \\\n--pid=\"host\" \\\n-w $CLAB_WORKDIR \\\n-v $CLAB_WORKDIR:$CLAB_WORKDIR \\\nghcr.io/srl-labs/clab bash\n</code></pre> <p>The first command in the snippet above sets the working directory which you intend to use on your Mac OS. The <code>~/clab</code> in the example above expands to <code>/Users/&lt;username&gt;/clab</code> and means that we intent to have our containerlab labs to be stored in this directory.</p> <p>Note</p> <ol> <li>It is best to create a directory under the <code>~/some/path</code> unless you know what to do<sup>5</sup></li> <li>vrnetlab based nodes will not be able to start, since Docker VM does not support virtualization.</li> <li>Docker Desktop for Mac introduced cgroups v2 support in 4.3.0 version; to support the images that require cgroups v1 follow these instructions.</li> <li>Docker Desktop relies on a LinuxKit based HyperKit VM. Unfortunately, it is shipped with a minimalist kernel, and some modules such as VRF are disabled by default. Follow these instructions to rebuild it with more modules. </li> </ol> <p>When the container is started, you will have a bash shell opened with the directory contents mounted from the Mac OS. There you can use <code>containerlab</code> commands right away.</p> Step by step example <p>Let's imagine I want to run a lab with two SR Linux containers running directly on a Mac OS.</p> <p>First, I need to have Docker Desktop for Mac installed and running.</p> <p>Then I will create a directory under the <code>$HOME</code> path on my mac:</p> <pre><code>mkdir -p ~/clab\n</code></pre> <p>Then I will create a clab file defining my lab in the newly created directory:</p> <pre><code>cat &lt;&lt;EOF &gt; ~/clab/2srl.clab.yml\nname: 2srl\ntopology:\n  nodes:\n    srl1:\n      kind: srl\n      image: ghcr.io/nokia/srlinux\n    srl2:\n      kind: srl\n      image: ghcr.io/nokia/srlinux\n  links:\n    - endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\nEOF\n</code></pre> <p>Now when the clab file is there, launch the container and don't forget to use path to the directory you created:</p> <pre><code>CLAB_WORKDIR=~/clab\n\ndocker run --rm -it --privileged \\\n--network host \\\n-v /var/run/docker.sock:/var/run/docker.sock \\\n-v /run/netns:/run/netns \\\n--pid=\"host\" \\\n-w $CLAB_WORKDIR \\\n-v $CLAB_WORKDIR:$CLAB_WORKDIR \\\nghcr.io/srl-labs/clab bash\n</code></pre> <p>Immediately you will get into the directory inside the container with your lab file available:</p> <pre><code>root@docker-desktop:/Users/romandodin/clab# ls\n2srl.clab.yml\n</code></pre> <p>Now you can launch the lab, as containerlab is already part of the image: <pre><code>root@docker-desktop:/Users/romandodin/clab# clab dep -t 2srl.clab.yml\nINFO[0000] Parsing &amp; checking topology file: 2srl.clab.yml \nINFO[0000] Creating lab directory: /Users/romandodin/clab/clab-2srl \nINFO[0000] Creating root CA                             \nINFO[0000] Creating docker network: Name='clab', IPv4Subnet='172.20.20.0/24', IPv6Subnet='2001:172:20:20::/64', MTU='1500' \nINFO[0000] Creating container: srl1                     \nINFO[0000] Creating container: srl2                     \nINFO[0001] Creating virtual wire: srl1:e1-1 &lt;--&gt; srl2:e1-1 \nINFO[0001] Adding containerlab host entries to /etc/hosts file \n+---+----------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| # |      Name      | Container ID |         Image         | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+----------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| 1 | clab-2srl-srl1 | 574bf836fb40 | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.2/24 | 2001:172:20:20::2/64 |\n| 2 | clab-2srl-srl2 | f88531a74ffb | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.3/24 | 2001:172:20:20::3/64 |\n+---+----------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n</code></pre></p>"},{"location":"install/#upgrade","title":"Upgrade","text":"<p>To upgrade <code>containerlab</code> to the latest available version issue the following command<sup>1</sup>:</p> <pre><code>containerlab version upgrade\n</code></pre> <p>This command will fetch the installation script and will upgrade the tool to its most recent version.</p> <p>or leverage <code>apt</code>/<code>yum</code> utilities if containerlab repo was added as explained in the Package managers section.</p>"},{"location":"install/#from-source","title":"From source","text":"<p>To build containerlab from source:</p> with go buildwith goreleaser <p>To build containerlab from source, clone the repository and issue <code>go build</code> at its root.</p> <p>When we release containerlab we use goreleaser project to build binaries for all supported platforms as well as the deb/rpm packages. Users can install <code>goreleaser</code> and do the same locally by issuing the following command: <pre><code>goreleaser --snapshot --skip-publish --rm-dist\n</code></pre></p>"},{"location":"install/#uninstall","title":"Uninstall","text":"<p>To uninstall containerlab when it was installed via installation script or packages:</p> Debian-based systemRPM-based systemsManual removal <pre><code>apt remove containerlab\n</code></pre> <pre><code>yum remove containerlab\n</code></pre> <p>Containerlab binary is located at <code>/usr/bin/containerlab</code>. In addition to the binary, containerlab directory with static files may be found at <code>/etc/containerlab</code>.</p>"},{"location":"install/#selinux","title":"SELinux","text":"<p>When SELinux set to enforced mode containerlab binary might fail to execute with <code>Segmentation fault (core dumped)</code> error. This might be because containerlab binary is compressed with upx and selinux prevents it from being decompressed by default.</p> <p>To fix this:</p> <pre><code>sudo semanage fcontext -a -t textrel_shlib_t $(which containerlab)\nsudo restorecon $(which containerlab)\n</code></pre> <p>or more globally:</p> <pre><code>sudo setsebool -P selinuxuser_execmod 1\n</code></pre> <ol> <li> <p>only available if installed from packages\u00a0\u21a9</p> </li> <li> <p>Most containerized NOS will require &gt;1 vCPU. RAM size depends on the lab size. Architecture: AMD64.\u00a0\u21a9</p> </li> <li> <p>No need to uninstall Docker Desktop, just make sure that it is not integrated with WSL2 machine that you intend to use with containerlab. Moreover, you can make it even work with Docker Desktop with a few additional steps, but installing docker-ce into the WSL maybe more intuitive.\u00a0\u21a9</p> </li> <li> <p>kudos to Michael Kashin who shared this approach with us\u00a0\u21a9</p> </li> <li> <p>otherwise make sure to add a custom shared directory to the docker on mac.\u00a0\u21a9</p> </li> </ol>"},{"location":"quickstart/","title":"Quick start","text":""},{"location":"quickstart/#installation","title":"Installation","text":"<p>Getting containerlab is as easy as it gets. Thanks to the trivial installation procedure it can be set up in a matter of a few seconds on any RHEL or Debian based OS<sup>1</sup>.</p> <pre><code># download and install the latest release (may require sudo)\nbash -c \"$(curl -sL https://get.containerlab.dev)\"\n</code></pre>"},{"location":"quickstart/#topology-definition-file","title":"Topology definition file","text":"<p>Once installed, containerlab manages the labs defined in the so-called topology definition, <code>clab</code> files. A user can write a topology definition file from scratch, or start with looking at various lab examples we provide within the containerlab package.</p> <p>In this quickstart we will be using one of the provided labs which consists of Nokia SR Linux and Arista cEOS nodes connected one to another.</p> <p>The lab topology is defined in the srlceos01.clab.yml file. To make use of this lab example, we first may want to copy the corresponding lab files to some directory:</p> <pre><code># create a directory for the lab\nmkdir ~/clab-quickstart\ncd ~/clab-quickstart\n\n# copy over the lab files\ncp -a /etc/containerlab/lab-examples/srlceos01/* .\n</code></pre> <p>Let's have a look at how this lab's topology is defined:</p> <pre><code>name: srlceos01\ntopology:\nnodes:\nsrl:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nceos:\nkind: ceos\nimage: ceos:4.25.0F\nlinks:\n- endpoints: [\"srl:e1-1\", \"ceos:eth1\"]\n</code></pre> <p>A topology definition deep-dive document provides a complete reference of the topology definition syntax. In this quickstart we keep it short, glancing over the key components of the file:</p> <ul> <li>Each lab has a <code>name</code>.</li> <li>The lab topology is defined under the <code>topology</code> element.</li> <li>Topology is a set of <code>nodes</code> and <code>links</code> between them.</li> <li>The nodes are always of a certain <code>kind</code>. The <code>kind</code> defines the node configuration and behavior.</li> <li>Containerlab supports a fixed number of <code>kinds</code>. In the example above, the <code>srl</code> and <code>ceos</code> are one of the supported kinds.</li> <li>The actual nodes of the topology are defined in the <code>nodes</code> section which holds a map of node names. In the example above, nodes with names <code>srl</code> and <code>ceos</code> are defined.</li> <li>Node elements must have a <code>kind</code> parameter to indicate which kind this node belongs to. Under the nodes section you typically provide node-specific parameters. This lab uses a node-specific parameters - <code>image</code>.  </li> <li><code>nodes</code> are interconnected with <code>links</code>. Each <code>link</code> is defined by a set of <code>endpoints</code>.</li> </ul>"},{"location":"quickstart/#container-image","title":"Container image","text":"<p>One of node's most important properties is the container <code>image</code> they use. In our example the nodes use a specific image which we imported upfront<sup>2</sup>.</p> <p>The image name follows the same rules as the images you use with, for example, Docker client.</p> <p>Container images versions</p> <p>Some lab examples use the images without a tag, i.e. <code>image: srlinux</code>. This means that the image with a <code>latest</code> tag must exist. A user needs to tag the image if the <code>latest</code> tag is missing.</p> <p>For example: <code>docker tag srlinux:20.6.1-286 srlinux:latest</code></p> <p>Images availability</p> <p>Quickstart lab includes Nokia SR Linux and Arista cEOS images. While Nokia SR Linux is a publicly available image and can be pulled by anyone, its counterpart Arista cEOS images needs to be downloaded by the users first.</p> <p>This means that you have to login with Arista website and download the image, then import it to docker image store before proceeding with this lab. Or you can swap the ceos image with another SR Linux image and enjoy the freedom of labbing.</p>"},{"location":"quickstart/#deploying-a-lab","title":"Deploying a lab","text":"<p>Now when we know what a basic topology file consists of and sorted out the container image name and node's license file, we can proceed with deploying this lab. To keep things easy and guessable, the command to deploy a lab is called <code>deploy</code>.</p> <pre><code># checking that topology file is present in ~/clab-quickstart\n\u276f ls\nsrlceos01.clab.yml\n\n# checking that container images are available\ndocker images | grep -E \"srlinux|ceos\"\nREPOSITORY             TAG                 IMAGE ID            CREATED             SIZE\nghcr.io/nokia/srlinux  latest              79019d14cfc7        3 months ago        1.32GB\nceos                   4.25.0F             15a5f97fe8e8        3 months ago        1.76GB\n\n# start the lab deployment\ncontainerlab deploy # (1)!\n</code></pre> <ol> <li><code>deploy</code> command will automatically lookup a file matching the <code>*.clab.y*ml</code> patter to select it.   If you have several files and want to pick a specific one, use <code>--topo &lt;path&gt;</code> flag.</li> </ol> <p>After a couple of seconds you will see the summary of the deployed nodes:</p> <pre><code>+---+---------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| # |        Name         | Container ID |       Image           | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+---------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n| 1 | clab-srlceos01-ceos | 2e2e04a42cea | ceos:4.25.0F          | ceos |       | running | 172.20.20.3/24 | 2001:172:20:20::3/80 |\n| 2 | clab-srlceos01-srl  | 1b9568fcdb01 | ghcr.io/nokia/srlinux | srl  |       | running | 172.20.20.4/24 | 2001:172:20:20::4/80 |\n+---+---------------------+--------------+-----------------------+------+-------+---------+----------------+----------------------+\n</code></pre> <p>The node name presented in the summary table is the fully qualified node name, it is built using the following pattern: <code>clab-{{lab-name}}-{{node-name}}</code>.</p>"},{"location":"quickstart/#connecting-to-the-nodes","title":"Connecting to the nodes","text":"<p>Since the topology nodes are regular containers, you can connect to them just like to any other container.</p> <pre><code>docker exec -it clab-srlceos01-srl1 bash\n</code></pre> <p>Info</p> <p>For each supported kind we document the management interfaces and the ways to leverage them. For example, <code>srl</code> kind documentation provides the commands to leverage SSH and gNMI interfaces. <code>ceos</code> kind has its own instructions.</p> <p>With containerized network OSes like Nokia SR Linux or Arista cEOS SSH access can be achieved by either using the management address assigned to the container:</p> <pre><code>\u276f ssh admin@172.20.20.3\nadmin@172.20.20.3's password:\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--\nA:srl1#\n</code></pre> <p>or by using node's fully qualified names, for which containerlab creates <code>/etc/hosts</code> entries:</p> <pre><code>ssh admin@clab-srlceos01-srl\n</code></pre> <p>The following tab view aggregates the ways to get CLI access per the lab node:</p> Nokia SR LinuxArista cEOS <pre><code># access CLI\ndocker exec -it &lt;name&gt; sr_cli\n# access bash\ndocker exec -it &lt;name&gt; bash\n</code></pre> <pre><code># access CLI\ndocker exec -it &lt;name&gt; Cli\n# access bash\ndocker exec -it &lt;name&gt; bash\n</code></pre>"},{"location":"quickstart/#destroying-a-lab","title":"Destroying a lab","text":"<p>To remove the lab, use the <code>destroy</code> command that takes a topology file as an argument:</p> <pre><code>containerlab destroy --topo srlceos01.clab.yml\n</code></pre>"},{"location":"quickstart/#what-next","title":"What next?","text":"<p>To get a broader view on the containerlab features and components, refer to the User manual section.</p> <p>Do not forget to check out the Lab examples section where we provide complete and ready-to-run topology definition files. This is a great starting point to explore containerlab by doing.</p> <ol> <li> <p>For other installation options such as package managers, manual binary downloads or instructions to get containerlab for non-RHEL/Debian distros, refer to the installation guide.\u00a0\u21a9</p> </li> <li> <p>Containerlab would try to pull the images upon the lab deployment, but if the images are not available publicly or you do not have the private repositories configured, then you need to import the images upfront.\u00a0\u21a9</p> </li> </ol>"},{"location":"cmd/completion/","title":"shell completions","text":""},{"location":"cmd/completion/#description","title":"Description","text":"<p>The <code>completion</code> command generates shell completions for bash/zsh/fish shells.</p>"},{"location":"cmd/completion/#usage","title":"Usage","text":"<p><code>containerlab completion [arg]</code></p>"},{"location":"cmd/completion/#bash-completions","title":"Bash completions","text":"<pre><code>source &lt;(containerlab completion bash)\n</code></pre> <p>To load completions for each session, execute once: <pre><code># linux\n$ containerlab completion bash &gt; /etc/bash_completion.d/containerlab\n\n# macOS\n$ containerlab completion bash &gt; /usr/local/etc/bash_completion.d/containerlab\n</code></pre></p>"},{"location":"cmd/completion/#zsh-completions","title":"ZSH completions","text":"<p>If shell completion is not already enabled in your environment, users will need to enable it. To do so, execute the following once: <pre><code>echo \"autoload -U compinit; compinit\" &gt;&gt; ~/.zshrc\n</code></pre></p> <p>To load completions for each session, execute once: <pre><code>containerlab completion zsh &gt; \"${fpath[1]}/_containerlab\"\n</code></pre></p> <p>Info</p> <p>Note: <code>$fpath[1]</code> in this command refers to the first path in <code>$fpath</code>. Ensure you use the index pointing to the completion folder, find the correct index by inspecting the output of <code>echo $fpath</code></p> <p>Start a new shell for this setup to take effect.</p>"},{"location":"cmd/completion/#fish-completions","title":"Fish completions","text":"<pre><code>containerlab completion fish | source\n</code></pre> <p>To load completions for each session, execute once: <pre><code>containerlab completion fish &gt; ~/.config/fish/completions/containerlab.fish\n</code></pre></p>"},{"location":"cmd/deploy/","title":"deploy command","text":""},{"location":"cmd/deploy/#description","title":"Description","text":"<p>The <code>deploy</code> command spins up a lab using the topology expressed via topology definition file.</p>"},{"location":"cmd/deploy/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] deploy [local-flags]</code></p> <p>aliases: <code>dep</code></p>"},{"location":"cmd/deploy/#flags","title":"Flags","text":""},{"location":"cmd/deploy/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology definition file that will be used to spin up a lab.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory. If a single file is found, it will be used.</p>"},{"location":"cmd/deploy/#name","title":"name","text":"<p>With the global <code>--name | -n</code> flag a user sets a lab name. This value will override the lab name value passed in the topology definition file.</p>"},{"location":"cmd/deploy/#vars","title":"vars","text":"<p>Global <code>--vars</code> option for using specified json or yaml file to load template variables from for generating topology file.</p> <p>Default is to lookup files with \"_vars\" suffix and common json/yaml file extensions next to topology file. For example, for <code>mylab.clab.gotmpl</code> template of topology definition file, variables from <code>mylab.clab_vars.yaml</code> file will be used by default, if it exists, or one with <code>.json</code> or <code>.yml</code> extension.</p> <p>See documentation on Generated topologies for more information and examples on how to use these variables.</p>"},{"location":"cmd/deploy/#reconfigure","title":"reconfigure","text":"<p>The local <code>--reconfigure | -c</code> flag instructs containerlab to first destroy the lab and all its directories and then start the deployment process. That will result in a clean (re)deployment where every configuration artefact will be generated (TLS, node config) from scratch.</p> <p>Without this flag present, containerlab will reuse the available configuration artifacts found in the lab directory.</p> <p>Refer to the configuration artifacts page to get more information on the lab directory contents.</p>"},{"location":"cmd/deploy/#max-workers","title":"max-workers","text":"<p>With <code>--max-workers</code> flag, it is possible to limit the number of concurrent workers that create containers or wire virtual links. By default, the number of workers equals the number of nodes/links to create.</p>"},{"location":"cmd/deploy/#runtime","title":"runtime","text":"<p>Containerlab nodes can be started by different runtimes, with <code>docker</code> being the default one. Besides that, containerlab has experimental support for <code>podman</code>, <code>containerd</code>, and <code>ignite</code> runtimes.</p> <p>A global runtime can be selected with a global <code>--runtime | -r</code> flag that will select a runtime to use. The possible value are:</p> <ul> <li><code>docker</code> - default</li> <li><code>podman</code> - beta support</li> <li><code>containerd</code> - experimental support</li> <li><code>ignite</code></li> </ul>"},{"location":"cmd/deploy/#timeout","title":"timeout","text":"<p>A global <code>--timeout</code> flag drives the timeout of API requests that containerlab send toward external resources. Currently the only external resource is the container runtime (i.e. docker).</p> <p>In a busy compute the runtime may respond longer than anticipated, in that case increasing the timeout may help.</p> <p>The default timeout is set to 2 minutes and can be changed to values like <code>30s, 10m</code>.</p>"},{"location":"cmd/deploy/#export-template","title":"export-template","text":"<p>The local <code>--export-template</code> flag allows a user to specify a custom Go template that will be used for exporting topology data into <code>topology-data.json</code> file under the lab directory. If not set, the default template path is <code>/etc/containerlab/templates/export/auto.tmpl</code>.</p> <p>To export full topology data instead of a subset of fields exported by default, use <code>--export-template /etc/containerlab/templates/export/full.tmpl</code>. Note, some fields exported via <code>full.tmpl</code> might contain sensitive information like TLS private keys. To customize export data, it is recommended to start with a copy of <code>auto.tmpl</code> and change it according to your needs.</p>"},{"location":"cmd/deploy/#log-level","title":"log-level","text":"<p>Global <code>--log-level</code> parameter can be used to configure logging verbosity of all containerlab operations. <code>--debug | -d</code> option is a shorthand for <code>--log-level debug</code> and takes priority over <code>--log-level</code> if specified.</p> <p>Following values are accepted, ordered from most verbose to least: <code>trace</code>, <code>debug</code>, <code>info</code>, <code>warning</code>, <code>error</code>, <code>fatal</code>. Default level is <code>info</code>.</p> <p>It should be useful to enable more verbose logging when something doesn't work as expected, to better understand what's going on, and to provide more useful output logs when reporting containerlab issues, while making it more terse in production environments.</p>"},{"location":"cmd/deploy/#environment-variables","title":"Environment variables","text":""},{"location":"cmd/deploy/#clab_runtime","title":"CLAB_RUNTIME","text":"<p>Default value of \"runtime\" key for nodes, same as global <code>--runtime | -r</code> flag described above. Affects all containerlab commands in the same way, not just <code>deploy</code>.</p> <p>Intended to be set in environments where non-default container runtime should be used, to avoid needing to specify it for every command invocation or in every configuration file.</p> <p>Example command-line usage: <code>CLAB_RUNTIME=podman containerlab deploy</code></p>"},{"location":"cmd/deploy/#clab_version_check","title":"CLAB_VERSION_CHECK","text":"<p>Can be set to \"disable\" value to prevent deploy command making a network request to check new version to report if one is available.</p> <p>Useful when running in an automated environments with restricted network access.</p> <p>Example command-line usage: <code>CLAB_VERSION_CHECK=disable containerlab deploy</code></p>"},{"location":"cmd/deploy/#clab_labdir_base","title":"CLAB_LABDIR_BASE","text":"<p>To change the lab directory location, set <code>CLAB_LABDIR_BASE</code> environment variable accordingly. It denotes the base directory in which the lab directory will be created.</p> <p>The default behavior is to create the lab directory in the current working dir.</p>"},{"location":"cmd/deploy/#examples","title":"Examples","text":""},{"location":"cmd/deploy/#deploy-a-lab-using-the-given-topology-file","title":"Deploy a lab using the given topology file","text":"<pre><code>containerlab deploy -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/deploy/#deploy-a-lab-and-regenerate-all-configuration-artifacts","title":"Deploy a lab and regenerate all configuration artifacts","text":"<pre><code>containerlab deploy -t mylab.clab.yml --reconfigure\n</code></pre>"},{"location":"cmd/deploy/#deploy-a-lab-without-specifying-topology-file","title":"Deploy a lab without specifying topology file","text":"<p>Given that a single topology file is present in the current directory.</p> <pre><code>containerlab deploy\n</code></pre>"},{"location":"cmd/deploy/#deploy-a-lab-using-short-flag-names","title":"Deploy a lab using short flag names","text":"<pre><code>clab dep -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/destroy/","title":"destroy command","text":""},{"location":"cmd/destroy/#description","title":"Description","text":"<p>The <code>destroy</code> command destroys a lab referenced by its topology definition file.</p>"},{"location":"cmd/destroy/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] destroy [local-flags]</code></p> <p>aliases: <code>des</code></p>"},{"location":"cmd/destroy/#flags","title":"Flags","text":""},{"location":"cmd/destroy/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology definition file that will be used get the elements of a lab that will be destroyed.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory. If a single file is found, it will be used.</p>"},{"location":"cmd/destroy/#cleanup","title":"cleanup","text":"<p>The local <code>--cleanup | -c</code> flag instructs containerlab to remove the lab directory and all its content.</p> <p>Without this flag present, containerlab will keep the lab directory and all files inside of it.</p> <p>Refer to the configuration artifacts page to get more information on the lab directory contents.</p>"},{"location":"cmd/destroy/#graceful","title":"graceful","text":"<p>To make containerlab attempt a graceful shutdown of the running containers, add the <code>--graceful</code> flag to destroy cmd. Without it, containers will be removed forcefully without even attempting to stop them.</p>"},{"location":"cmd/destroy/#keep-mgmt-net","title":"keep-mgmt-net","text":"<p>Do not try to remove the management network. Usually the management docker network (in case of docker) and the underlaying bridge are being removed. If you have attached additional resources outside of containerlab and you want the bridge to remain intact just add the <code>--keep-mgmt-net</code> flag.</p>"},{"location":"cmd/destroy/#all","title":"all","text":"<p>Destroy command provided with <code>--all | -a</code> flag will perform the deletion of all the labs running on the container host. It will not touch containers launched manually.</p>"},{"location":"cmd/destroy/#examples","title":"Examples","text":""},{"location":"cmd/destroy/#destroy-a-lab-described-in-the-given-topology-file","title":"Destroy a lab described in the given topology file","text":"<pre><code>containerlab destroy -t mylab.clab.yml\n</code></pre>"},{"location":"cmd/destroy/#destroy-a-lab-and-remove-the-lab-directory","title":"Destroy a lab and remove the Lab directory","text":"<pre><code>containerlab destroy -t mylab.clab.yml --cleanup\n</code></pre>"},{"location":"cmd/destroy/#destroy-a-lab-without-specifying-topology-file","title":"Destroy a lab without specifying topology file","text":"<p>Given that a single topology file is present in the current directory.</p> <pre><code>containerlab destroy\n</code></pre>"},{"location":"cmd/destroy/#destroy-all-labs-on-the-container-host","title":"Destroy all labs on the container host","text":"<pre><code>containerlab destroy -a\n</code></pre>"},{"location":"cmd/destroy/#destroy-a-lab-using-short-flag-names","title":"Destroy a lab using short flag names","text":"<pre><code>clab des\n</code></pre>"},{"location":"cmd/exec/","title":"exec command","text":""},{"location":"cmd/exec/#description","title":"Description","text":"<p>The <code>exec</code> command allows to run a command inside the nodes that part of a certain lab.</p> <p>This command does exactly the same thing as <code>docker exec</code> does, but it allows to run the same command across all the nodes of a lab.</p>"},{"location":"cmd/exec/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] exec [local-flags]</code></p>"},{"location":"cmd/exec/#flags","title":"Flags","text":""},{"location":"cmd/exec/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user specifies from which lab to take the containers and perform the exec command.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory. If a single file is found, it will be used.</p>"},{"location":"cmd/exec/#cmd","title":"cmd","text":"<p>The command to be executed on the nodes is provided with <code>--cmd</code> flag. The command is provided as a string, thus it needs to be quoted to accommodate for spaces or special characters.</p>"},{"location":"cmd/exec/#format","title":"format","text":"<p>The <code>--format | -f</code> flag allows to select between plain text format output or a json variant. Consult with the examples below to see the differences between these two formatting options.</p> <p>Defaults to <code>plain</code> output format.</p>"},{"location":"cmd/exec/#label","title":"label","text":"<p>By default <code>exec</code> command will attempt to execute the command across all the nodes of a lab. To limit the scope of the execution, the users can leverage the <code>--label</code> flag to filter out the nodes of interest.</p>"},{"location":"cmd/exec/#examples","title":"Examples","text":""},{"location":"cmd/exec/#execute-a-command-on-all-nodes-of-the-lab","title":"Execute a command on all nodes of the lab","text":"<p>Show ipv4 information from all the nodes of the lab with a plain text output</p> <pre><code>\u276f containerlab exec -t srl02.yml --cmd 'ip -4 a show dummy-mgmt0'\nINFO[0000] clab-srl02-srl1: stdout:\n6: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\ninet 172.20.20.3/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever\nINFO[0000] clab-srl02-srl2: stdout:\n6: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\ninet 172.20.20.2/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever\n</code></pre>"},{"location":"cmd/exec/#execute-a-command-on-a-node-referenced-by-its-name","title":"Execute a command on a node referenced by its name","text":"<p>Show ipv4 information from a specific node of the lab with a plain text output</p> <pre><code>\u276f containerlab exec -t srl02.yml --label clab-node-name\\=srl2 --cmd 'ip -4 a show dummy-mgmt0'\nINFO[0000] Parsing &amp; checking topology file: srl02.yml  INFO[0000] Executed command 'ip -4 a show dummy-mgmt0' on clab-srl02-srl2. stdout:\n6: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\ninet 172.20.20.5/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever </code></pre>"},{"location":"cmd/exec/#execute-a-cli-command","title":"Execute a CLI Command","text":"<pre><code>\u276f containerlab exec -t srl02.yml --cmd 'sr_cli  \"show version\"'\nINFO[0001] clab-srl02-srl1: stdout:\n----------------------------------------------------\nHostname          : srl1\nChassis Type      : 7250 IXR-6\nPart Number       : Sim Part No.\nSerial Number     : Sim Serial No.\nSystem MAC Address: 02:00:6B:FF:00:00\nSoftware Version  : v20.6.3\nBuild Number      : 145-g93496a3f8c\nArchitecture      : x86_64\nLast Booted       : 2021-06-24T10:25:26.722Z\nTotal Memory      : 24052875 kB\nFree Memory       : 21911906 kB\n----------------------------------------------------\nINFO[0003] clab-srl02-srl2: stdout:\n----------------------------------------------------\nHostname          : srl2\nChassis Type      : 7250 IXR-6\nPart Number       : Sim Part No.\nSerial Number     : Sim Serial No.\nSystem MAC Address: 02:D8:A9:FF:00:00\nSoftware Version  : v20.6.3\nBuild Number      : 145-g93496a3f8c\nArchitecture      : x86_64\nLast Booted       : 2021-06-24T10:25:26.904Z\nTotal Memory      : 24052875 kB\nFree Memory       : 21911914 kB\n----------------------------------------------------\n</code></pre>"},{"location":"cmd/exec/#execute-a-command-with-json-formatted-output","title":"Execute a Command with json formatted output","text":"<pre><code>\u276f containerlab exec -t srl02.yml --cmd 'sr_cli  \"show version | as json\"' -f json | jq\n{\n\"clab-srl02-srl1\": {\n\"stderr\": \"\",\n    \"stdout\": {\n\"basic system info\": {\n\"Architecture\": \"x86_64\",\n        \"Build Number\": \"145-g93496a3f8c\",\n        \"Chassis Type\": \"7250 IXR-6\",\n        \"Free Memory\": \"21911367 kB\",\n        \"Hostname\": \"srl1\",\n        \"Last Booted\": \"2021-06-24T10:25:26.722Z\",\n        \"Part Number\": \"Sim Part No.\",\n        \"Serial Number\": \"Sim Serial No.\",\n        \"Software Version\": \"v20.6.3\",\n        \"System MAC Address\": \"02:00:6B:FF:00:00\",\n        \"Total Memory\": \"24052875 kB\"\n}\n}\n},\n  \"clab-srl02-srl2\": {\n\"stderr\": \"\",\n    \"stdout\": {\n\"basic system info\": {\n\"Architecture\": \"x86_64\",\n        \"Build Number\": \"145-g93496a3f8c\",\n        \"Chassis Type\": \"7250 IXR-6\",\n        \"Free Memory\": \"21911367 kB\",\n        \"Hostname\": \"srl2\",\n        \"Last Booted\": \"2021-06-24T10:25:26.904Z\",\n        \"Part Number\": \"Sim Part No.\",\n        \"Serial Number\": \"Sim Serial No.\",\n        \"Software Version\": \"v20.6.3\",\n        \"System MAC Address\": \"02:D8:A9:FF:00:00\",\n        \"Total Memory\": \"24052875 kB\"\n}\n}\n}\n}\n</code></pre>"},{"location":"cmd/generate/","title":"generate command","text":""},{"location":"cmd/generate/#description","title":"Description","text":"<p>The <code>generate</code> command generates the topology definition file based on the user input provided via CLI flags.</p> <p>With this command it is possible to generate definition file for a CLOS fabric by just providing the number of nodes on each tier. The generated topology can be saved in a file or immediately scheduled for deployment.</p> <p>It is assumed, that the interconnection between the tiers is done in a full-mesh fashion. Such as tier1 nodes are fully meshed with tier2, tier2 is meshed with tier3 and so on.</p>"},{"location":"cmd/generate/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] generate [local-flags]</code></p> <p>aliases: <code>gen</code></p>"},{"location":"cmd/generate/#flags","title":"Flags","text":""},{"location":"cmd/generate/#name","title":"name","text":"<p>With the global <code>--name | -n</code> flag a user sets the name of the lab that will be generated.</p>"},{"location":"cmd/generate/#nodes","title":"nodes","text":"<p>The user configures the CLOS fabric topology by using the <code>--nodes</code> flag. The flag value is a comma separated list of CLOS tiers where each tier is defined by the number of nodes, its kind and type. Multiple <code>--node</code> flags can be specified.</p> <p>For example, the following flag value will define a 2-tier CLOS fabric with tier1 (leafs) consists of 4x SR Linux containers of IXR-D3 type and the 2x Arista cEOS spines: <pre><code>4:srl:ixrd3,2:ceos\n</code></pre></p> <p>Note, that the default kind is <code>srl</code>, so you can omit the kind for SR Linux node. The same nodes value can be expressed like that: <code>4:ixrd3,2:ceos</code></p>"},{"location":"cmd/generate/#kind","title":"kind","text":"<p>With <code>--kind</code> flag it is possible to set the default kind that will be set for the nodes which do not have a kind specified in the <code>--nodes</code> flag.</p> <p>For example the following value will generate a 3-tier CLOS fabric of cEOS nodes:</p> <pre><code># cEOS fabric\ncontainerlab gen -n 3tier --kind ceos --nodes 4,2,1\n\n# since SR Linux kind is assumed by default\n# SRL fabric command is even shorter\ncontainerlab gen -n 3tier --nodes 4,2,1\n</code></pre>"},{"location":"cmd/generate/#image","title":"image","text":"<p>Use <code>--image</code> flag to specify the container image that should be used by a given kind.</p> <p>The value of this flag follows the <code>kind=image</code> pattern. For example, to set the container image <code>ceos:4.21.F</code> for the <code>ceos</code> kind the flag will be: <code>--image ceos=ceos:4.21.F</code>.</p> <p>To set images for multiple kinds repeat the flag: <code>--image srl=srlinux:latest --image ceos=ceos:4.21.F</code> or use the comma separated form: <code>--image srl=srlinux:latest,ceos=ceos:latest</code></p> <p>If the kind information is not provided in the <code>image</code> flag, the kind value will be taken from the <code>--kind</code> flag.</p>"},{"location":"cmd/generate/#license","title":"license","text":"<p>With <code>--license</code> flag it is possible to set the license path that should be used by a given kind.</p> <p>The value of this flag follows the <code>kind=path</code> pattern. For example, to set the license path for the <code>srl</code> kind: <code>--license srl=/tmp/license.key</code>.</p> <p>To set license for multiple kinds repeat the flag: <code>--license &lt;kind1&gt;=/path1 --image &lt;kind2&gt;=/path2</code> or use the comma separated form: <code>--license &lt;kind1&gt;=/path1,&lt;kind2&gt;=/path2</code></p>"},{"location":"cmd/generate/#deploy","title":"deploy","text":"<p>When <code>--deploy</code> flag is present, the lab deployment process starts using the generated topology definition file.</p> <p>The generated definition file is first saved by the path set with <code>--file</code> or, if file path is not set, by the default path of <code>&lt;lab-name&gt;.clab.yml</code>. Then the equivalent of the <code>deploy -t &lt;file&gt; --reconfigure</code> command is executed.</p>"},{"location":"cmd/generate/#max-workers","title":"max-workers","text":"<p>With <code>--max-workers</code> flag it is possible to limit the amout of concurrent workers that create containers or wire virtual links. By default the number of workers equals the number of nodes/links to create.</p> <p>If during the deployment of a large scaled lab you see errors about max number of opened files reached, limit the max workers with this flag.</p>"},{"location":"cmd/generate/#file","title":"file","text":"<p>With <code>--file</code> flag its possible to save the generated topology definition in a file by a given path.</p>"},{"location":"cmd/generate/#node-prefix","title":"node-prefix","text":"<p>With <code>--node-prefix</code> flag a user sets the name prefix of every node in a lab.</p> <p>Nodes will be named by the following template: <code>&lt;node-prefix&gt;-&lt;tier&gt;-&lt;node-number&gt;</code>. So a node named <code>node1-3</code> means this is the third node in a first tier of a topology.</p> <p>Default prefix: <code>node</code>.</p>"},{"location":"cmd/generate/#group-prefix","title":"group-prefix","text":"<p>With <code>--group-prefix</code> it is possible to change the Group value of a node. Group information is used in the topology graph rendering.</p>"},{"location":"cmd/generate/#network","title":"network","text":"<p>With <code>--network</code> flag a user sets the name of the management network that will be created by container orchestration system such as docker.</p> <p>Default: <code>clab</code>.</p>"},{"location":"cmd/generate/#ipv4-subnet-ipv6-subnet","title":"ipv4-subnet | ipv6-subnet","text":"<p>With <code>--ipv4-subnet</code> and <code>ipv6-subnet</code> its possible to change the address ranges of the management network. Nodes will receive IP addresses from these ranges if they are configured with DHCP.</p>"},{"location":"cmd/generate/#examples","title":"Examples","text":""},{"location":"cmd/generate/#generate-topology-for-a-3-tier-clos-network","title":"Generate topology for a 3-tier CLOS network","text":"<p>Generate and deploy a lab topology for 3-tier CLOS network with 8 leafs, 4 spines and 2 superspines. All using Nokia SR Linux nodes with license and image provided.</p> <p>Note</p> <p>The <code>srl</code> kind in the image and license flags can be omitted, as it is implied by default</p> <pre><code>containerlab generate --name 3tier --image srl=srlinux:latest \\\n--license srl=license.key \\\n--nodes 8,4,2 --deploy\n</code></pre>"},{"location":"cmd/graph/","title":"graph command","text":""},{"location":"cmd/graph/#description","title":"Description","text":"<p>The <code>graph</code> command generates graphical representations of a topology.</p> <p>Two graphing options are available:</p> <ul> <li>an HTML page served by <code>containerlab</code> web-server based on a user-provided HTML template and static files.</li> <li>a graph description file in dot format that can be rendered using Graphviz or viewed online.<sup>1</sup></li> </ul>"},{"location":"cmd/graph/#html","title":"HTML","text":"<p>The HTML-based graph representation is the default graphing option. The topology will be graphed and served online using the embedded web server.</p> <p>The default graph template is based on the NeXt UI framework<sup>2</sup>.</p> <p></p> <p>To render a topology using this default graph engine:</p> <pre><code>containerlab graph -t &lt;path/to/topo.clab.yml&gt;\n</code></pre>"},{"location":"cmd/graph/#next-ui","title":"NeXt UI","text":"<p>Topology graph created with NeXt UI has some control elements that allow you to choose the color theme of the web view, scaling and panning. Besides these generic controls it is possible to enable auto-layout of the components using buttons at the top of the screen.</p>"},{"location":"cmd/graph/#layout-and-sorting","title":"Layout and sorting","text":"<p>The graph engine can automatically pan and sort elements in your topology based on their role. We encode the role via <code>group</code> property of a node.</p> <p>Today we have the following sort orders available to users:</p> <p><pre><code>sortOrder: ['10', '9', 'superspine', '8', 'dc-gw', '7', '6', 'spine', '5', '4', 'leaf', 'border-leaf', '3', 'server', '2', '1'],\n</code></pre> The values are sorted so that <code>10</code> is placed higher in the hierarchy than <code>9</code> and so on.</p> <p>Consider the following snippet:</p> <pre><code>topology:\nnodes:\n### SPINES ###\nspine1:\ngroup: spine\n### LEAFS ###\nleaf1:\ngroup: leaf\n### CLIENTS ###\nclient1:\nkind: linux\ngroup: server\n</code></pre> <p>The <code>group</code> property set to the predefined value will automatically auto-align the elements based on their role.</p>"},{"location":"cmd/graph/#graphviz","title":"Graphviz","text":"<p>When <code>graph</code> command is called without the <code>--srv</code> flag, containerlab will generate a graph description file in dot format.</p> <p>The dot file can be used to view the graphical representation of the topology either by rendering the dot file into a PNG file or using online dot viewer.</p>"},{"location":"cmd/graph/#online-vs-offline-graphing","title":"Online vs offline graphing","text":"<p>When HTML graph option is used, containerlab will try to build the topology graph by inspecting the running containers which are part of the lab. This essentially means, that the lab must be running. Although this method provides some additional details (like IP addresses), it is not always convenient to run a lab to see its graph.</p> <p>The other option is to use the topology file solely to build the graph. This is done by adding <code>--offline</code> flag.</p> <p>If <code>--offline</code> flag was not provided and no containers were found matching the lab name, containerlab will use the topo file only (as if offline mode was set).</p>"},{"location":"cmd/graph/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] graph [local-flags]</code></p>"},{"location":"cmd/graph/#flags","title":"Flags","text":""},{"location":"cmd/graph/#topology","title":"topology","text":"<p>With the global <code>--topo | -t</code> flag a user sets the path to the topology file that will be used to get the nodes of a lab.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory. If a single file is found, it will be used.</p>"},{"location":"cmd/graph/#srv","title":"srv","text":"<p>The <code>--srv</code> flag allows a user to customize the HTTP address and port for the web server. Default value is <code>:50080</code>.</p> <p>A single path <code>/</code> is served, where the graph is generated based on either a default template or on the template supplied using <code>--template</code>.</p>"},{"location":"cmd/graph/#template","title":"template","text":"<p>The <code>--template</code> flag allows to customize the HTML based graph by supplying a user defined template that will be rendered and exposed on the address specified by <code>--srv</code>.</p>"},{"location":"cmd/graph/#static-dir","title":"static-dir","text":"<p>The <code>--static-dir</code> flag enables the embedded HTML web-server to serve static files from the specified directory. Must be used together with the <code>--template</code> flag.</p> <p>With this flag, it is possible to link to local files (JS, CSS, fonts, etc.) from the custom HTML template.</p>"},{"location":"cmd/graph/#dot","title":"dot","text":"<p>With <code>--dot</code> flag provided containerlab will generate the <code>dot</code> file instead of serving the topology with embedded HTTP server.</p>"},{"location":"cmd/graph/#examples","title":"Examples","text":""},{"location":"cmd/graph/#render-graph-of-topology-on-html-server","title":"Render graph of topology on HTML server","text":"<p>This will render the running lab if the lab is running and the topology file if it isn't. Default options will be used (HTML server running on port <code>50080</code>).</p> <pre><code>containerlab graph -t /path/to/topo1.clab.yml\n</code></pre>"},{"location":"cmd/graph/#render-graph-on-specified-http-server-port","title":"Render graph on specified http server port","text":"<pre><code>containerlab graph --topo /path/to/topo1.clab.yml --srv \":3002\"\n</code></pre>"},{"location":"cmd/graph/#render-graph-using-a-custom-html-template","title":"Render graph using a custom html template","text":"<pre><code>containerlab graph --topo /path/to/topo1.clab.yml --template my_template.html\n</code></pre>"},{"location":"cmd/graph/#render-graph-using-a-custom-template-that-links-to-local-files","title":"Render graph using a custom template that links to local files","text":"<p>The HTML server will use a custom template that links to local files located at /path/to/static_files directory <pre><code>containerlab graph --topo /path/to/topo1.clab.yml --template my_template.html --static-dir /path/to/static_files\n</code></pre></p> <ol> <li> <p>This method is prone to errors when node names contain dashes and special symbols. Use with caution, and prefer the HTML server alternative.\u00a0\u21a9</p> </li> <li> <p>NeXt UI css/js files can be found at <code>/etc/containerlab/templates/graph/nextui</code> directory\u00a0\u21a9</p> </li> </ol>"},{"location":"cmd/inspect/","title":"inspect command","text":""},{"location":"cmd/inspect/#description","title":"Description","text":"<p>The <code>inspect</code> command provides the information about the deployed labs.</p>"},{"location":"cmd/inspect/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] inspect [local-flags]</code></p>"},{"location":"cmd/inspect/#flags","title":"Flags","text":""},{"location":"cmd/inspect/#all","title":"all","text":"<p>With the local <code>--all</code> flag its possible to list all deployed labs in a single table. The output will also show the relative path to the topology file that was used to spawn this lab.</p> <p>The lab name and path values will be set for the first node of such lab, to reduce the clutter. Refer to the examples section for more details.</p>"},{"location":"cmd/inspect/#topology-name","title":"topology | name","text":"<p>With the global <code>--topo | -t</code> or <code>--name | -n</code> flag a user specifies which particular lab they want to get the information about.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory. If a single file is found, it will be used.</p>"},{"location":"cmd/inspect/#format","title":"format","text":"<p>The local <code>--format</code> flag enables different output stylings. By default the table view will be used.</p> <p>Currently, the only other format option is <code>json</code> that will produce the output in the JSON format.</p>"},{"location":"cmd/inspect/#details","title":"details","text":"<p>The <code>inspect</code> command produces a brief summary about the running lab components. It is also possible to get a full view on the running containers by adding <code>--details</code> flag.</p> <p>With this flag inspect command will output every bit of information about the running containers. This is what <code>docker inspect</code> command provides.</p>"},{"location":"cmd/inspect/#examples","title":"Examples","text":""},{"location":"cmd/inspect/#list-all-running-labs-on-the-host","title":"List all running labs on the host","text":"<pre><code>\u276f containerlab inspect --all\n+---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+\n| # | Topo Path  | Lab Name |      Name       | Container ID |       Image        | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+\n| 1 | newlab.yml | newlab   | clab-newlab-n1  | 3c8262034088 | srlinux:20.6.3-145 | srl  |       | running | 172.20.20.4/24 | 2001:172:20:20::4/80 |\n| 2 |            |          | clab-newlab-n2  | 79c562b71997 | srlinux:20.6.3-145 | srl  |       | running | 172.20.20.5/24 | 2001:172:20:20::5/80 |\n| 3 | srl02.yml  | srl01    | clab-srl01-srl  | 13c9e7543771 | srlinux:20.6.3-145 | srl  |       | running | 172.20.20.2/24 | 2001:172:20:20::2/80 |\n| 4 |            |          | clab-srl01-srl2 | 8cfca93b7b6f | srlinux:20.6.3-145 | srl  |       | running | 172.20.20.3/24 | 2001:172:20:20::3/80 |\n+---+------------+----------+-----------------+--------------+--------------------+------+-------+---------+----------------+----------------------+\n</code></pre>"},{"location":"cmd/inspect/#provide-information-about-a-specific-running-lab-by-its-name","title":"Provide information about a specific running lab by its name","text":"<p>Provide information about the running lab named <code>srlceos01</code></p> <pre><code>\u276f containerlab inspect --name srlceos01\n+---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| # |        Name         | Container ID |  Image  | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| 1 | clab-srlceos01-ceos | 90bebb1e2c5f | ceos    | ceos |       | running | 172.20.20.4/24 | 2001:172:20:20::4/80 |\n| 2 | clab-srlceos01-srl  | 82e9aa3c7e6b | srlinux | srl  |       | running | 172.20.20.3/24 | 2001:172:20:20::3/80 |\n+---+---------------------+--------------+---------+------+-------+---------+----------------+----------------------+\n</code></pre>"},{"location":"cmd/inspect/#provide-information-about-a-specific-running-lab-by-its-topology-file","title":"Provide information about a specific running lab by its topology file","text":"<pre><code>\u276f clab inspect -t srl02.clab.yml INFO[0000] Parsing &amp; checking topology file: srl02.clab.yml +---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+\n| # |      Name       | Container ID |         Image         | Kind |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+\n| 1 | clab-srl02-srl1 | 7a7c101be7d8 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.4/24 | 2001:172:20:20::4/64 |\n| 2 | clab-srl02-srl2 | 5e3737621753 | ghcr.io/nokia/srlinux | srl  | running | 172.20.20.5/24 | 2001:172:20:20::5/64 |\n+---+-----------------+--------------+-----------------------+------+---------+----------------+----------------------+\n</code></pre>"},{"location":"cmd/inspect/#provide-information-about-a-specific-running-lab-in-json-format","title":"Provide information about a specific running lab in json format","text":"<pre><code>\u276f containerlab inspect --name srlceos01 -f json\n[\n{\n\"lab_name\": \"srlceos01\",\n    \"name\": \"clab-srlceos01-srl\",\n    \"container_id\": \"82e9aa3c7e6b\",\n    \"image\": \"srlinux\",\n    \"kind\": \"srl\",\n    \"state\": \"running\",\n    \"ipv4_address\": \"172.20.20.3/24\",\n    \"ipv6_address\": \"2001:172:20:20::3/80\"\n},\n  {\n\"lab_name\": \"srlceos01\",\n    \"name\": \"clab-srlceos01-ceos\",\n    \"container_id\": \"90bebb1e2c5f\",\n    \"image\": \"ceos\",\n    \"kind\": \"ceos\",\n    \"state\": \"running\",\n    \"ipv4_address\": \"172.20.20.4/24\",\n    \"ipv6_address\": \"2001:172:20:20::4/80\"\n}\n]\n</code></pre>"},{"location":"cmd/save/","title":"save command","text":""},{"location":"cmd/save/#description","title":"Description","text":"<p>The <code>save</code> command perform configuration save for all the containers running in a lab.</p> <p>The exact command that performs configuration save depends on a given kind. The below table explains the method used for each kind:</p> Kind Command Notes Nokia SR Linux <code>sr_cli -d tools system configuration save</code> Nokia SR OS delivered via netconf RPC <code>copy-config running startup</code> Arista cEOS <code>Cli -p 15 -c wr</code>"},{"location":"cmd/save/#usage","title":"Usage","text":"<p><code>containerlab [global-flags] save [local-flags]</code></p>"},{"location":"cmd/save/#flags","title":"Flags","text":""},{"location":"cmd/save/#topology-name","title":"topology | name","text":"<p>With the global <code>--topo | -t</code> or <code>--name | -n</code> flag a user specifies from which lab to take the containers and perform the save configuration task.</p> <p>When the topology file flag is omitted, containerlab will try to find the matching file name by looking at the current working directory. If a single file is found, it will be used.</p>"},{"location":"cmd/save/#examples","title":"Examples","text":""},{"location":"cmd/save/#save-the-configuration-of-the-containers-in-a-specific-lab","title":"Save the configuration of the containers in a specific lab","text":"<p>Save the configuration of the containers running in lab named srl02</p> <pre><code>\u276f containerlab save -n srl02\nINFO[0001] clab-srl02-srl1: stdout: /system:\n    Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:54.998Z' and comment ''\nINFO[0002] clab-srl02-srl2: stdout: /system:\n    Generated checkpoint '/etc/opt/srlinux/checkpoint/checkpoint-0.json' with name 'checkpoint-2020-11-18T09:00:56.444Z' and comment ''\n</code></pre>"},{"location":"cmd/tools/disable-tx-offload/","title":"disable-tx-offload command","text":""},{"location":"cmd/tools/disable-tx-offload/#description","title":"Description","text":"<p>The <code>disable-tx-offload</code> command under the <code>tools</code> command disables tx checksum offload for <code>eth0</code> interface of a container referenced by its name.</p> <p>The need for <code>disable-tx-offload</code> might arise when you launch a container outside of containerlab or restart a container. Some nodes, like SR Linux, will require correct checksums in TCP packets; thus, it is needed to disable checksum offload on those containers to do checksum calculations instead of offloading it.</p>"},{"location":"cmd/tools/disable-tx-offload/#usage","title":"Usage","text":"<p><code>containerlab tools disable-tx-offload [local-flags]</code></p>"},{"location":"cmd/tools/disable-tx-offload/#flags","title":"Flags","text":""},{"location":"cmd/tools/disable-tx-offload/#container","title":"container","text":"<p>With the local mandatory <code>--container | -c</code> flag, a user specifies which container to remove tx offload.</p>"},{"location":"cmd/tools/disable-tx-offload/#examples","title":"Examples","text":"<pre><code># disable tx checksum offload on gnmic container\n\u276f clab tools disable-tx-offload -c clab-st-gnmic\nINFO[0000] getting container 'clab-st-gnmic' information INFO[0000] Tx checksum offload disabled for eth0 interface of clab-st-gnmic container </code></pre>"},{"location":"cmd/tools/cert/sign/","title":"Cert sign","text":""},{"location":"cmd/tools/cert/sign/#description","title":"Description","text":"<p>The <code>sign</code> sub-command under the <code>tools cert</code> command creates a private key and a certificate and signs the created certificate with a given Certificate Authority.</p>"},{"location":"cmd/tools/cert/sign/#usage","title":"Usage","text":"<p><code>containerlab tools cert sign [local-flags]</code></p>"},{"location":"cmd/tools/cert/sign/#flags","title":"Flags","text":""},{"location":"cmd/tools/cert/sign/#name","title":"Name","text":"<p>To set a name under which the certificate and key files will be saved, use the <code>--name | -n</code> flag. A name set to <code>mynode</code> will create files <code>mynode.pem</code>, <code>mynode.key</code> and <code>mynode.csr</code>. The default value is <code>cert</code>.</p>"},{"location":"cmd/tools/cert/sign/#path","title":"Path","text":"<p>A directory path under which the generated files will be placed is set with <code>--path | -p</code> flag. Defaults to acurrent working directory.</p>"},{"location":"cmd/tools/cert/sign/#ca-cert-and-ca-key","title":"CA Cert and CA Key","text":"<p>To indicate which CA should sign the certificate request, the provide a path to the CA certificate and key files.</p> <p><code>--ca-cert</code> flag sets the path to the CA certificate file. <code>--ca-key</code> flag sets the path to the CA private key file.</p>"},{"location":"cmd/tools/cert/sign/#common-name","title":"Common Name","text":"<p>Certificate Common Name (CN) field is set with <code>--cn</code> flag. Defaults to <code>containerlab.dev</code>.</p>"},{"location":"cmd/tools/cert/sign/#hosts","title":"Hosts","text":"<p>To add Subject Alternative Names (SAN) use the <code>--hosts</code> flag that takes a comma separate list of SAN values. Users can provide both DNS names and IP address, and the values will be placed into the DSN SAN and IP SAN automatically.</p>"},{"location":"cmd/tools/cert/sign/#country","title":"Country","text":"<p>Certificate Country (C) field is set with <code>--country | -c</code> flag. Defaults to <code>Internet</code>.</p>"},{"location":"cmd/tools/cert/sign/#locality","title":"Locality","text":"<p>Certificate Locality (L) field is set with <code>--locality | -l</code> flag. Defaults to <code>Server</code>.</p>"},{"location":"cmd/tools/cert/sign/#organization","title":"Organization","text":"<p>Certificate Organization (O) field is set with <code>--organization | -o</code> flag. Defaults to <code>Containerlab</code>.</p>"},{"location":"cmd/tools/cert/sign/#organization-unit","title":"Organization Unit","text":"<p>Certificate Organization Unit (OU) field is set with <code>--ou</code> flag. Defaults to <code>Containerlab Tools</code>.</p>"},{"location":"cmd/tools/cert/sign/#examples","title":"Examples","text":"<pre><code># create a private key and certificate and sign the latter\n# with the Hosts list of [node.io, 192.168.0.1]\n# saving both files under the default name `cert` in the PWD\n# and signed by the CA identified by cert ca.pem and key ca-key.pem\ncontainerlab tools cert sign --ca-cert /tmp/ca.pem \\\n--ca-key /tmp/ca.key \\\n--hosts node.io,192.168.0.1\n</code></pre> <p>Generated certificate can be verified/viewed with openssl tool:</p> <pre><code>openssl x509 -in ca.pem -text\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            3f:a7:77:54:e1:2f:47:d6:ca:56:72:e1:d1:d8:c9:0c:e8:46:fd:65\n&lt;SNIP&gt;\n</code></pre>"},{"location":"cmd/tools/cert/ca/create/","title":"CA Create","text":""},{"location":"cmd/tools/cert/ca/create/#description","title":"Description","text":"<p>The <code>create</code> sub-command under the <code>tools cert ca</code> command creates a Certificate Authority (CA) certificate and its private key.</p>"},{"location":"cmd/tools/cert/ca/create/#usage","title":"Usage","text":"<p><code>containerlab tools cert ca create [local-flags]</code></p>"},{"location":"cmd/tools/cert/ca/create/#flags","title":"Flags","text":""},{"location":"cmd/tools/cert/ca/create/#name","title":"Name","text":"<p>To set a name under which the certificate and key files will be saved, use the <code>--name | -n</code> flag. A name set to <code>myname</code> will create files <code>myname.pem</code>, <code>mynamey.key</code> and <code>myname.csr</code>. The default value is <code>ca</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#path","title":"Path","text":"<p>A directory path under which the generated files will be placed is set with <code>--path | -p</code> flag. Defaults to current working directory.</p>"},{"location":"cmd/tools/cert/ca/create/#expiry","title":"Expiry","text":"<p>Certificate validity period is set as a duration interval with <code>--expiry | -e</code> flag. Defaults to <code>87600h</code>, which is 10 years.</p>"},{"location":"cmd/tools/cert/ca/create/#common-name","title":"Common Name","text":"<p>Certificate Common Name (CN) field is set with <code>--cn</code> flag. Defaults to <code>containerlab.dev</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#country","title":"Country","text":"<p>Certificate Country (C) field is set with <code>--country | -c</code> flag. Defaults to <code>Internet</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#locality","title":"Locality","text":"<p>Certificate Locality (L) field is set with <code>--locality | -l</code> flag. Defaults to <code>Server</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#organization","title":"Organization","text":"<p>Certificate Organization (O) field is set with <code>--organization | -o</code> flag. Defaults to <code>Containerlab</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#organization-unit","title":"Organization Unit","text":"<p>Certificate Organization Unit (OU) field is set with <code>--ou</code> flag. Defaults to <code>Containerlab Tools</code>.</p>"},{"location":"cmd/tools/cert/ca/create/#examples","title":"Examples","text":"<pre><code># create CA cert and key in the current dir.\n# uses default values for all certificate attributes\n# as a result, ca.pem and ca-cert.pem files will be written to the\n# current working directory\ncontainerlab tools cert ca create\n\n# create CA cert and key by the specified path with a filename root-ca\n# and a validity period of 1 minute\ncontainerlab tools cert ca create --path /tmp/certs/myca --name root-ca \\\n--expiry 1m\n\nopenssl x509 -in /tmp/certs/myca/root-ca.pem -text | grep -A 2 Validity\n        Validity\n            Not Before: Mar 25 15:28:00 2021 GMT\n            Not After : Mar 25 15:29:00 2021 GMT\n</code></pre> <p>Generated certificate can be verified/viewed with openssl tool:</p> <pre><code>openssl x509 -in ca.pem -text\nCertificate:\n    Data:\n        Version: 3 (0x2)\n        Serial Number:\n            3f:a7:77:54:e1:2f:47:d6:ca:56:72:e1:d1:d8:c9:0c:e8:46:fd:65\n&lt;SNIP&gt;\n</code></pre>"},{"location":"cmd/tools/mysocketio/login/","title":"Mysocketio login","text":""},{"location":"cmd/tools/mysocketio/login/#description","title":"Description","text":"<p>The <code>login</code> sub-command under the <code>tools mysocketio</code> command performs a login to mysocketio service and saves the acquired authentication token<sup>1</sup>.</p> <p>The token is saved as <code>$PWD/.mysocketio_token</code> file.</p>"},{"location":"cmd/tools/mysocketio/login/#usage","title":"Usage","text":"<p><code>containerlab tools mysocketio login [local-flags]</code></p>"},{"location":"cmd/tools/mysocketio/login/#flags","title":"Flags","text":""},{"location":"cmd/tools/mysocketio/login/#email","title":"email","text":"<p>With mandatory <code>--email | -e</code> flag user sets an email address used to register with mysocketio service</p>"},{"location":"cmd/tools/mysocketio/login/#password","title":"password","text":"<p>The <code>--password | -p</code> sets the password for a user. If flag is not set, the prompt will appear on the terminal to allow for safe enter of the password.</p>"},{"location":"cmd/tools/mysocketio/login/#examples","title":"Examples","text":"<pre><code># Login with password entered from the prompt\ncontainerlab tools mysocketio login -e myemail@dot.com\nPassword:\nINFO[0000] Written mysocketio token to a file /root/containerlab/.mysocketio_token\n\n# Login with password passed as a flag\ncontainerlab tools mysocketio login -e myemail@dot.com -p Pa$$word\nPassword:\nINFO[0000] Written mysocketio token to a file /root/containerlab/.mysocketio_token\n</code></pre> <ol> <li> <p>Authentication token is used to publish ports of a containerlab nodes.\u00a0\u21a9</p> </li> </ol>"},{"location":"cmd/tools/veth/create/","title":"vEth create","text":""},{"location":"cmd/tools/veth/create/#description","title":"Description","text":"<p>The <code>create</code> sub-command under the <code>tools veth</code> command creates a vEth interface between the following combination of nodes:</p> <ol> <li>container &lt;-&gt; container</li> <li>container &lt;-&gt; linux bridge</li> <li>container &lt;-&gt; ovs bridge</li> <li>container &lt;-&gt; host</li> </ol> <p>To specify the both endpoints of the veth interface pair the following two notations are used:</p> <ol> <li>two elements notation: <code>&lt;node-name&gt;:&lt;interface-name&gt;</code>     this notation is used for <code>container &lt;-&gt; container</code> or <code>container &lt;-&gt; host</code> attachments.</li> <li>three elements notation: <code>&lt;kind&gt;:&lt;node-name&gt;:&lt;interface-name&gt;</code>     this notation is used for <code>container &lt;-&gt; bridge</code> and <code>container &lt;-&gt; ovs-bridge</code> attachments</li> </ol> <p>Check out examples to see how these notations are used.</p>"},{"location":"cmd/tools/veth/create/#usage","title":"Usage","text":"<p><code>containerlab tools veth create [local-flags]</code></p>"},{"location":"cmd/tools/veth/create/#flags","title":"Flags","text":""},{"location":"cmd/tools/veth/create/#a-endpoint","title":"a-endpoint","text":"<p>vEth interface endpoint A is set with <code>--a-endpoint | -a</code> flag.</p>"},{"location":"cmd/tools/veth/create/#b-endpoint","title":"b-endpoint","text":"<p>vEth interface endpoint B is set with <code>--b-endpoint | -b</code> flag.</p>"},{"location":"cmd/tools/veth/create/#mtu","title":"mtu","text":"<p>vEth interface MTU is set to <code>9500</code> by default, and can be changed with <code>--mtu | -m</code> flag.</p>"},{"location":"cmd/tools/veth/create/#examples","title":"Examples","text":"<pre><code># create veth interface between containers clab-demo-node1 and clab-demo-node2\n# both ends of veth pair will be named `eth1`\ncontainerlab tools veth create -a clab-demo-node1:eth1 -b clab-demo-node2:eth1\n\n# create veth interface between container clab-demo-node1 and linux bridge br-1\ncontainerlab tools veth create -a clab-demo-node1:eth1 -b bridge:br-1:br-eth1\n\n# create veth interface between container clab-demo-node1 and OVS bridge ovsbr-1\ncontainerlab tools veth create -a clab-demo-node1:eth1 -b ovs-bridge:ovsbr-1:br-eth1\n\n# create veth interface between container clab-demo-node1 and host\n# note that a special node-name `host` is reserved to indicate that attachment is destined for container host system\ncontainerlab tools veth create -a clab-demo-node1:eth1 -b host:veth-eth1\n</code></pre>"},{"location":"cmd/tools/vxlan/create/","title":"vxlan create","text":""},{"location":"cmd/tools/vxlan/create/#description","title":"Description","text":"<p>The <code>create</code> sub-command under the <code>tools vxlan</code> command creates a VxLAN interface and sets <code>tc</code> rules to redirect traffic to/from a specified interface available in root namespace of a container host.</p> <p>This combination of a VxLAN interface and <code>tc</code> rules make possible to transparently connect lab nodes running on different VMs/hosts.</p> <p>VxLAN interface name will be a catenation of a prefix <code>vx-</code> and the interface name that is used to redirect traffic. If the existing interface is named <code>srl_e1-1</code>, then VxLAN interface created for this interface will be named <code>vx-srl_e1-1</code>.</p>"},{"location":"cmd/tools/vxlan/create/#usage","title":"Usage","text":"<p><code>containerlab tools vxlan create [local-flags]</code></p>"},{"location":"cmd/tools/vxlan/create/#flags","title":"Flags","text":""},{"location":"cmd/tools/vxlan/create/#remote","title":"remote","text":"<p>VxLAN tunnels set up with this command are unidirectional in nature. To set the remote endpoint address the <code>--remote</code> flag should be used.</p>"},{"location":"cmd/tools/vxlan/create/#id","title":"id","text":"<p>VNI that the VxLAN tunnel will use is set with <code>--id | -i</code> flag. Defaults to <code>10</code>.</p>"},{"location":"cmd/tools/vxlan/create/#link","title":"link","text":"<p>As mentioned above, the tunnels are set up with a goal to transparently connect containers deployed on different hosts.</p> <p>To indicate which interface will be \"piped\" to a VxLAN tunnel the <code>--link | -l</code> flag should be used.</p>"},{"location":"cmd/tools/vxlan/create/#dev","title":"dev","text":"<p>With <code>--dev</code> flag users can set the linux device that should be used in setting up the tunnel.</p> <p>Normally this flag can be omitted, since containerlab will take the device name which is used to reach the remote address as seen by the kernel routing table.</p>"},{"location":"cmd/tools/vxlan/create/#mtu","title":"mtu","text":"<p>With <code>--mtu | -m</code> flag it is possible to set VxLAN MTU. Max MTU is automatically set, so this flag is only needed when MTU lower than max is needed to be provisioned.</p>"},{"location":"cmd/tools/vxlan/create/#examples","title":"Examples","text":"<pre><code># create vxlan tunnel and redirect traffic to/from existing interface srl_e1-1 to it\n# this effectively means anything that appears on srl_e1-1 interface will be piped to vxlan interface\n# and vice versa.\n# srl_e1-1 interface exists in root namespace\n\u276f ip l show srl_e1-1\n617: srl_e1-1@if618: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether fa:4c:16:11:11:05 brd ff:ff:ff:ff:ff:ff link-netns clab-vx-srl1\n\n# create a vxlan tunnel to a remote vtep 10.0.0.20 with VNI 10 and redirect traffic to srl_e1-1 interface\n\u276f clab tools vxlan create --remote 10.0.0.20 -l srl_e1-1 --id 10\nINFO[0000] Adding VxLAN link vx-srl_e1-1 under ens3 to remote address 10.0.0.20 with VNI 10\nINFO[0000] configuring ingress mirroring with tc in the direction of vx-srl_e1-1 -&gt; srl_e1-1\nINFO[0000] configuring ingress mirroring with tc in the direction of srl_e1-1 -&gt; vx-srl_e1-1\n\n# check the created interface\n\u276f ip l show vx-srl_e1-1\n619: vx-srl_e1-1: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1400 qdisc noqueue state UNKNOWN mode DEFAULT group default qlen 1000\nlink/ether 7a:6e:ba:82:a4:6f brd ff:ff:ff:ff:ff:ff\n</code></pre>"},{"location":"cmd/tools/vxlan/delete/","title":"vxlan delete","text":""},{"location":"cmd/tools/vxlan/delete/#description","title":"Description","text":"<p>The <code>delete</code> sub-command under the <code>tools vxlan</code> command deletes VxLAN interfaces which name matches a user specified prefix. The <code>delete</code> command is typically used to remove VxLAN interfaces created with <code>create</code> command.</p>"},{"location":"cmd/tools/vxlan/delete/#usage","title":"Usage","text":"<p><code>containerlab tools vxlan delete [local-flags]</code></p>"},{"location":"cmd/tools/vxlan/delete/#flags","title":"Flags","text":""},{"location":"cmd/tools/vxlan/delete/#prefix","title":"prefix","text":"<p>Set a prefix with <code>--prefix | -p</code> flag. The VxLAN interfaces which name is matched by the prefix will be deleted. Default prefix is <code>vx-</code> which is matched the default prefix used by <code>create</code> command.</p>"},{"location":"cmd/tools/vxlan/delete/#examples","title":"Examples","text":"<pre><code># delete all VxLAN interfaces created by containerlab\n\u276f clab tools vxlan create delete\nINFO[0000] Deleting VxLAN link vx-srl_e1-1\n</code></pre>"},{"location":"lab-examples/bgp-vpls-nok-jun/","title":"BGP VPLS between Nokia and Juniper","text":"Description BGP VPLS between Nokia SR OS and Juniper vMX Components Nokia SR OS, Juniper vMX Resource requirements<sup>1</sup>  2  7-10 GB Lab location hellt/bgp-vpls-lab Topology file vpls.clab.yml Version information<sup>2</sup> <code>containerlab:0.10.1</code>, <code>vr-sros:20.10.R1</code>, <code>vr-vmx:20.4R1.12</code>, <code>docker-ce:19.03.13</code>, <code>vrnetlab</code><sup>3</sup>"},{"location":"lab-examples/bgp-vpls-nok-jun/#description","title":"Description","text":"<p>This lab demonstrates how containerlab can be used in a classical networking labs where the prime focus is not on the containerized NOS, but on a classic VM-based routers.</p> <p>The topology created in this lab matches the network used in the BGP VPLS Deep Dive article:</p> <p></p> <p>It allows readers to follow through the article with the author and create BGP VPLS service between the Nokia and Juniper routers using configuration snippets provided within the lab repository.</p> <p>As the article was done before Nokia introduced MD-CLI, the configuration snippets for SR OS were translated to MD-CLI.</p>"},{"location":"lab-examples/bgp-vpls-nok-jun/#quickstart","title":"Quickstart","text":"<ol> <li>Ensure that your host supports virtualization and/or nested virtualization in case of a VM.</li> <li>Install<sup>4</sup> containerlab.</li> <li>Build if needed, vrnetlab container images for the routers used in the lab.</li> <li>Clone lab repository.</li> <li>Deploy the lab topology <code>clab dep -t vpls.clab.yml</code></li> </ol> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information. Memory deduplication techniques like UKSM might help with RAM consumption.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> <li> <p>Router images are built with vrnetlab aebe377. To reproduce the image, checkout to this commit and build the relevant images. Note, that you might need to use containerlab of the version that is stated in the description.\u00a0\u21a9</p> </li> <li> <p>If installing the latest containerlab, make sure to use the latest hellt/vrnetlab project as well, as there might have been changes with the integration. If unsure, install the containerlab version that is specified in the lab description.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/cfg-clos/","title":"5-stage SR Linux based Clos fabric with config engine","text":"Description A 5-stage Clos topology based on SR Linux nodes configured using Config Engine Components Nokia SR Linux, Nokia SR OS Resource requirements<sup>1</sup>  4  12 GB Lab folder lab-examples/Clos03 Version information <code>containerlab:0.19.0</code>, <code>srlinux:21.6.2-67</code>, <code>vr-sros:21.7.R1</code> Authors Bastien Claeys"},{"location":"lab-examples/cfg-clos/#description","title":"Description","text":"<p>This lab features a 5-stage (two tier) Clos fabric which consists of the following components:</p> <ul> <li>leaf and spine elements based on Nokia SR Linux</li> <li>DC Gateway elements running SR OS</li> <li>CE devices emulated by a single SR OS device to allow for BGP configuration between a Leaf and a CE. </li> </ul> <p>In addition to the topology file, the lab directory contains a set of files to handle the interface and BGP configurations, providing an environment ready for the provisioning of services/workloads.\u200b</p> <p>This topology leverages the Configuration Engine embedded in Containerlab. With the provided templates, configuration of nodes can be achieved in a few seconds.</p> <p>The provided topology is using the following configuration:</p> <ul> <li>Underlay networking is achieved via eBGP</li> <li>Overlay iBGP sessions established to exchange EVPN routes</li> </ul>"},{"location":"lab-examples/cfg-clos/#lab-walkthrough","title":"Lab Walkthrough","text":""},{"location":"lab-examples/cfg-clos/#execution","title":"Execution","text":"<pre><code># Deploy the topology\n$ containerlab deploy -t cfg-clos.clab.yml\n\n# Generate and apply the configuration from the templates\n$ containerlab config -t cfg-clos.clab.yml  -p . -l cfg-clos\n</code></pre>"},{"location":"lab-examples/cfg-clos/#understanding-the-configuration-engine","title":"Understanding the Configuration Engine","text":"<p>The Configuration Engine of ContainerLab allows to prepare configuration templates, such that adding a new node in a topology requires only a little effort. The following steps will guide you through the files and their execution, to help understand the process behind it.</p>"},{"location":"lab-examples/cfg-clos/#a-declaring-variables","title":"a) Declaring variables","text":"<p>This topology contains multiple nodes, each one having its own specific aspects. Generating a different configuration for all of them at once seems a bit tricky. But with the usage of variables within the topology file, the Configuration Engine can easily customize templates for each device. Let's dissect the topology file.</p> <p>Several variable types are used in the topology to flexibly configure the nodes: global variables, node variables and link variables.</p> <p>The variables are scoped under <code>.vars</code> container which is present on all of the above mentioned levels.</p>"},{"location":"lab-examples/cfg-clos/#global-variables","title":"Global variables","text":"<pre><code>topology:\ndefaults:\nconfig:\nvars:\noverlay_as: 65555\n</code></pre>"},{"location":"lab-examples/cfg-clos/#node-variables","title":"Node variables","text":"<pre><code>topology:\nnodes:\ndcgw1:\nkind: vr-sros\ntype: sr-1\nconfig:\nvars:\nsystem_ip: 10.0.0.31\nas: 65030\n</code></pre>"},{"location":"lab-examples/cfg-clos/#link-variables","title":"Link variables","text":"<pre><code>topology:\nlinks:\n- endpoints: [\"dcgw1:eth1\",\"spine1:e1-31\"]\nvars:\nport: [1/1/c1, ethernet-1/31]\nclab_link_ip: 100.31.21.1/30\nbgp_underlay: true\n</code></pre> <p>Those defined variables are declared in the topology and then referenced directly in the templates. Note the usage of a magic variable in the link context - <code>clab_ip_link</code>.</p>"},{"location":"lab-examples/cfg-clos/#b-generating-variables","title":"b) Generating variables","text":"<p>Once the topology is defined and variables are placed in the relevant levels, the full list of variables can be retrieved and introspected:  <pre><code>$ containerlab config --topo cfg-clos.clab.yml template --vars\n</code></pre></p> <p>The following output represents the variables generated for <code>dcgw1</code> node: <pre><code>INFO[0000] dcgw1 vars = as: 65030\nclab_links:\n- bgp_underlay: true\n  clab_far:\n    bgp_underlay: true\n    clab_link_ip: 100.31.22.2/30\n    clab_link_name: to_dcgw1\n    clab_node: spine2\n    port: ethernet-1/31\n  clab_link_ip: 100.31.22.1/30\n  clab_link_name: to_spine2\n  port: 1/1/c2\n- bgp_underlay: true\n  clab_far:\n    bgp_underlay: true\n    clab_link_ip: 100.31.21.2/30\n    clab_link_name: to_dcgw1\n    clab_node: spine1\n    port: ethernet-1/31\n  clab_link_ip: 100.31.21.1/30\n  clab_link_name: to_spine1\n  port: 1/1/c1\nclab_node: dcgw1\nclab_nodes: '{leaf1: {...}, dcgw1: {...}, leaf4: {...}, spine2: {...}, spine1: {...},\n  leaf2: {...}, leaf3: {...}, sros-client: {...}, dcgw2: {...}, }'\nclab_role: vr-sros\noverlay_as: 65555\nsystem_ip: 10.0.0.31\n</code></pre></p> <p>This output is extremely helpful for a template designer, as it shows which variables and their values can be used in the configuration template.</p>"},{"location":"lab-examples/cfg-clos/#c-writing-templates","title":"c) Writing templates","text":"<p>Now that we have defined a topology and verified that output variables were correct, let's see how to use them in a template.</p> <p>This topology contains of leaves, spines, DCGWs and CE elements. Even though we have many nodes in the topology, as far as the node kinds are concerned, we only have two: srlinux and sros.</p> <p>Understandably, the configuration of SR Linux nodes differs from SR OS one, hence we will create two templates, one for SR Linux nodes and one for SR OS nodes.</p> <p>Containerlab config engine by default assumes that templates are created per containerlab kind, in our case, the kinds are: <code>srl</code> and <code>vr-sros</code>.</p> <p>So what we will do here is create two templates named <code>cfg-clos__srl.tmpl</code> and <code>cfg-clos_vr-sros.tmpl</code>. That way config engine will know which template to use against which node.</p> <p>Note</p> <p>Notice, how node's kind is encoded in the template name by suffixing the file name with <code>__$kindName</code>.</p> <p>The below section of <code>cfg-clos__srl.tmpl</code> template illustrates how each set of variables can be used to generate node's configuration.</p> <pre><code>{{/* If the bgp_underlay flag specified under the link then configure underlay ebgp on links */}}\n{{- range $name, $link := .clab_links -}}\n  {{- if .bgp_underlay }}\n/ network-instance default protocols bgp neighbor {{ ip $link.clab_far.clab_link_ip }}  peer-group underlay\n/ network-instance default protocols bgp neighbor {{ ip $link.clab_far.clab_link_ip }} peer-as {{(index $.clab_nodes $link.clab_far.clab_node).as}}\n  {{- end }} \n{{- end -}}\n</code></pre> <p><code>clab_links</code> contains all the links related to a node. <code>range</code> iterates on that variable and for each link, the existence of <code>bgp_underlay</code> variable is checked. If so, a peering is defined using the remote link IP address and AS number.</p> <p>Feel free to navigate through the templates, they will teach you how useful variables can be in this context.</p>"},{"location":"lab-examples/cfg-clos/#d-generating-configurations-from-templates","title":"d) Generating configurations from templates","text":"<p>Now that we have seen how variables are used, let's see the resulting configuration with : <pre><code>containerlab config -t cfg-clos.clab.yml template -p . -l cfg-clos\n</code></pre></p> <p>This command will dump the final configuration as it would be sent to the node in a later step. Make sure that the config appears to be correct before proceeding.</p>"},{"location":"lab-examples/cfg-clos/#e-applying-the-configurations","title":"e) Applying the configurations","text":"<p>To apply the templated configuration on the deployed nodes, simply use : <pre><code>containerlab config -t cfg-clos.clab.yml -p . -l cfg-clos\n</code></pre></p> <p>Containerlab will render the templates and use SSH client to connect to the nodes and apply the configuration.</p> <p>Note</p> <p>Entering in the configuration mode and commit steps are carried out by containerlab and are not part of the templates.</p> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/cvx01/","title":"Cumulus Linux and FRR","text":"Description Cumulus Linux connected back-to-back with FRR Components Cumulus Linux Resource requirements[^1]  1  1 GB Topology file topo.clab.yml Name cvx01 Version information[^2] <code>cvx:4.3.0</code> <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/cvx01/#description","title":"Description","text":"<p>The lab consists of Cumulus Linux and FRR nodes connected back-to-back over a point-to-point ethernet link. Both nodes are also connected to the clab docker network over their management <code>eth0</code> interfaces.</p>"},{"location":"lab-examples/cvx01/#configuration","title":"Configuration","text":"<p>Both nodes have been provided with a startup configuration and should come up with all their interfaces fully configured.</p> <p>Once the lab is started, the nodes will be able to ping each other:</p> <pre><code>$ ssh lab-cvx01-sw1\nWarning: Permanently added 'clab-cvx01-sw1,192.168.223.2' (ECDSA) to the list of known hosts.\nroot@clab-cvx01-sw1's password:\nLinux 1c3f259f31872500 4.19.0-cl-1-amd64 #1 SMP Cumulus 4.19.149-1+cl4.3u1 (2021-01-28) x86_64\nroot@1c3f259f31872500:mgmt:~# ping 12.12.12.2 -c 1\nvrf-wrapper.sh: switching to vrf \"default\"; use '--no-vrf-switch' to disable\nPING 12.12.12.2 (12.12.12.2) 56(84) bytes of data.\n64 bytes from 12.12.12.2: icmp_seq=1 ttl=64 time=0.400 ms\n\n--- 12.12.12.2 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.400/0.400/0.400/0.000 ms\n</code></pre>"},{"location":"lab-examples/cvx01/#use-cases","title":"Use cases","text":"<ul> <li>Demonstrate how a <code>cvx</code> node, running in its default <code>ignite</code> runtime, can connect to nodes running in other runtimes, e.g. <code>docker</code></li> <li>Demonstrate how to inject startup configuration into a <code>cvx</code> node.</li> <li>Verify basic control plane and data plane operations</li> </ul>"},{"location":"lab-examples/cvx02/","title":"Cumulus Linux (docker runtime) and Host","text":"Description Cumulus Linux in Docker runtime connected to a Host Components Cumulus Linux Resource requirements[^1]  1  1 GB Topology file topo.clab.yml Name cvx02 Version information[^2] <code>cvx:4.3.0</code> <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/cvx02/#description","title":"Description","text":"<p>The lab consists of Cumulus Linux running in its non-default, docker runtime, connected to a <code>linux</code> container playing the role of an end-host. Both nodes are also connected to the clab docker network over their management <code>eth0</code> interfaces.</p>"},{"location":"lab-examples/cvx02/#configuration","title":"Configuration","text":"<p>Both nodes have been provided with a startup configuration and should come up with all their interfaces fully configured.</p> <p>Once the lab is started, the nodes will be able to ping each other:</p> <pre><code>$  clab-cvx02-sw1\nWarning: Permanently added 'clab-cvx02-sw1,2001:172:20:20::3' (ECDSA) to the list of known hosts.\nroot@clab-cvx02-sw1's password:\nLinux sw1 5.10.16.3-networkop+ #17 SMP Mon May 24 15:22:51 BST 2021 x86_64\nroot@sw1:mgmt:~# ping 12.12.12.2\nvrf-wrapper.sh: switching to vrf \"default\"; use '--no-vrf-switch' to disable\nPING 12.12.12.2 (12.12.12.2) 56(84) bytes of data.\n64 bytes from 12.12.12.2: icmp_seq=1 ttl=64 time=0.297 ms\n^C\n--- 12.12.12.2 ping statistics ---\n1 packets transmitted, 1 received, 0% packet loss, time 0ms\nrtt min/avg/max/mdev = 0.297/0.297/0.297/0.000 ms\nroot@sw1:mgmt:~#\n</code></pre>"},{"location":"lab-examples/cvx02/#use-cases","title":"Use cases","text":"<ul> <li>Demonstrate how a <code>cvx</code> node can run in its non-default <code>docker</code> runtime</li> <li>Demonstrate how to inject startup configuration into a <code>cvx</code> node.</li> <li>Verify basic control plane and data plane operations</li> </ul>"},{"location":"lab-examples/ext-bridge/","title":"External bridge capability","text":"Description Connecting nodes via linux bridges Components Nokia SR Linux Resource requirements<sup>1</sup>  2  2 GB Topology file br01.clab.yml Name br01"},{"location":"lab-examples/ext-bridge/#description","title":"Description","text":"<p>This lab consists of three Nokia SR Linux nodes connected to a linux bridge.</p> <p></p> <p>Note</p> <p><code>containerlab</code> will not create/remove the bridge interface on your behalf.</p> <p>bridge element must be part of the lab nodes. Consult with the topology file to see how to reference a bridge.</p>"},{"location":"lab-examples/ext-bridge/#use-cases","title":"Use cases","text":"<p>By introducing a link of <code>bridge</code> type to the containerlab topology, we are opening ourselves to some additional scenarios:</p> <ul> <li>interconnect nodes via a broadcast domain</li> <li>connect multiple fabrics together</li> <li>connect containerlab nodes to the applications/nodes running outside of the lab host</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/frr01/","title":"FRR","text":"Description A 3-node ring of FRR routers with OSPF IGP Components FRR Resource requirements<sup>1</sup>  2  2 GB Topology file frr01.clab.yml Name frr01 Version information<sup>2</sup> <code>containerlab:0.13.0</code>, <code>frrouting/frr:v7.5.1</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/frr01/#description","title":"Description","text":"<p>This lab example consists of three FRR routers connected in a ring topology. Each router has one PC connected to it.</p> <p>This is also an example of how to pre-configure lab nodes of <code>linux</code> kind in containerlab.</p> <p>To start this lab, run the <code>run.sh</code> script, which will run the containerlab deploy commands, and then configure the PC interfaces.</p> <p>The lab configuration is documented in detail at: https://www.brianlinkletter.com/2021/05/use-containerlab-to-emulate-open-source-routers/</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/ixiacone-srl/","title":"Keysight IXIA-C and Nokia SR Linux","text":"Description A Keysight ixia-c-one node connected with Nokia SR Linux Components Keysight ixia-c-one, Nokia SR Linux Resource requirements<sup>1</sup>  2  2 GB Topology file ixiacone-srl.clab.yaml Name ixiac01 Version information<sup>2</sup> <code>containerlab:0.26.0</code>, <code>ixia-c-one:0.0.1-2738</code>, <code>srlinux:21.11.2</code>, <code>docker-ce:20.10.2</code>"},{"location":"lab-examples/ixiacone-srl/#description","title":"Description","text":"<p>This lab consists of a Keysight ixia-c-one node with 2 ports connected to 2 ports on a Nokia SR Linux node via two point-to-point ethernet links. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p> <p>Keysight ixia-c-one is a single-container distribution of ixia-c, which in turn is Keysight's reference implementation of Open Traffic Generator API. This example will demonstrate how test case designers can leverage Go SDK client gosnappi to configure ixia-c traffic generator and execute a test verifying IPv4 forwarding.</p>"},{"location":"lab-examples/ixiacone-srl/#use-cases","title":"Use cases","text":"<p>This lab allows users to validate an IPv4 traffic forwarding scenario between Keysight ixia-c-one and Nokia SR Linux.</p>"},{"location":"lab-examples/ixiacone-srl/#ipv4-traffic-forwarding","title":"IPv4 Traffic forwarding","text":"<p>This lab demonstrates a simple IPv4 traffic forwarding scenario where</p> <ul> <li>One Keysight ixia-c-one port acts as a transmit port (IP <code>1.1.1.1</code>) and the other as receive port (IP <code>2.2.2.2</code>)</li> <li>Nokia SR Linux is configured to forward the traffic destined for <code>20.20.20.0/24</code> to <code>2.2.2.2</code> using static route configuration in the default network instance</li> </ul>"},{"location":"lab-examples/ixiacone-srl/#configuration","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, users need to configure the lab nodes to forward and receive traffic.</p> SR LinuxKeysight ixia-c-one <p>SR Linux node comes up pre-configured with the commands listed in srl.cfg file which configure IPv4 addresses on both interfaces and install a static route to route the traffic coming from ixia-c.</p> <p>IPv4 addresses for data ports eth1/2 of ixia-c node are configured with <code>./ifcfg</code> scripts executed by containerlab on successful deployment<sup>3</sup>. These commands are listed in the topology file under <code>exec</code> node property.</p> <p>When a lab boots up, containerlab will also execute a command on SR Linux node to fetch MAC address of its <code>e1-1</code> interface which is connected to tx port of ixia-c-one. Write down this MAC address<sup>4</sup> as it will serve as an argument in the test script we will run afterwards.</p> <pre><code># partial output of `containerlab deploy` cmd that lists fetched MAC address\nINFO[0019] Executed command 'bash -c \"ip l show e1-1 | grep -o -E '([[:xdigit:]]{1,2}:){5}[[:xdigit:]]{1,2}' | head -1\"' on clab-ixiac01-srl. stdout:\n1a:b0:01:ff:00:01 </code></pre>"},{"location":"lab-examples/ixiacone-srl/#execution","title":"Execution","text":"<p>The test case is written in Go language hence Go &gt;= 1.17 needs to be installed first.</p> <p>Once installed, change into the lab directory:</p> <pre><code>cd /etc/containerlab/lab-examples/ixiac01\n</code></pre> <p>Run the test with MAC address obtained in previous step:</p> <pre><code>go run ipv4_forwarding.go -dstMac=\"&lt;MAC address&gt;\"\n</code></pre> <p>The test is configured to send 100 IPv4 packets with a rate 10pps from <code>10.10.10.1</code> to <code>10.20.20.x</code>, where <code>x</code> is changed from 1 to 5. Once 100 packets are sent, the test script checks that we received all the sent packets.</p> <p>During the test run you will see flow metrics reported each second with the current flow data such as:</p> <pre><code>2022/04/12 16:28:10 Metrics Response:\nchoice: flow_metrics\nflow_metrics:\n- bytes_rx: \"44032\"\n  bytes_tx: \"0\"\n  frames_rx: \"86\"\n  frames_rx_rate: 10\n  frames_tx: \"86\"\n  frames_tx_rate: 9\n  name: p1.v4.p2\n  transmit: started\n</code></pre>"},{"location":"lab-examples/ixiacone-srl/#verification","title":"Verification","text":"<p>The test that we ran above will continuously keep checking flow metrics to ensure packet count received on rx port of ixia-c-one are as expected. If the condition is not met in 10 seconds, the test will timeout, hence indicating failure.</p> <p>Upon success, last flow metrics output will indicate the latest status with <code>transmit</code> set to <code>stopped</code>.</p> <pre><code>2022/04/12 16:28:11 Metrics Response:\nchoice: flow_metrics\nflow_metrics:\n- bytes_rx: \"51200\"\nbytes_tx: \"0\"\nframes_rx: \"100\"\nframes_rx_rate: 9\nframes_tx: \"100\"\nframes_tx_rate: 10\nname: p1.v4.p2\ntransmit: stopped\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> <li> <p>Replace <code>add</code> with <code>del</code> to undo.\u00a0\u21a9</p> </li> <li> <p>The docker commands above shall not be required for upcoming releases of ixia-c-one with added ARP/ND capability.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/lab-examples/","title":"About lab examples","text":"<p><code>containerlab</code> aims to provide a simple, intuitive and yet customizable way to run container based labs. To help our users get a glimpse on the features containerlab packages, we ship some essential lab topologies within the <code>containerlab</code> package.</p> <p>Note</p> <p>The lab examples that you find on this site are merely explain the basics of containerlab. For the real-life labs built with containerlab check the clabs.netdevops.me catalog, where comprehensive labs are posted by the community members.</p> <p>These lab examples are meant to be used as-is or as a base layer to a more customized or elaborated lab scenarios. Once <code>containerlab</code> is installed, you will find the lab examples directories by the <code>/etc/containerlab/lab-examples</code> path.  Copy those directories over to your working directory to start using the provided labs.</p> <p>Container images versions</p> <p>Some lab examples may use the images without a tag, i.e. <code>image: srlinux</code>. This means that the image with a <code>latest</code> tag must exist. A user needs to tag the image themselves if the <code>latest</code> tag is missing.</p> <p>For example: <code>docker tag srlinux:20.6.1-286 srlinux:latest</code></p> <p>The source code of the lab examples is contained within the containerlab repo unless mentioned otherwise; any questions, issues or contributions related to the provided examples can be addressed via Github issues.</p> <p>Each lab comes with a definitive description that can be found in this documentation section.</p>"},{"location":"lab-examples/lab-examples/#how-to-deploy-a-lab-from-the-lab-catalog","title":"How to deploy a lab from the lab catalog?","text":"<p>Running the labs from the catalog is easy.</p>"},{"location":"lab-examples/lab-examples/#copy-lab-catalog","title":"Copy lab catalog","text":"<p>First, you need to copy the lab catalog to some place, for example to a current working directory. By copying labs from their original place we ensure that the changes we might make to the lab files will not be overwritten once we upgrade containerlab. To copy the entire catalog into your working directory:</p> <pre><code># copy over the srl02 lab files\ncp -a /etc/containerlab/lab-examples/* .\n</code></pre> <p>as a result of this command you will get several directories copied to the current working directory.</p> <p>Note</p> <p>Some big labs or community provided labs are typically stored in a separate git repository. To fetch those labs you will need to clone the lab' repo instead of copying the directories from <code>/etc/containerlab/lab-examples</code>.</p>"},{"location":"lab-examples/lab-examples/#get-the-lab-name","title":"Get the lab name","text":"<p>Every lab in the catalog has a unique short name. For example this lab states in the summary table that it's name is <code>srl02</code>. You will find a folder matching this name in your working directory, change into it: <pre><code>cd srl02\n</code></pre></p>"},{"location":"lab-examples/lab-examples/#check-images-and-licenses","title":"Check images and licenses","text":"<p>Within the lab directory you will find the files that are used in the lab. Usually, only the topology definition file and, sometimes, config files are present in the lab directory.</p> <p>If you check the topology file you will see if any license files are required and what images are specified for each node/kind.</p> <p>Either change the topology file to point to the right image/license or change the image/license to match the topo definition file values.</p>"},{"location":"lab-examples/lab-examples/#deploy-the-lab","title":"Deploy the lab","text":"<p>You are ready to deploy!</p> <pre><code>containerlab deploy -t &lt;topo-file&gt;\n</code></pre>"},{"location":"lab-examples/lab-examples/#ssh-access","title":"SSH access","text":"<p>For nodes that come up with <code>ssh</code> enabled, the following lines can be added to the <code>~/.ssh/config</code> file on the containerlab host system to simplify access and prevent future ssh key warnings:</p> <pre><code>Host clab-*\n  User root\n  StrictHostKeyChecking no\n  UserKnownHostsFile /dev/null\n</code></pre>"},{"location":"lab-examples/lab-examples/#public-clab-catalogs","title":"Public clab catalogs","text":"<p>As mentioned in the introduction of this article, the lab examples shipped with containerlab explain the features containerlab offers. The comprehensive lab examples are not part of containerlab installation as we want the community to own their work.</p> <p>Some well-known catalogs of clab based labs and/or individual submissions:</p> <ul> <li>clabs.netdevops.me</li> </ul>"},{"location":"lab-examples/min-5clos/","title":"5-stage Clos fabric","text":"Description A 5-stage CLOS topology based on Nokia SR Linux Components Nokia SR Linux Resource requirements<sup>1</sup>  4  8 GB Topology file clos02.clab.yml Name clos02"},{"location":"lab-examples/min-5clos/#description","title":"Description","text":"<p>This labs provides a lightweight folded 5-stage CLOS fabric with Super Spine level bridging two PODs.</p> <p>The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation.</p>"},{"location":"lab-examples/min-5clos/#use-cases","title":"Use cases","text":"<p>With this lightweight CLOS topology a user can exhibit the following scenarios:</p> <ul> <li>perform configuration tasks applied to the 5-stage CLOS fabric</li> <li>demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves</li> </ul>"},{"location":"lab-examples/min-5clos/#configuration-setup","title":"Configuration setup","text":"<p>To help you get faster to the provisioning of the services on this mini fabric we added an auto-configuration script to this example.</p> <p>In order to make a fully deterministic lab setup we added another topology file called setup.clos02.clab.yml where the management interfaces of each network node and clients are statically addressed with <code>mgmt_ipv4/6</code> config option. Other than that, the topology files does not have any changes.</p>"},{"location":"lab-examples/min-5clos/#prerequisites","title":"Prerequisites","text":"<p>The configuration of the fabric elements is carried out with <code>gnmic</code> client, therefore it needs to be installed on the machine where you run the lab.</p>"},{"location":"lab-examples/min-5clos/#run-instructions","title":"Run instructions","text":"<p>First deploy this topology as per usual:</p> <pre><code>containerlab deploy -t setup.clos02.clab.yml\n</code></pre> <p>Once the lab is deployed, execute the configuration script:</p> <pre><code>bash setup.sh\n</code></pre>"},{"location":"lab-examples/min-5clos/#configuration-schema","title":"Configuration schema","text":"<p>The setup script will use the following IP addresses across the nodes of the lab:</p> <p>The script will configure the following:</p> <ol> <li>IP addresses for Management, System and Link interfaces of leaves and spines.</li> <li>IP addresses for Clients eth0 (Management) and eth1 interfaces.</li> <li>BGP, ISIS &amp; OSPF protocols.</li> </ol> <p>The following table outlines the addressing plan used in this lab:</p> Source Interface Towards IPv4 IPv6 leaf1 mgmt0.0 - <code>172.100.100.2/24</code> <code>2001:172:100:100::2/64</code> system0.0 - <code>30.0.0.1/32</code> <code>3000:30:0:0::1/128</code> ethernet-1/1.0 spine1 <code>10.0.0.0/31</code> <code>1000:10:0:0::0/127</code> ethernet-1/2.0 spine2 <code>10.0.0.2/31</code> <code>1000:10:0:0::2/127</code> ethernet-1/3.0 client1 <code>10.0.0.24/31</code> <code>1000:10:0:0::24/127</code> leaf2 mgmt0.0 - <code>172.100.100.3/24</code> <code>2001:172:100:100::3/64</code> system0.0 - <code>30.0.0.2/32</code> <code>3000:30:0:0::2/128</code> ethernet-1/1.0 spine1 <code>10.0.0.4/31</code> <code>1000:10:0:0::4/127</code> ethernet-1/2.0 spine2 <code>10.0.0.6/31</code> <code>1000:10:0:0::6/127</code> ethernet-1/3.0 client2 <code>10.0.0.26/31</code> <code>1000:10:0:0::26/127</code> leaf3 mgmt0.0 - <code>172.100.100.4/24</code> <code>2001:172:100:100::4/64</code> system0.0 - <code>30.0.0.3/32</code> <code>3000:30:0:0::3/128</code> ethernet-1/1.0 spine3 <code>10.0.0.12/31</code> <code>1000:10:0:0::12/127</code> ethernet-1/2.0 spine4 <code>10.0.0.14/31</code> <code>1000:10:0:0::14/127</code> ethernet-1/3.0 client3 <code>10.0.0.28/31</code> <code>1000:10:0:0::28/127</code> leaf4 mgmt0.0 - <code>172.100.100.5/24</code> <code>2001:172:100:100::5/64</code> system0.0 - <code>30.0.0.4/32</code> <code>3000:30:0:0::4/128</code> ethernet-1/1.0 spine3 <code>10.0.0.16/31</code> <code>1000:10:0:0::16/127</code> ethernet-1/2.0 spine4 <code>10.0.0.18/31</code> <code>1000:10:0:0::18/127</code> ethernet-1/3.0 client4 <code>10.0.0.30/31</code> <code>1000:10:0:0::30/127</code> spine1 mgmt0.0 - <code>172.100.100.6/24</code> <code>2001:172:100:100::6/64</code> system0.0 - <code>30.0.0.5/32</code> <code>3000:30:0:0::5/128</code> ethernet-1/1.0 leaf1 <code>10.0.0.1/31</code> <code>1000:10:0:0::1/127</code> ethernet-1/2.0 leaf2 <code>10.0.0.5/31</code> <code>1000:10:0:0::5/127</code> ethernet-1/3.0 superspine1 <code>10.0.0.8/31</code> <code>1000:10:0:0::8/127</code> spine2 mgmt0.0 - <code>172.100.100.7/24</code> <code>2001:172:100:100::7/64</code> system0.0 - <code>30.0.0.6/32</code> <code>3000:30:0:0::6/128</code> ethernet-1/1.0 leaf1 <code>10.0.0.3/31</code> <code>1000:10:0:0::3/127</code> ethernet-1/2.0 leaf2 <code>10.0.0.7/31</code> <code>1000:10:0:0::7/127</code> ethernet-1/3.0 superspine2 <code>10.0.0.10/31</code> <code>1000:10:0:0::10/127</code> spine3 mgmt0.0 - <code>172.100.100.8/24</code> <code>2001:172:100:100::8/64</code> system0.0 - <code>30.0.0.7/32</code> <code>3000:30:0:0::7/128</code> ethernet-1/1.0 leaf3 <code>10.0.0.13/31</code> <code>1000:10:0:0::13/127</code> ethernet-1/2.0 leaf4 <code>10.0.0.17/31</code> <code>1000:10:0:0::17/127</code> ethernet-1/3.0 superspine1 <code>10.0.0.20/31</code> <code>1000:10:0:0::20/127</code> spine4 mgmt0.0 - <code>172.100.100.9/24</code> <code>2001:172:100:100::9/64</code> system0.0 - <code>30.0.0.8/32</code> <code>3000:30:0:0::8/128</code> ethernet-1/1.0 leaf3 <code>10.0.0.15/31</code> <code>1000:10:0:0::15/127</code> ethernet-1/2.0 leaf4 <code>10.0.0.19/31</code> <code>1000:10:0:0::19/127</code> ethernet-1/3.0 superspine2 <code>10.0.0.22/31</code> <code>1000:10:0:0::22/127</code> superspine1 mgmt0.0 - <code>172.100.100.10/24</code> <code>2001:172:100:100::10/64</code> system0.0 - <code>30.0.0.9/32</code> <code>3000:30:0:0::9/128</code> ethernet-1/1.0 spine1 <code>10.0.0.9/31</code> <code>1000:10:0:0::9/127</code> ethernet-1/2.0 spine3 <code>10.0.0.21/31</code> <code>1000:10:0:0::21/127</code> superspine2 mgmt0.0 - <code>172.100.100.11/24</code> <code>2001:172:100:100::11/64</code> system0.0 - <code>30.0.0.10/32</code> <code>3000:30:0:0::10/128</code> ethernet-1/1.0 spine2 <code>10.0.0.11/31</code> <code>1000:10:0:0::11/127</code> ethernet-1/2.0 spine4 <code>10.0.0.23/31</code> <code>1000:10:0:0::23/127</code> client1 eth0 - <code>172.100.100.12/24</code> <code>2001:172:100:100::12/64</code> eth1 leaf1 <code>10.0.0.25/31</code> <code>1000:10:0:0::25/127</code> client2 eth0 - <code>172.100.100.13/24</code> <code>2001:172:100:100::13/64</code> eth1 leaf2 <code>10.0.0.27/31</code> <code>1000:10:0:0::27/127</code> client3 eth0 - <code>172.100.100.14/24</code> <code>2001:172:100:100::14/64</code> eth1 leaf3 <code>10.0.0.29/31</code> <code>1000:10:0:0::29/127</code> client4 eth0 - <code>172.100.100.15/24</code> <code>2001:172:100:100::15/64</code> eth1 leaf4 <code>10.0.0.31/31</code> <code>1000:10:0:0::31/127</code> <p>Configuration snippets that are used to provision the nodes are contained within the <code>configs</code> subdirectory.</p> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/min-clos/","title":"3-nodes Clos fabric","text":"Description A minimal CLOS topology with two leafs and a spine Components Nokia SR Linux Resource requirements<sup>1</sup>  2  3 GB Topology file clos01.clab.yml Name clos01"},{"location":"lab-examples/min-clos/#description","title":"Description","text":"<p>This labs provides a lightweight folded CLOS fabric topology using a minimal set of nodes: two leaves and a single spine.</p> <p></p> <p>The topology is additionally equipped with the Linux containers connected to leaves to facilitate use cases which require access side emulation.</p>"},{"location":"lab-examples/min-clos/#use-cases","title":"Use cases","text":"<p>With this lightweight CLOS topology a user can exhibit the following scenarios:</p> <ul> <li>perform configuration tasks applied to the 3-stage CLOS fabric</li> <li>demonstrate fabric behavior leveraging the user-emulating linux containers attached to the leaves</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/multinode/","title":"Multi-node labs","text":"Description A lab demonstrating multi-node (multi-vm) capabilities Components Nokia SR OS, Juniper vMX Resource requirements<sup>1</sup>  2  6 GB per node Topology file vxlan-vmx.clab.yml, vxlan-sros.clab.yml Name vxlan01 Version information<sup>2</sup> <code>containerlab:0.11.0</code>, <code>vr-sros:20.2.R1</code>, <code>vr-vmx:20.4R1.12</code>, <code>docker-ce:20.10.2</code>"},{"location":"lab-examples/multinode/#description","title":"Description","text":"<p>This lab demonstrates how containerlab can deploy labs on different machines and stitch the interfaces of the running nodes via VxLAN tunnels.</p> <p>With such approach users are allowed to spread the load between multiple VMs and still have the nodes connected via p2p links as if they were sitting on the same virtual machine.</p> <p>For the sake of the demonstration the topology used in this lab consists of just two virtualized routers packaged in a container format - Nokia SR OS and Juniper vMX. Although the routers are running on different VMs, they logically form a back-to-back connection over a pair of interfaces aggregated in a logical bundle.</p> <p>Upon succesful lab deployment and configuration, the routers will be able to exchange LACP frames, thus proving a transparent L2 connectivity and will be able to ping each other.</p>"},{"location":"lab-examples/multinode/#deployment","title":"Deployment","text":"<p>Since this lab is of a multi-node nature, a user needs to have two machines/VMs and perform lab deployment process on each of them. The lab directory has topology files named <code>vxlan-sros.clab.yml</code> and <code>vxlan-vmx.clab.yml</code> which are meant to be deployed on VM1 and VM2 accordingly.</p> <p>The following command will deploy a lab on a specified host:</p> VM1 (SROS)VM2 (VMX) <pre><code>clab dep -t vxlan-sros.clab.yml\n</code></pre> <pre><code>clab dep -t vxlan-vmx.clab.yml\n</code></pre>"},{"location":"lab-examples/multinode/#host-links","title":"host links","text":"<p>Both topology files leverage host link feature which allows a container to have its interface to be connected to a container host namespace. Once the topology is created you will have one side of the veth link visible in the root namespace by the names specified in topo file. For example, <code>vxlan-sros.clab.yml</code> file has the following <code>links</code> section:</p> <pre><code>  links:\n# we expose two sros container interfaces\n# to host namespace by using host interfaces style\n# docs: https://containerlab.dev/manual/network/#host-links\n- endpoints: [\"sros:eth1\", \"host:sr-eth1\"]\n- endpoints: [\"sros:eth2\", \"host:sr-eth2\"]\n</code></pre> <p>This will effectively make two veth pairs. Let us consider the first veth pair where one end of a it will be placed inside the container' namespace and named <code>eth1</code>, the other end will stay in the container host root namespace and will be named <code>sros-eth1</code>.  </p> <p>Same picture will be on VM2 with vMX interfaces exposed to a container host.</p> verify host link VM1VM2 <pre><code>\u276f ip l | grep sros-eth\n622: sr-eth1@if623: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default 624: sr-eth2@if625: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default </code></pre> <pre><code>\u276f ip l | grep vmx-eth\n1982: vmx-eth1@if1983: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n1984: vmx-eth2@if1985: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default\n</code></pre>"},{"location":"lab-examples/multinode/#vxlan-tunneling","title":"vxlan tunneling","text":"<p>At this moment there is no connectivity between the routers, as the datapath is not ready. What we need to add is the VxLAN tunnels that will stitch SR OS container with vMX.</p> <p>We do this by provisioning VxLAN tunnels that will stitch the interfaces of our routers.</p> <p>Logically we make our interface appear to be connected in a point-to-point fashion. To make these tunnels we leverage containerlab' <code>tools vxlan create</code> command, that will create the VxLAN tunnel and the necessary redirection rules to forward traffic back-and-forth to a relevant host interface.</p> <p>All we need is to provide the VMs address and choose VNI numbers. And do this on both hosts.</p> VM1VM2 <pre><code>\u276f clab tools vxlan create --remote 10.0.0.20 --id 10 --link sr-eth1\n\n\u276f clab tools vxlan create --remote 10.0.0.20 --id 20 --link sr-eth2\n</code></pre> <pre><code>\u276f clab tools vxlan create --remote 10.0.0.18 --id 10 --link vmx-eth1\n\n\u276f clab tools vxlan create --remote 10.0.0.18 --id 20 --link vmx-eth2\n</code></pre> <p>The above set of commands will create the necessary VxLAN tunnels and the datapath is ready.</p> <p>At this moment, the connectivity diagrams becomes complete and can be depicted as follows:</p>"},{"location":"lab-examples/multinode/#configuration","title":"Configuration","text":"<p>Once the datapath is in place, we proceed with the configuration of a simple LACP use case, where both SR OS and vMX routers have their pair of interfaces aggregated into a LAG and form an LACP neighborship.</p> SR OSvMX <pre><code>configure lag \"lag-aggr\" admin-state enable\nconfigure lag \"lag-aggr\" mode hybrid\nconfigure lag \"lag-aggr\" lacp mode active\nconfigure lag \"lag-aggr\" port 1/1/c1/1\nconfigure lag \"lag-aggr\" port 1/1/c2/1\n\nconfigure port 1/1/c1 admin-state enable\nconfigure port 1/1/c1 connector breakout c1-100g\nconfigure port 1/1/c1/1 admin-state enable\nconfigure port 1/1/c1/1 ethernet\nconfigure port 1/1/c1/1 ethernet mode hybrid\n\nconfigure port 1/1/c2 admin-state enable\nconfigure port 1/1/c2 connector breakout c1-100g\nconfigure port 1/1/c2/1 admin-state enable\nconfigure port 1/1/c2/1 ethernet mode hybrid\n\nconfigure router \"Base\" interface \"toVMX\" port lag-aggr:0\nconfigure router \"Base\" interface \"toVMX\" ipv4 primary address 192.168.1.1 prefix-length 24\n</code></pre> <pre><code>set interfaces ge-0/0/0 gigether-options 802.3ad ae0\nset interfaces ge-0/0/1 gigether-options 802.3ad ae0\nset interfaces ae0 aggregated-ether-options minimum-links 1\nset interfaces ae0 aggregated-ether-options link-speed 1g\nset interfaces ae0 aggregated-ether-options lacp active\nset interfaces ae0 unit 0 family inet address 192.168.1.2/24\n</code></pre>"},{"location":"lab-examples/multinode/#verification","title":"Verification","text":"<p>To verify that LACP protocol works the following commands can be issued on both routers to display information about the aggregated interface and LACP status:</p> SR OSvMX <pre><code># verifying operational status of LAG interface\nA:admin@sros# show lag \"lag-aggr\"\n\n===============================================================================\nLag Data\n===============================================================================\nLag-id         Adm     Opr     Weighted Threshold Up-Count MC Act/Stdby\n    name\n-------------------------------------------------------------------------------\n65             up      up      No       0         2        N/A\n    lag-aggr\n===============================================================================\n\n# show LACP statistics. Both incoming and trasmitted counters will increase\nA:admin@sros# show lag \"lag-aggr\" lacp-statistics\n\n===============================================================================\nLAG LACP Statistics\n===============================================================================\nLAG-id    Port-id        Tx         Rx         Rx Error   Rx Illegal\n                        (Pdus)     (Pdus)     (Pdus)     (Pdus)\n-------------------------------------------------------------------------------\n65        1/1/c1/1       78642      77394      0          0\n65        1/1/c2/1       78644      77396      0          0\n-------------------------------------------------------------------------------\nTotals                   157286     154790     0          0\n===============================================================================\n</code></pre> <pre><code>admin@vmx&gt; show interfaces ae0 brief\nPhysical interface: ae0, Enabled, Physical link is Up\nLink-level type: Ethernet, MTU: 1514, Speed: 2Gbps, Loopback: Disabled, Source filtering: Disabled, Flow control: Disabled\nDevice flags   : Present Running\nInterface flags: SNMP-Traps Internal: 0x4000\n\nLogical interface ae0.0\n    Flags: Up SNMP-Traps 0x4004000 Encapsulation: ENET2\n    inet  192.168.1.2/24\n    multiservice\n\nadmin@vmx&gt; show lacp interfaces\nAggregated interface: ae0\n    LACP state:       Role   Exp   Def  Dist  Col  Syn  Aggr  Timeout  Activity\n    ge-0/0/0       Actor    No    No   Yes  Yes  Yes   Yes     Fast    Active\n    ge-0/0/0     Partner    No    No   Yes  Yes  Yes   Yes     Fast    Active\n    ge-0/0/1       Actor    No    No   Yes  Yes  Yes   Yes     Fast    Active\n    ge-0/0/1     Partner    No    No   Yes  Yes  Yes   Yes     Fast    Active\n    LACP protocol:        Receive State  Transmit State          Mux State\n    ge-0/0/0                  Current   Fast periodic Collecting distributing\n    ge-0/0/1                  Current   Fast periodic Collecting distributing\n\nadmin@vmx&gt; show lacp statistics interfaces ae0\nAggregated interface: ae0\n    LACP Statistics:       LACP Rx     LACP Tx   Unknown Rx   Illegal Rx\n    ge-0/0/0               78104       77469            0            0\n    ge-0/0/1               78106       77471            0            0\n</code></pre> <p>After the control plane verfification let's verify that the dataplane is working by pinging the IP address of the remote interface (issued from SR OS node in the example):</p> <pre><code>A:admin@sros# ping 192.168.1.2\nPING 192.168.1.2 56 data bytes\n64 bytes from 192.168.1.2: icmp_seq=1 ttl=64 time=13.5ms.\n64 bytes from 192.168.1.2: icmp_seq=2 ttl=64 time=2.61ms.\nping aborted by user\n\n---- 192.168.1.2 PING Statistics ----\n2 packets transmitted, 2 packets received, 0.00% packet loss\nround-trip min = 2.61ms, avg = 8.04ms, max = 13.5ms, stddev = 0.000ms\n</code></pre> <p>Great! Additionally users can capture the traffic from any of the interfaces involved in the datapath. To see the VxLAN encapsulation the VM's outgoing interfaces should be used.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/single-srl/","title":"Single SR Linux node","text":"Description a single Nokia SR Linux node Components Nokia SR Linux Resource requirements<sup>1</sup>  2  2 GB Topology file srl01.clab.yml Name srl01"},{"location":"lab-examples/single-srl/#description","title":"Description","text":"<p>A lab consists of a single SR Linux container equipped with a single interface - its management interface. No other network/data interfaces are created.</p> <p></p> <p>The SR Linux's <code>mgmt</code> interface is connected to the <code>containerlab</code> docker network that is created as part of the lab deployment process. The <code>mgmt</code> interface of SRL will get IPv4/6 address information via DHCP service provided by docker daemon.</p>"},{"location":"lab-examples/single-srl/#use-cases","title":"Use cases","text":"<p>This lightweight lab enables the users to perform the following exercises:</p> <ul> <li>get familiar with SR Linux architecture</li> <li>explore SR Linux extensible CLI</li> <li>navigate the SR Linux YANG tree</li> <li>play with gNMI<sup>2</sup> and JSON-RPC programmable interfaces</li> <li>write/debug/manage custom apps built for SR Linux NDK</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> <li> <p>Check out gnmic gNMI client to interact with SR Linux gNMI server.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-ceos/","title":"Nokia SR Linux and Arista cEOS","text":"Description A Nokia SR Linux connected back-to-back with Arista cEOS Components Nokia SR Linux, Arista cEOS Resource requirements<sup>1</sup>  2  2 GB Topology file srlceos01.clab.yml Name srlceos01 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>ceos:4.25.0F</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-ceos/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Arista cEOS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p>"},{"location":"lab-examples/srl-ceos/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Arista cEOS operating systems.</p>"},{"location":"lab-examples/srl-ceos/#bgp","title":"BGP","text":"<p>This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and Arista cEOS. Both nodes exchange NLRI with their loopback prefix making it reachable.</p>"},{"location":"lab-examples/srl-ceos/#configuration","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes.</p> srlceos <p>Get into SR Linux CLI with <code>docker exec -it clab-srlceos01-srl sr_cli</code> and start configuration <pre><code># enter candidate datastore\nenter candidate\n\n# configure loopback and data interfaces\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.1.1/24\n\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4 address 10.10.10.1/32\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface lo0.0\n\n# configure BGP\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp router-id 10.10.10.1\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp group ibgp export-policy export-lo\nset / network-instance default protocols bgp neighbor 192.168.1.2 admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-group ibgp\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-as 65001\n# create export policy\nset / routing-policy policy export-lo statement 10 match protocol local\nset / routing-policy policy export-lo statement 10 action accept\n\n# commit config\ncommit now\n</code></pre></p> <p>Get into cEOS CLI with <code>docker exec -it clab-srlceos01-ceos Cli</code> and start configuration <pre><code># enter configuration mode\nconfigure\nip routing\n\n# configure loopback and data interfaces\ninterface Ethernet1\n  no switchport\n  ip address 192.168.1.2/24\nexit\ninterface Loopback0\n  ip address 10.10.10.2/32\nexit\n# configure BGP\nrouter bgp 65001\nrouter-id 10.10.10.2\n  neighbor 192.168.1.1 remote-as 65001\nnetwork 10.10.10.2/32\nexit\n</code></pre></p>"},{"location":"lab-examples/srl-ceos/#verification","title":"Verification","text":"<p>Once BGP peering is established, the routes can be seen in GRT of both nodes:</p> srlceos <pre><code>A:srl# show network-instance default route-table ipv4-unicast summary | grep bgp\n| 10.10.10.2/32                 | 0     | true       | bgp             | 0       | 170   | 192.168.1.2 (indirect)                   | None              |\n</code></pre> <pre><code>ceos&gt;show ip route\n\nVRF: default\nCodes: C - connected, S - static, K - kernel,\n    O - OSPF, IA - OSPF inter area, E1 - OSPF external type 1,\n    E2 - OSPF external type 2, N1 - OSPF NSSA external type 1,\n    N2 - OSPF NSSA external type2, B - BGP, B I - iBGP, B E - eBGP,\n    R - RIP, I L1 - IS-IS level 1, I L2 - IS-IS level 2,\n    O3 - OSPFv3, A B - BGP Aggregate, A O - OSPF Summary,\n    NG - Nexthop Group Static Route, V - VXLAN Control Service,\n    DH - DHCP client installed default route, M - Martian,\n    DP - Dynamic Policy Route, L - VRF Leaked,\n    RC - Route Cache Route\n\nGateway of last resort:\nK        0.0.0.0/0 [40/0] via 172.20.20.1, Management0\n\nB I      10.10.10.1/32 [200/0] via 192.168.1.1, Ethernet1\nC        10.10.10.2/32 is directly connected, Loopback0\nC        172.20.20.0/24 is directly connected, Management0\nC        192.168.1.0/24 is directly connected, Ethernet1\n</code></pre> <p>Data plane confirms that routes have been programmed to FIB: <pre><code>A:srl# ping 10.10.10.2 network-instance default\nUsing network instance default\nPING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.\n64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=3.47 ms\n</code></pre></p> <p>[ceos]:</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-crpd/","title":"Nokia SR Linux and Juniper cRPD","text":"Description A Nokia SR Linux connected back-to-back with Juniper cRPD Components Nokia SR Linux, Juniper cRPD Resource requirements<sup>1</sup>  2  2 GB Topology file srlcrpd01.clab.yml Name srlcrpd01 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>crpd:20.2R1.10</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-crpd/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Juniper cRPD via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p>"},{"location":"lab-examples/srl-crpd/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper cRPD network operating systems.</p>"},{"location":"lab-examples/srl-crpd/#ospf","title":"OSPF","text":""},{"location":"lab-examples/srl-crpd/#configuration","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable OSPF on both nodes.</p> srlcrpd <p>Get into SR Linux CLI with <code>docker exec -it clab-srlcrpd01-srl sr_cli</code> and start configuration <pre><code># enter candidate datastore\nenter candidate\n\n# configure loopback and data interfaces\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.1.1/24\n\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4 address 10.10.10.1/32\n\n# configure OSPF\nset / network-instance default router-id 10.10.10.1\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface lo0.0\nset / network-instance default protocols ospf instance main admin-state enable\nset / network-instance default protocols ospf instance main version ospf-v2\nset / network-instance default protocols ospf instance main area 0.0.0.0 interface ethernet-1/1.0 interface-type point-to-point\nset / network-instance default protocols ospf instance main area 0.0.0.0 interface ethernet-1/1.0\n\n# commit config\ncommit now\n</code></pre></p> <p>cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process <code>bash</code> shell and configure interfaces: <code>docker exec -it clab-srlcrpd01-crpd bash</code> <pre><code># configure linux interfaces\nip addr add 192.168.1.2/24 dev eth1\nip addr add 10.10.10.2/32 dev lo\n</code></pre> Then launch the CLI and continue configuration <code>docker exec -it clab-srlcrpd01-crpd cli</code>: <pre><code># enter configuration mode\nconfigure\nset routing-options router-id 10.10.10.2\n\nset protocols ospf area 0.0.0.0 interface eth1 interface-type p2p\nset protocols ospf area 0.0.0.0 interface lo.0 interface-type nbma\n\n# commit configuration\ncommit\n</code></pre></p>"},{"location":"lab-examples/srl-crpd/#verificaton","title":"Verificaton","text":"<p>After the configuration is done on both nodes, verify the control plane by checking the route tables on both ends and ensuring dataplane was programmed as well by pinging the remote loopback</p> srlcrpd <p><pre><code># control plane verification\nA:srl# / show network-instance default route-table ipv4-unicast summary | grep ospf\n| 10.10.10.2/32                 | 0     | true       | ospfv2          | 1       | 10    | 192.168.1.2 (direct)                     | ethernet-1/1.0    |\n</code></pre> <pre><code># data plane verification\nA:srl# ping 10.10.10.2 network-instance default\nUsing network instance default\nPING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.\n64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms\n</code></pre></p> <pre><code># control plane verification\nroot@crpd&gt; show route | match OSPF\n10.10.10.1/32      *[OSPF/10] 00:01:24, metric 1\n224.0.0.5/32       *[OSPF/10] 00:05:49, metric 1\n</code></pre>"},{"location":"lab-examples/srl-crpd/#is-is","title":"IS-IS","text":""},{"location":"lab-examples/srl-crpd/#configuration_1","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable IS-IS on both nodes.</p> srlcrpd <p>Get into SR Linux CLI with <code>docker exec -it clab-srlcrpd01-srl sr_cli</code> and start configuration <pre><code># enter candidate datastore\nenter candidate\n\n# configure loopback and data interfaces\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.1.1/24\n\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4 address 10.10.10.1/32\n\n# configure IS-IS\nset / network-instance default router-id 10.10.10.1\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface lo0.0\nset / network-instance default protocols isis instance main admin-state enable\nset / network-instance default protocols isis instance main net [ 49.0001.0100.1001.0001.00 ]\nset / network-instance default protocols isis instance main interface ethernet-1/1.0 admin-state enable\nset / network-instance default protocols isis instance main interface ethernet-1/1.0 circuit-type point-to-point\nset / network-instance default protocols isis instance main interface lo0.0\n\n# commit config\ncommit now\n</code></pre></p> <p>cRPD configuration needs to be done both from the container process, as well as within the CLI. First attach to the container process <code>bash</code> shell and configure interfaces: <code>docker exec -it clab-srlcrpd01-crpd bash</code> <pre><code># configure linux interfaces\nip addr add 192.168.1.2/24 dev eth1\nip addr add 10.10.10.2/32 dev lo\n</code></pre> Then launch the CLI and continue configuration <code>docker exec -it clab-srlcrpd01-crpd cli</code>: <pre><code># enter configuration mode\nconfigure\nset interfaces lo0 unit 0 family iso address 49.0001.0100.1001.0002.00\nset routing-options router-id 10.10.10.2\n\nset protocols isis interface all point-to-point\nset protocols isis interface lo0.0\nset protocols isis level 1 wide-metrics-only\nset protocols isis level 2 wide-metrics-only\nset protocols isis reference-bandwidth 100g\n\n# commit configuration\ncommit\n</code></pre></p>"},{"location":"lab-examples/srl-crpd/#verification","title":"Verification","text":"srlcrpd <p><pre><code># control plane verification\nA:srl# / show network-instance default route-table ipv4-unicast summary | grep isis\n| 10.10.10.2/32                 | 0     | true       | isis            | 10      | 18    | 192.168.1.2 (direct)                     | ethernet-1/1.0    |\n| 172.20.20.0/24                | 0     | true       | isis            | 110     | 18    | 192.168.1.2 (direct)                     | ethernet-1/1.0    |\n</code></pre> <pre><code># data plane verification\nA:srl# ping 10.10.10.2 network-instance default\nUsing network instance default\nPING 10.10.10.2 (10.10.10.2) 56(84) bytes of data.\n64 bytes from 10.10.10.2: icmp_seq=1 ttl=64 time=1.15 ms\n</code></pre></p> <pre><code># control plane verification\nroot@crpd&gt; show route table inet.0 | match IS-IS\n10.10.10.1/32      *[IS-IS/18] 00:00:13, metric 100\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-frr/","title":"Nokia SR Linux and FRR","text":"Description A Nokia SR Linux connected back-to-back FRR router Components Nokia SR Linux, FRR Resource requirements<sup>1</sup>  2  2 GB Topology file srlfrr01.clab.yml Name srlfrr01 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>frrouting/frr:v7.5.0</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-frr/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with FRR router via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p>"},{"location":"lab-examples/srl-frr/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic control plane interoperability scenarios between Nokia SR Linux and FRR network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration. There you will find the config files to demonstrate a classic iBGP peering use case:</p> <ul> <li><code>daemons</code>: frr daemons config that is bind mounted to the frr container to trigger the start of the relevant FRR services</li> <li><code>frr.cfg</code>: vtysh config lines to configure a basic iBGP peering</li> <li><code>srl.cfg</code>: sr_cli config lines to configure a basic iBGP peering</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-sonic/","title":"Nokia SR Linux and SONiC","text":"Description A Nokia SR Linux connected back-to-back with SONiC-VS Components Nokia SR Linux, SONiC Resource requirements<sup>1</sup>  2  2 GB Topology file sonic01.clab.yml Name sonic01 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>docker-sonic-vs:202012</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-sonic/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Azure SONiC via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p>"},{"location":"lab-examples/srl-sonic/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and SONiC operating systems.</p>"},{"location":"lab-examples/srl-sonic/#bgp","title":"BGP","text":"<p>This lab demonstrates a simple iBGP peering scenario between Nokia SR Linux and SONiC. Both nodes exchange NLRI with their loopback prefix making it reachable.</p>"},{"location":"lab-examples/srl-sonic/#configuration","title":"Configuration","text":"<p>Once the lab is deployed with containerlab, use the following configuration instructions to make interfaces configuration and enable BGP on both nodes.</p> srlsonic <p>Get into SR Linux CLI with <code>docker exec -it clab-sonic01-srl sr_cli</code> and start configuration <pre><code># enter candidate datastore\nenter candidate\n\n# configure loopback and data interfaces\nset / interface ethernet-1/1 admin-state enable\nset / interface ethernet-1/1 subinterface 0 admin-state enable\nset / interface ethernet-1/1 subinterface 0 ipv4 address 192.168.1.1/24\n\nset / interface lo0 subinterface 0 admin-state enable\nset / interface lo0 subinterface 0 ipv4 address 10.10.10.1/32\nset / network-instance default interface ethernet-1/1.0\nset / network-instance default interface lo0.0\n\n# configure BGP\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp router-id 10.10.10.1\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp group ibgp export-policy export-lo\nset / network-instance default protocols bgp neighbor 192.168.1.2 admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-group ibgp\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-as 65001\n# create export policy\nset / routing-policy policy export-lo statement 10 match protocol local\nset / routing-policy policy export-lo statement 10 action accept\n\n# commit config\ncommit now\n</code></pre></p> <p>Get into sonic container shell with <code>docker exec -it clab-sonic01-sonic bash</code> and configure the so-called front-panel ports. Since we defined only one data interface for our sonic/srl nodes, we need to confgure a single port and a loopback interface: <pre><code>config interface ip add Ethernet0 192.168.1.2/24\nconfig interface startup Ethernet0\nconfig loopback add Loopback0\nconfig interface ip add Loopback0 10.10.10.2/32\nconfig interface startup Loopback0\n</code></pre> Now when data interface has been configured, check to make sure in /etc/frr/daemons that \"bgpd=yes\".  Restart the frr service if required and verify that bgpd is running. <pre><code>root@sonic:/# service frr restart\n[ ok ] Stopped watchfrr.\n[ ok . Stopped staticd[....] Stopped zebra[....] Stopped bgpd\n.\n.\n[ ok ] Started watchfrr.\nroot@sonic:/# service frr status\n[ ok ] Status of watchfrr: running.\n[ ok ] Status of zebra: running.\n[ ok ] Status of bgpd: running.\n[ ok ] Status of staticd: running.\n</code></pre> Then enter in the FRR shell to configure BGP by typing <code>vtysh</code> command inside the sonic container. <pre><code># enter configuration mode\nconfigure\n\n# configure BGP\nrouter bgp 65001\nbgp router-id 10.10.10.2\n  neighbor 192.168.1.1 remote-as 65001\naddress-family ipv4 unicast\n    network 10.10.10.2/32\n  exit-address-family\nexit\naccess-list all seq 5 permit any\n</code></pre></p>"},{"location":"lab-examples/srl-sonic/#verification","title":"Verification","text":"<p>Once BGP peering is established, the routes can be seen in GRT of both nodes:</p> srlsonic <pre><code>A:srl# / show network-instance default route-table ipv4-unicast summary | grep bgp\n| 10.10.10.2/32                 | 0     | true       | bgp             | 0       | 170   | 192.168.1.2 (indirect)                   | None              |\n</code></pre> <pre><code>sonic# sh ip route\nCodes: K - kernel route, C - connected, S - static, R - RIP,\n      O - OSPF, I - IS-IS, B - BGP, E - EIGRP, N - NHRP,\n      T - Table, v - VNC, V - VNC-Direct, A - Babel, D - SHARP,\n      F - PBR, f - OpenFabric,\n      &gt; - selected route, * - FIB route, q - queued route, r - rejected route\n\nK&gt;* 0.0.0.0/0 [0/0] via 172.20.20.1, eth0, 00:20:55\nB&gt;* 10.10.10.1/32 [200/0] via 192.168.1.1, Ethernet0, 00:01:51\nC&gt;* 10.10.10.2/32 is directly connected, Loopback0, 00:00:53\nC&gt;* 172.20.20.0/24 is directly connected, eth0, 00:20:55\nB   192.168.1.0/24 [200/0] via 192.168.1.0 inactive, 00:01:51\nC&gt;* 192.168.1.0/24 is directly connected, Ethernet0, 00:03:50\n</code></pre> <p>Data plane confirms that routes have been programmed to FIB: <pre><code>sonic# ping 10.10.10.1\nPING 10.10.10.1 (10.10.10.1) 56(84) bytes of data.\n64 bytes from 10.10.10.1: icmp_seq=1 ttl=64 time=2.28 ms\n64 bytes from 10.10.10.1: icmp_seq=2 ttl=64 time=2.84 ms\n</code></pre></p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/srl-xrd/","title":"Nokia SR Linux and Cisco XRd","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRd Components Nokia SR Linux, Cisco XRd Resource requirements<sup>1</sup>  2  4 GB Topology file srlxrd01.clab.yml Name srlxrd01 Version information<sup>2</sup> <code>containerlab:0.34.0</code>, <code>srlinux:22.11.1</code>, <code>xrd:7.8.1</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/srl-xrd/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Cisco XRd via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>containerlab</code> docker network.</p>"},{"location":"lab-examples/srl-xrd/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRd operating systems.</p>"},{"location":"lab-examples/srl-xrd/#configuration","title":"Configuration","text":"<p>Both SR Linux and XRd nodes come with a startup config files referenced for them. These user-defined startup files introduce the following change on top of the default config that these nodes boot with:</p> <ul> <li>On SR Linux, interface <code>ethernet-1/1</code> is configured with <code>192.168.0.0/31</code> address and this interface attached to the default network instance.</li> <li>On XRd, interface <code>Gi 0/0/0/0</code> is configured with <code>192.168.0.1/31</code> address.</li> </ul>"},{"location":"lab-examples/srl-xrd/#verification","title":"Verification","text":"<p>When the deployment of the lab finishes, users can validate that the datapath works between the nodes by pinging the directly connected interfaces from either node.</p> <p>Here is an example from SR Linux side:</p> <pre><code>--{ running }--[  ]--\nA:srl# ping network-instance default 192.168.0.1 Using network instance default\nPING 192.168.0.1 (192.168.0.1) 56(84) bytes of data.\n64 bytes from 192.168.0.1: icmp_seq=1 ttl=255 time=12.0 ms\n64 bytes from 192.168.0.1: icmp_seq=2 ttl=255 time=6.83 ms\n^C\n--- 192.168.0.1 ping statistics ---\n2 packets transmitted, 2 received, 0% packet loss, time 1001ms\nrtt min/avg/max/mdev = 6.830/9.428/12.027/2.600 ms\n</code></pre> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/templated01/","title":"Leaf-spine topology","text":"Description A Full Meshed X Leaf(s), Y Spine(s) Clos topology Components Nokia SR Linux Topology template file templated01.clab.gotmpl Topology variable file templated01.clab_vars.yaml Name templated01"},{"location":"lab-examples/templated01/#description","title":"Description","text":"<p>This lab consists of a customizable Leaf and Spine Clos topology. The number and type of SR Linux Leaf and Spine nodes is configurable, it can be set using the topology variable file <code>templated01.clab_vars.yaml</code>.</p> <p>The type of SR Linux used and the naming prefixes can be customized as well.</p> <pre><code>spines:\n# SRL spine type\ntype: ixrd3\n# number of spines\nnum: 2\n# prefix of spines name: ${prefix}${index}\nprefix: spine\nleaves:\n# SRL leaf type\ntype: ixrd3\n# number of leaves\nnum: 4\n# prefix of leaf name: ${prefix}${index}\nprefix: leaf\n</code></pre>"},{"location":"lab-examples/templated01/#configuration","title":"Configuration","text":"<p>Deploy the lab</p> <pre><code>clab deploy -t templated01.clab.gotmpl\n</code></pre> <p>Run <code>configure.sh</code> script to configure the lab</p> <pre><code>bash configure.sh\n</code></pre> <p>The <code>configure.sh</code> script relies on gomplate and gnmic.</p> <ul> <li>gomplate is used to generate the necessary configuration variables based on the number of spines and leaves, their type and prefix.</li> <li>gnmic is used to generate configuration payloads per node and push it using a gNMI Set RPC.</li> </ul>"},{"location":"lab-examples/templated02/","title":"5-stage Clos topology","text":"Description A 5-stage Clos topology with X Pod(s), Y Super Spine(s) Components Nokia SR Linux Topology template file templated02.clab.gotmpl Topology variable file templated02.clab_vars.yaml Name templated02"},{"location":"lab-examples/templated02/#description","title":"Description","text":"<p>This lab consists of a customizable 5 stage Clos topology. Each pod in this lab consists of a configurable number of fully meshed spines and leaves. The spines in each pod are connected to a configurable number of super spines.</p> <p>The topology template is rendered using the variable file shown below:</p> <pre><code>super_spines:\n# SRL super spine type\ntype: ixrd3\n# number of super spines\nnum: 2\n# prefix of super spines name: ${prefix}${index}\nprefix: super-spine\npods:\n# number of pods\nnum: 2\nspines:\n# SRL spine type\ntype: ixrd3\n# number of spines per pod\nnum: 2\n# prefix of spines name: ${prefix}${index}\nprefix: spine\nleaves:\n# SRL leaf type\ntype: ixrd2\n# number of leaves per pod\nnum: 4\n# prefix of leaf name: ${prefix}${index}\nprefix: leaf\n</code></pre>"},{"location":"lab-examples/templated02/#configuration","title":"Configuration","text":"<pre><code>clab deploy -t templated02.clab.gotmpl\n</code></pre> <p>Run <code>configure.sh</code> script to configure the lab</p> <pre><code>bash configure.sh\n</code></pre>"},{"location":"lab-examples/two-srls/","title":"Two SR Linux nodes","text":"Description Two Nokia SR Linux nodes Components Nokia SR Linux Resource requirements<sup>1</sup>  2  2 GB Topology file srl02.clab.yml Name srl02 Validated versions<sup>2</sup> <code>containerlab v0.26.2</code>,<code>srlinux:21.11.3</code>"},{"location":"lab-examples/two-srls/#description","title":"Description","text":"<p>A lab consists of two SR Linux nodes connected via a point-to-point link over <code>e1-1</code> interfaces. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p>"},{"location":"lab-examples/two-srls/#configuration","title":"Configuration","text":"<p>The nodes of this lab have been provided with a startup configuration using <code>startup-config</code> directive. The startup configuration adds loopback and interfaces addressing as per the diagram above.</p> <p>Once the lab is started, the nodes will be able to ping each other via configured interfaces:</p> <pre><code>--{ running }--[  ]--\nA:srl1# ping network-instance default 192.168.0.1\nUsing network instance default\nPING 192.168.0.1 (192.168.0.1) 56(84) bytes of data.\n64 bytes from 192.168.0.1: icmp_seq=1 ttl=64 time=55.2 ms\n64 bytes from 192.168.0.1: icmp_seq=2 ttl=64 time=6.61 ms\n64 bytes from 192.168.0.1: icmp_seq=3 ttl=64 time=8.92 ms\n64 bytes from 192.168.0.1: icmp_seq=4 ttl=64 time=14.2 ms\n^C\n--- 192.168.0.1 ping statistics ---\n4 packets transmitted, 4 received, 0% packet loss, time 3005ms\nrtt min/avg/max/mdev = 6.610/21.232/55.173/19.790 ms\n</code></pre>"},{"location":"lab-examples/two-srls/#use-cases","title":"Use cases","text":"<p>This lab, besides having the same objectives as srl01 lab, also enables the following scenarios:</p> <ul> <li>get to know protocols and services configuration</li> <li>verify basic control plane and data plane operations</li> <li>explore SR Linux state datastore for the paths which reflect control plane operation metrics or dataplane counters</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> <li> <p>versions of respective container images or software that was used to create the lab.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vr-sros/","title":"Nokia SR Linux and Nokia SR OS","text":"Description A Nokia SR Linux connected back-to-back with Nokia SR OS Components Nokia SR Linux, Nokia SR OS Resource requirements<sup>1</sup>  2  5 GB Topology file vr01.clab.yml Name vr01 Version information<sup>2</sup> <code>containerlab:0.27.1</code>, <code>srlinux:22.3.2</code>, <code>vr-sros:22.5.R1</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/vr-sros/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Nokia SR OS via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p> <p>Nokia SR OS VM is launched as a container, using vrnetlab integration.</p>"},{"location":"lab-examples/vr-sros/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Nokia SR OS network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vr-vmx/","title":"Nokia SR Linux and Juniper vMX","text":"Description A Nokia SR Linux connected back-to-back with Juniper vMX Components Nokia SR Linux, Juniper vMX Resource requirements<sup>1</sup>  2  8 GB Topology file vr02.clab.yml Name vr02 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>vr-vmx:20.2R1.10</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/vr-vmx/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Juniper vMX via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p> <p>Juniper vMX VM is launched as a container, using vrnetlab integration.</p>"},{"location":"lab-examples/vr-vmx/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Juniper vMX network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vr-xrv/","title":"Nokia SR Linux and Cisco XRv","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRv Components Nokia SR Linux, Cisco XRv Resource requirements<sup>1</sup>  1  3 GB Topology file vr03.clab.yml Name vr03 Version information<sup>2</sup> <code>containerlab:0.9.0</code>, <code>srlinux:20.6.3-145</code>, <code>vr-xrv:6.1.2</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/vr-xrv/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Cisco XRv via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p> <p>Cisco XRv VM is launched as a container, using vrnetlab integration.</p>"},{"location":"lab-examples/vr-xrv/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/vr-xrv9k/","title":"Nokia SR Linux and Cisco XRv9k","text":"Description A Nokia SR Linux connected back-to-back with Cisco XRv9k Components Nokia SR Linux, Cisco XRv9k Resource requirements<sup>1</sup>  2  12 GB Topology file vr04.clab.yml Name vr04 Version information<sup>2</sup> <code>containerlab:0.9.5</code>, <code>srlinux:20.6.3-145</code>, <code>vr-xrv9k:7.2.1</code>, <code>docker-ce:19.03.13</code>"},{"location":"lab-examples/vr-xrv9k/#description","title":"Description","text":"<p>A lab consists of an SR Linux node connected with Cisco XRv9k via a point-to-point ethernet link. Both nodes are also connected with their management interfaces to the <code>clab</code> docker network.</p> <p>Cisco XRv9k VM is launched as a container, using vrnetlab integration.</p>"},{"location":"lab-examples/vr-xrv9k/#use-cases","title":"Use cases","text":"<p>This lab allows users to launch basic interoperability scenarios between Nokia SR Linux and Cisco XRv9k network operating systems.</p> <p>The lab directory contains files with essential configurations which can be used to jumpstart the interop demonstration.</p> <ol> <li> <p>Resource requirements are provisional. Consult with the installation guides for additional information.\u00a0\u21a9</p> </li> <li> <p>The lab has been validated using these versions of the required tools/components. Using versions other than stated might lead to a non-operational setup process.\u00a0\u21a9</p> </li> </ol>"},{"location":"lab-examples/wan/","title":"WAN topology","text":"Description WAN emulating topology Components Nokia SR Linux Resource requirements<sup>1</sup>  2  3 GB Topology file srl03.clab.yml Name srl03"},{"location":"lab-examples/wan/#description","title":"Description","text":"<p>Nokia SR Linux while focusing on the data center deployments in the first releases, will also be suitable for WAN deployments. In this lab users presented with a small WAN topology of four interconnected SR Linux nodes with multiple p2p interfaces between them.</p> <p></p>"},{"location":"lab-examples/wan/#use-cases","title":"Use cases","text":"<p>The WAN-centric scenarios can be tested with this lab:</p> <ul> <li>Link aggregation</li> <li>WAN protocols and features</li> </ul> <ol> <li> <p>Resource requirements are provisional. Consult with SR Linux Software Installation guide for additional information.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/cert/","title":"Certificate management","text":"<p>As more and more services move to \"secure by default\" behavior, it becomes important to simplify the PKI/TLS infrastructure provisioning in the lab environments. Containerlab embeds parts of cfssl project to automate certificate generation and provisioning.</p> <p>For SR Linux nodes containerlab creates Certificate Authority (CA) and generates signed cert and key for each node of a lab. This makes SR Linux node to boot up with TLS profiles correctly configured and enable operation of a secured management protocol - gNMI.</p> <p>Note</p> <p>For other nodes the automated TLS pipeline is not provided yet and can be addressed by contributors.</p> <p>Apart from automated pipeline for certificate provisioning, containerlab exposes the following commands that can create a CA and node's cert/key:</p> <ul> <li><code>tools cert ca create</code> - creates a Certificate Authority</li> <li><code>tools cert sign</code> - creates certificate/key for a host and signs the certificate with CA</li> </ul> <p>With these two commands users can easily create CA node certificates and secure the transport channel of various protocols. This lab demonstrates how with containerlab's help one can easily create certificates and configure Nokia SR OS to use it for secured gNMI communication.</p>"},{"location":"manual/conf-artifacts/","title":"Configuration artifacts","text":"<p>When containerlab deploys a lab it creates a Lab Directory in the current working directory. This directory is used to keep all the necessary files that are needed to run/configure the nodes. We call these files configuration artifacts.</p> <p>Things like:</p> <ul> <li>CA certificate and node' TLS certificate and private keys</li> <li>node config file (if applicable and supported by the kind)</li> <li>node-specific files and directories that are required to launch the container</li> <li>license files if needed</li> </ul> <p>all these artifacts will be available under a Lab Directory.</p> <p>Note</p> <p>If you configure a node with <code>binds</code> mounts and the source of the bind is not within the lab directory already, containerlab will copy over the source files/dirs into the lab directory on users behalf.</p>"},{"location":"manual/conf-artifacts/#identifying-a-lab-directory","title":"Identifying a lab directory","text":"<p>The lab directory name follows the <code>clab-&lt;lab_name&gt;</code> template. Thus, if the name of your lab is <code>srl02</code> you will find the <code>clab-srl02</code> directory created in the current working directory.</p> <pre><code>\u276f ls -lah clab-srl02\ntotal 4.0K\ndrwxr-xr-x  5 root root   40 Dec  1 22:11 .\ndrwxr-xr-x 23 root root 4.0K Dec  1 22:11 ..\ndrwxr-xr-x  5 root root   42 Dec  1 22:11 .tls\ndrwxr-xr-x  3 root root   79 Dec  1 22:11 srl1\ndrwxr-xr-x  3 root root   79 Dec  1 22:11 srl2\n</code></pre> <p>The contents of this directory will contain kind-specific files and directories. Containerlab will name directories after the node names and will only created those if they are needed. For instance, by default any node of kind <code>linux</code> will not have it's own directory under the Lab Directory.</p>"},{"location":"manual/conf-artifacts/#persistance-of-a-lab-directory","title":"Persistance of a lab directory","text":"<p>When a user first deploy a lab, the Lab Directory gets created if it was not present. Depending on a node's kind, this directory might act as a persistent storage area for a node. A common case is having the configuration file saved when the changes are made to the node via management interfaces.</p> <p>Below is an example of the <code>srl1</code> node directory contents. It keeps a directory that is mounted to containers configuration path, as well as stores additional files needed to launch and configure the node.</p> <pre><code>~/clab/clab-srl02\n\u276f ls -lah srl1\ndrwxrwxrwx+ 6 1002 1002   87 Dec  1 22:11 config\n-rw-r--r--  1 root root 2.8K Dec  1 22:11 license.key\n-rw-r--r--  1 root root 4.4K Dec  1 22:11 srlinux.conf\n-rw-r--r--  1 root root  233 Dec  1 22:11 topology.clab.yml\n</code></pre> <p>When a user destroys a lab without providing the <code>--cleanup</code> flag to the <code>destroy</code> command, the Lab Directory does not get deleted. This means that every configuration artifact will be kept on disk.</p> <p>Moreover, when the user will deploy the same lab, containerlab will reuse the configuration artifacts if possible, which will, for example, start the nodes with the config files saved from the previous lab run.</p> <p>To be able to deploy a lab without reusing existing configuration artifact use the <code>--reconfigure</code> flag with <code>deploy</code> command. With that setting, containerlab will first delete the Lab Directory and then will start the deployment process.</p>"},{"location":"manual/images/","title":"Image management","text":"<p>For a traditional networking lab orchestration system <code>containerlab</code> appears to be quite unique in a way that it runs containers, not VMs. This inherently means that container images need to be available to spin up the nodes.</p> <p>To keep things simple, containerlab adheres to the same principles of referencing container images as common tools like docker, podman, k8s do. The following example shows a clab file that references container images using various forms:</p> <pre><code>name: images\ntopology:\nnodes:\nnode1:\n# image from docker hub registry with implicit `latest` tag\nimage: alpine\nnode2:\n# image from docker hub with explicit tag\nimage: ubuntu:20.04\nnode3:\n# image from github registry\nimage: ghcr.io/hellt/network-multitool\nnode4:\n# image from some private registry\nimage: myregistry.local/private/alpine:custom\n</code></pre> <p>When containerlab launches a lab, it reads the image name from the topology file and expects to find the referenced images locally or by pulling them from the registry.</p> <p>If in the example above, the image named <code>myregistry.local/private/alpine:custom</code> was not loaded to docker local image store before, containerlab will attempt to pull this image and will expect the private registry to be reachable.</p> <p>Container images offer a great flexibility and reproducibility of lab builds, to embrace it fully, we wanted to capture some basic image management operations and workflows in this article.</p>"},{"location":"manual/images/#tagging-images","title":"Tagging images","text":"<p>A container image name can appear in various forms. A short form of <code>alpine</code> will be expanded by docker daemon to <code>docker.io/alpine:latest</code>. At the same time an image named <code>myregistry.local/private/alpine:custom</code> is already a fully qualified name and indicates the container registry (<code>myregistry.local</code>) image repository name (<code>private/alpine</code>) and its tag (<code>custom</code>).</p> <p>With a <code>docker tag</code> command it is possible to \"rename\" an image to something else. This can be needed for various purposes, but most common needs are:</p> <ol> <li>rename the image so it can be pushed to another repository</li> <li>rename the image to users liking</li> </ol> <p>Let's imagine that we have a private repository from which we pulled the image with a name <code>registry.srlinux.dev/pub/vr-sros:20.10.R3</code>. By using this name in our clab file we can make use of this image in our lab. But that is quite a lengthy name, we might want to shorten it to something less verbose:</p> <pre><code># docker tag &lt;old-name&gt; &lt;new-name&gt;\ndocker tag registry.srlinux.dev/pub/vr-sros:20.10.R3 sros:20.10.R3\n</code></pre> <p>With that we make a new image named <code>sros:20.10.R3</code> that references the same original image. Now we can use the short name in our clab files.</p>"},{"location":"manual/images/#pushing-to-a-new-registry","title":"Pushing to a new registry","text":"<p>Same <code>docker tag</code> command can be used to rename the image so it can be pushed to another registry. For example consider the newly built SR OS 21.2.R1 vrnetlab image that by default will have a name of <code>vrnetlab/vr-sros:21.2.R1</code>. This container image can't be pushed anywhere in its current form, but retagging will help us out.</p> <p>If we wanted to push this image to a public registry like Github Container Registry, we could do the following:</p> <pre><code># retag the image to a fully qualified name that is suitable for\n# push to github container registry\ndocker tag vrnetlab/vr-sros:21.2.R1 ghcr.io/srl-labs/vr-sros:21.2.R1\n\n# and now we can push it\ndocker push ghcr.io/srl-labs/vr-sros:21.2.R1\n</code></pre>"},{"location":"manual/images/#exchanging-images","title":"Exchanging images","text":"<p>Container images are a perfect fit for sharing. Once anyone built an image with a certain NOS inside it can share it with anyone via container registry. Sensitive and proprietary images are typically pushed to private registries and internal users pull it from there.</p> <p>But sometimes you need to share an image with a colleague or your own setup that doesn't have access to a private registry. There are couple of ways to achieve that.</p>"},{"location":"manual/images/#as-tgz-archive","title":"As tgz archive","text":"<p>A container image can be saved as <code>tar.gz</code> file that you can then share via various channels:</p> <pre><code>docker save vrnetlab/vr-sros:21.2.R1 | gzip &gt; sros.tar.gz\n</code></pre> <p>Now you can push the tar.gz file to Google Drive, Dropbox, etc.</p> <p>On the receiving end you can load the container image:</p> <pre><code>docker load -i sros.tar.gz\n</code></pre>"},{"location":"manual/images/#via-temp-registry","title":"Via temp registry","text":"<p>Another cool way of sharing a container image is via ttl.sh registry which offers a way to push an image to their public registry but the image will expire with a timeout you set.</p> <p>For example, let's push our image to the ttl.sh registry under a random name and make it expire in 15 minutes.</p> <pre><code># generate random 6 char sequence\nIMAGE=$(cat /dev/urandom | tr -dc 'a-z0-9' | fold -w 6 | head -n 1)\n# set ttl\nTTL=15m\n\n# tag and push\ndocker tag vrnetlab/vr-sros:21.2.R1 ttl.sh/$IMAGE:$TTL\ndocker push ttl.sh/$IMAGE:$TTL\necho \"pull the image with \\\"docker pull ttl.sh/$IMAGE:$TTL\\\" in the next $TTL\"\n</code></pre> <p>That is a very convenient way of sharing images with a small security compromise.</p>"},{"location":"manual/inventory/","title":"Inventory","text":"<p>To accommodate for smooth transition from lab deployment to subsequent automation activities, containerlab generates inventory files for different automation tools.</p>"},{"location":"manual/inventory/#ansible","title":"Ansible","text":"<p>Ansible inventory is generated automatically for every lab. The inventory file can be found in the lab directory under the <code>ansible-inventory.yml</code> name.</p> <p>Lab nodes are grouped under their kinds in the inventory so that the users can selectively choose the right group of nodes in the playbooks.</p> topology filegenerated ansible inventory <pre><code>name: ansible\ntopology:\nnodes:\nr1:\nkind: crpd\nimage: crpd:latest\nr2:\nkind: ceos\nimage: ceos:latest\nr3:\nkind: ceos\nimage: ceos:latest\ngrafana:\nkind: linux\nimage: grafana/grafana:7.4.3\n</code></pre> <pre><code>all:\nchildren:\ncrpd:\nhosts:\nclab-ansible-r1:\nansible_host: &lt;mgmt-ipv4-address&gt;\nceos:\nhosts:\nclab-ansible-r2:\nansible_host: &lt;mgmt-ipv4-address&gt;\nclab-ansible-r3:\nansible_host: &lt;mgmt-ipv4-address&gt;\nlinux:\nhosts:\nclab-ansible-grafana:\nansible_host: &lt;mgmt-ipv4-address&gt;\n</code></pre>"},{"location":"manual/inventory/#removing-ansible_host-var","title":"Removing <code>ansible_host</code> var","text":"<p>If you want to use a plugin<sup>1</sup> that doesn't play well with the <code>ansible_host</code> variable injected by containerlab in the inventory file, you can leverage the <code>ansible-no-host-var</code> label. The label can be set on per-node, kind, or default levels; if set, containerlab will not generate the <code>ansible_host</code> variable in the inventory for the nodes with that label. Note that without the <code>ansible_host</code> variable, the connection plugin will use the <code>inventory_hostname</code> and resolve the name accordingly if network reachability is needed.</p> topology filegenerated ansible inventory <pre><code>name: ansible\ntopology:\ndefaults:\nlabels:\nansible-no-host-var: \"true\"\nnodes:\nnode1:\nnode2:\n</code></pre> <pre><code>all:\nchildren:\nlinux:\nhosts:\nclab-ansible-node1:\nclab-ansible-node2:\n</code></pre>"},{"location":"manual/inventory/#user-defined-groups","title":"User-defined groups","text":"<p>Users can enforce custom grouping of nodes in the inventory by adding the <code>ansible-inventory</code> label to the node definition:</p> <pre><code>name: custom-groups\ntopology:\nnodes:\nnode1:\n# &lt;some node config data&gt;\nlabels:\nansible-group: spine\nnode2:\n# &lt;some node config data&gt;\nlabels:\nansible-group: extra_group\n</code></pre> <p>As a result of this configuration, the generated inventory will look like this:</p> <pre><code>  children:\nsrl:\nhosts:\nclab-custom-groups-node1:\nansible_host: 172.100.100.11\nclab-custom-groups-node2:\nansible_host: 172.100.100.12\nextra_group:\nhosts:\nclab-custom-groups-node2:\nansible_host: 172.100.100.12\nspine:\nhosts:\nclab-custom-groups-node1:\nansible_host: 172.100.100.11\n</code></pre>"},{"location":"manual/inventory/#topology-data","title":"Topology Data","text":"<p>Every time a user runs a <code>deploy</code> command, containerlab automatically exports information about the topology into <code>topology-data.json</code> file in the lab directory. Schema of exported data is determined based on a Go template specified in <code>--export-template</code> parameter, or a default template <code>/etc/containerlab/templates/export/auto.tmpl</code>, if the parameter is not provided.</p> <p>Containerlab internal data that is submitted for export via the template, has the following structure:</p> <pre><code>type TopologyExport struct {\nName        string                       `json:\"name\"`                  // Containerlab topology name\nType        string                       `json:\"type\"`                  // Always 'clab'\nClab        *CLab                        `json:\"clab,omitempty\"`        // Data parsed from a topology definitions yaml file\nNodeConfigs map[string]*types.NodeConfig `json:\"nodeconfigs,omitempty\"` // Definitions of nodes expanded with dynamically created data\n}\n</code></pre> <p>To get the full list of fields available for export, you can export topology data with the following template <code>--export-template /etc/containerlab/templates/export/full.tmpl</code>. Note, some fields exported via <code>full.tmpl</code> might contain sensitive information like TLS private keys. To customize export data, it is recommended to start with a copy of <code>auto.tmpl</code> and change it according to your needs.</p> <p>Example of exported data when using default <code>auto.tmpl</code> template:</p> topology file srl02.clab.ymlsample generated topology-data.json <pre><code>name: srl02\ntopology:\nkinds:\nsrl:\ntype: ixrd3\nimage: ghcr.io/nokia/srlinux\nnodes:\nsrl1:\nkind: srl\nsrl2:\nkind: srl\nlinks:\n- endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n</code></pre> <pre><code>{\n\"name\": \"srl02\",\n\"type\": \"clab\",\n\"clab\": {\n\"config\": {\n\"prefix\": \"clab\",\n\"mgmt\": {\n\"network\": \"clab\",\n\"bridge\": \"br-&lt;...&gt;\",\n\"ipv4-subnet\": \"172.20.20.0/24\",\n\"ipv6-subnet\": \"2001:172:20:20::/64\",\n\"mtu\": \"1500\",\n\"external-access\": true\n},\n\"config-path\": \"&lt;full path to a directory with srl02.clab.yml&gt;\"\n}\n},\n\"nodes\": {\n\"srl1\": {\n\"index\": \"0\",\n\"shortname\": \"srl1\",\n\"longname\": \"clab-srl02-srl1\",\n\"fqdn\": \"srl1.srl02.io\",\n\"group\": \"\",\n\"labdir\": \"&lt;full path to the lab node directory&gt;\",\n\"kind\": \"srl\",\n\"image\": \"ghcr.io/nokia/srlinux\",\n\"mgmt-net\": \"\",\n\"mgmt-intf\": \"\",\n\"mgmt-ipv4-address\": \"172.20.20.3\",\n\"mgmt-ipv4-prefix-length\": 24,\n\"mgmt-ipv6-address\": \"2001:172:20:20::3\",\n\"mgmt-ipv6-prefix-length\": 64,\n\"mac-address\": \"\",\n\"labels\": {\n\"clab-mgmt-net-bridge\": \"br-&lt;...&gt;\",\n\"clab-node-group\": \"\",\n\"clab-node-kind\": \"srl\",\n\"clab-node-lab-dir\": \"&lt;full path to the lab node directory&gt;\",\n\"clab-node-name\": \"srl1\",\n\"clab-node-type\": \"ixrd3\",\n\"clab-topo-file\": \"&lt;full path to the srl02.clab.yml file&gt;\",\n\"containerlab\": \"srl02\"\n}\n},\n\"srl2\": {\n\"index\": \"1\",\n\"shortname\": \"srl2\",\n\"longname\": \"clab-srl02-srl2\",\n\"fqdn\": \"srl2.srl02.io\",\n\"group\": \"\",\n\"labdir\": \"&lt;full path to the lab node directory&gt;\",\n\"kind\": \"srl\",\n\"image\": \"ghcr.io/nokia/srlinux\",\n\"mgmt-net\": \"\",\n\"mgmt-intf\": \"\",\n\"mgmt-ipv4-address\": \"172.20.20.2\",\n\"mgmt-ipv4-prefix-length\": 24,\n\"mgmt-ipv6-address\": \"2001:172:20:20::2\",\n\"mgmt-ipv6-prefix-length\": 64,\n\"mac-address\": \"\",\n\"labels\": {\n\"clab-mgmt-net-bridge\": \"br-&lt;...&gt;\",\n\"clab-node-group\": \"\",\n\"clab-node-kind\": \"srl\",\n\"clab-node-lab-dir\": \"&lt;full path to the lab node directory&gt;\",\n\"clab-node-name\": \"srl2\",\n\"clab-node-type\": \"ixrd3\",\n\"clab-topo-file\": \"&lt;full path to the srl02.clab.yml file&gt;\",\n\"containerlab\": \"srl02\"\n}\n}\n},\n\"links\": [\n{\n\"a\": {\n\"node\": \"srl1\",\n\"interface\": \"e1-1\",\n\"mac\": \"&lt;mac address&gt;\",\n\"peer\": \"z\"\n},\n\"z\": {\n\"node\": \"srl2\",\n\"interface\": \"e1-1\",\n\"mac\": \"&lt;mac address&gt;\",\n\"peer\": \"a\"\n}\n}\n]\n}\n</code></pre> <ol> <li> <p>For example Ansible Docker connection plugin.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/multi-node/","title":"Multi-node labs","text":"<p>Containerlab is a perfect tool of choice when all the lab components/nodes fit into one VM or bare metal server. Unfortunately, sometimes it is hard to satisfy this requirement and fit a big and sophisticated lab on a single host.</p> <p>Although containerlab is not (yet) capable of deploying topologies over a number of container hosts, we have embedded some capabilities that can help you to workaround the single-host resources constraint.</p>"},{"location":"manual/multi-node/#exposing-services","title":"Exposing services","text":"<p>Sometimes all that is needed is to make certain services running inside the nodes launched with containerlab available to a system running outside of the container host. For example, you might have an already running telemetry stack somewhere in your lab and you want to use it with the routing systems deployed with containerlab.</p> <p>In that case, the simple solution would be to expose the nodes' ports which are used to collect telemetry information. Take a look the following example where two nodes are defined in the topology file and get their gNMI port exposed to a host under a user-defined host-port.</p> <pre><code>name: telemetry\ntopology:\nnodes:\nceos:\nkind: ceos\nimage: ceos:latest\nports:\n# host port 57401 is mapped to port 57400 of ceos node\n- 57401:57400\nsrl:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nports:\n- 57402:57400\nlinks:\n- endpoints: [\"ceos:eth1\", \"srl:e1-1\"]\n</code></pre> <p>Once the container's ports/services are exposed to a host under host-port, the telemetry collector running outside of the container host system can reach each node gNMI service.</p> <p>If container host has IP address of <code>$IP</code>, then telemetry collector can reach <code>ceos</code> telemetry service by <code>$IP:57401</code> address and <code>srl</code> gNMI service will be reachable via <code>$IP:57402</code>.</p>"},{"location":"manual/multi-node/#exposing-management-network","title":"Exposing management network","text":"<p>Exposing services on a per-port basis as shown above is a quick and easy way to make a certain service available via a host port, likely being the most common way of exposing services with containerlab. Unfortunately, not every use case can be covered with such approach.</p> <p>Imagine if you want to integrate an NMS system running elsewhere with a lab you launched with containerlab. Typically you would need to expose the entire management network for an NMS to start managing the nodes with management protocols required. In this scenario you wouldn't get far with exposing services via host-ports, as NMS would expect to have IP connectivity with the node it is about to adopt for managing.</p> <p>For integration tasks like this containerlab users can leverage static routing towards containerlab management network. Consider the following diagram:</p> <p>This solution requires to set up routing between the host which runs the NMS and the container host that has containerlab nodes inside. Since containers are always attached to a common management network, we can make this network reachable by installing, for example, a static route on the NMS host. This will provision the datapath between the NMS and the containerlab management network.</p> <p>By default, containerlab management network is addressed with <code>172.20.20./0</code> IPv4 address, but this can be easily changed to accommodate for network environment.</p>"},{"location":"manual/multi-node/#bridging","title":"Bridging","text":"<p>Previous examples were aiming management network access, but what if we need to rather connect a network interfaces of a certain node with a system running outside of the container host? An example for such connectivity requirement could be a traffic generator connected to a containerized node port.</p> <p>In this case we can leverage the bridge kind<sup>1</sup> that containerlab offers to connect container' interface to a pre-created bridge and slice the network with VLANs to create a L2 connectivity between the ports:</p>"},{"location":"manual/multi-node/#vxlan-tunneling","title":"VxLAN Tunneling","text":"<p>Sometimes VLAN bridging is not possible, for example when the other end of the virtual wire is reachable via routing, not bridging. We have developed a semi-automated solution for this case as well.</p> <p>The idea is to create unicast VxLAN tunnels between the VMs hosting nodes requiring connectivity.</p> <p>Refer to the multinode lab that goes deep in details on how to create this tunneling and explains the technicalities of such dataplane. </p> <ol> <li> <p>Both regular linux bridge and ovs-bridge kinds can be used, depending on the requirements.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/network/","title":"Network wiring concepts","text":"<p>One of the most important tasks in the process of building container based labs is to create a virtual wiring between the containers and the host. That is one of the problems that containerlab was designed to solve.</p> <p>In this document we will discuss the networking concepts that containerlab employs to provide the following connectivity scenarios:</p> <ol> <li>Make containers available from the lab host</li> <li>Interconnect containers to create network topologies of users choice</li> </ol>"},{"location":"manual/network/#management-network","title":"Management network","text":"<p>As governed by the well-established container networking principles containers are able to get network connectivity using various drivers/methods. The most common networking driver that is enabled by default for docker-managed containers is the bridge driver.</p> <p>The bridge driver connects containers to a linux bridge interface named <code>docker0</code> on most linux operating systems. The containers are then able to communicate with each other and the host via this virtual switch (bridge interface).</p> <p>In containerlab we follow a similar approach: containers launched by containerlab will be attached with their interface to a containerlab-managed docker network. It's best to be explained by an example which we will base on a two nodes lab from our catalog:</p> <pre><code>name: srl02\ntopology:\nkinds:\nsrl:\ntype: ixrd3\nimage: ghcr.io/nokia/srlinux\nnodes:\nsrl1:\nkind: srl\nsrl2:\nkind: srl\nlinks:\n- endpoints: [\"srl1:e1-1\", \"srl2:e1-1\"]\n</code></pre> <p>As seen from the topology definition file, the lab consists of the two SR Linux nodes which are interconnected via a single point-to-point link.</p> <p>The diagram above shows that these two nodes are not only interconnected between themselves, but also connected to a bridge interface on the lab host. This is driven by the containerlab default management network settings.</p>"},{"location":"manual/network/#default-settings","title":"default settings","text":"<p>When no information about the management network is provided within the topo definition file, containerlab will do the following</p> <ol> <li>create, if not already created, a docker network named <code>clab</code></li> <li>configure the IPv4/6 addressing pertaining to this docker network</li> </ol> <p>Info</p> <p>We often refer to <code>clab</code> docker network simply as management network since its the network to which management interfaces of the containerized NOS'es are connected.</p> <p>The addressing information that containerlab will use on this network:</p> <ul> <li>IPv4: subnet 172.20.20.0/24, gateway 172.20.20.1</li> <li>IPv6: subnet 2001:172:20:20::/64, gateway 2001:172:20:20::1</li> </ul> <p>This management network will be configured with MTU value matching the value of a <code>docker0</code> host interface to match docker configuration on the system. This option is configurable.</p> <p>With these defaults in place, the two containers from this lab will get connected to that management network and will be able to communicate using the IP addresses allocated by docker daemon. The addresses that docker carves out for each container are presented to a user once the lab deployment finishes or can be queried any time after:</p> <pre><code># addressing information is available once the lab deployment completes\n\u276f containerlab deploy -t srl02.clab.yml\n# deployment log omitted for brevity\n+---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| # |      Name       | Container ID |  Image  | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| 1 | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl  |       | running | 172.20.20.3/24 | 2001:172:20:20::3/80 |\n| 2 | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl  |       | running | 172.20.20.2/24 | 2001:172:20:20::2/80 |\n+---+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n\n# addresses can also be fetched afterwards with `inspect` command\n\u276f containerlab inspect -a\n+---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| # | Lab Name |      Name       | Container ID |  Image  | Kind | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n| 1 | srl02    | clab-srl02-srl1 | ca24bf3d23f7 | srlinux | srl  |       | running | 172.20.20.3/24 | 2001:172:20:20::3/80 |\n| 2 | srl02    | clab-srl02-srl2 | ee585eac9e65 | srlinux | srl  |       | running | 172.20.20.2/24 | 2001:172:20:20::2/80 |\n+---+----------+-----------------+--------------+---------+------+-------+---------+----------------+----------------------+\n</code></pre> <p>The output above shows that srl1 container has been assigned <code>172.20.20.3/24 / 2001:172:20:20::3/80</code> IPv4/6 address. We can ensure this by querying the srl1 management interfaces address info:</p> <pre><code>\u276f docker exec clab-srl02-srl1 ip address show dummy-mgmt0\n6: dummy-mgmt0: &lt;BROADCAST,NOARP&gt; mtu 1500 qdisc noop state DOWN group default qlen 1000\nlink/ether 2a:66:2b:09:2e:4d brd ff:ff:ff:ff:ff:ff\n    inet 172.20.20.3/24 brd 172.20.20.255 scope global dummy-mgmt0\n       valid_lft forever preferred_lft forever\n    inet6 2001:172:20:20::3/80 scope global\n       valid_lft forever preferred_lft forever\n</code></pre> <p>Now it's possible to reach the assigned IP address from the lab host as well as from other containers connected to this management network. <pre><code># ping srl1 management interface from srl2\n\u276f docker exec -it clab-srl02-srl2 sr_cli \"ping 172.20.20.3 network-instance mgmt\"\nx -&gt; $176x48\nUsing network instance mgmt\nPING 172.20.20.3 (172.20.20.3) 56(84) bytes of data.\n64 bytes from 172.20.20.3: icmp_seq=1 ttl=64 time=2.43 ms\n</code></pre></p> <p>Note</p> <p>If you run multiple labs without changing the default management settings, the containers of those labs will end up connecting to the same management network with their management interface.</p>"},{"location":"manual/network/#host-mode-networking","title":"host mode networking","text":"<p>In addition to the bridge-based management network containerlab supports launching nodes in host networking mode. In this mode containers are attached to the host network namespace. Host mode is enabled with network-mode node setting.</p>"},{"location":"manual/network/#configuring-management-network","title":"configuring management network","text":"<p>Most of the time there is no need to change the defaults for management network configuration, but sometimes it is needed. For example, it might be that the default network ranges are overlapping with the existing addressing scheme on the lab host, or it might be desirable to have predefined management IP addresses.</p> <p>For such cases, the users need to add the <code>mgmt</code> container at the top level of their topology definition file:</p> <pre><code>name: srl02\nmgmt:\nnetwork: custom_mgmt                # management network name\nipv4_subnet: 172.100.100.0/24       # ipv4 range\nipv6_subnet: 2001:172:100:100::/80  # ipv6 range (optional)\ntopology:\n# the rest of the file is omitted for brevity\n</code></pre> <p>With these settings in place, container will get their IP addresses from the specified ranges accordingly.</p>"},{"location":"manual/network/#user-defined-addresses","title":"user-defined addresses","text":"<p>By default, container runtime will assign the management IP addresses for the containers. But sometimes, it's helpful to have user-defined addressing in the management network.</p> <p>For such cases, users can define the desired IPv4/6 addresses on a per-node basis:</p> <pre><code>mgmt:\nnetwork: fixedips\nipv4_subnet: 172.100.100.0/24\nipv6_subnet: 2001:172:100:100::/80\ntopology:\nnodes:\nn1:\nkind: srl\nmgmt_ipv4: 172.100.100.11       # set ipv4 address on management network\nmgmt_ipv6: 2001:172:100:100::11 # set ipv6 address on management network\n</code></pre> <p>Users can specify either IPv4 or IPv6 or both addresses. If one of the addresses is omitted, it will be assigned by container runtime in an arbitrary fashion.</p> <p>Note</p> <ol> <li>If user-defined IP addresses are needed, they must be provided for all containers attached to a given network to avoid address collision.</li> <li>IPv4/6 addresses set on a node level must be from the management network range.</li> </ol>"},{"location":"manual/network/#mtu","title":"MTU","text":"<p>The MTU of the management network defaults to an MTU value of <code>docker0</code> interface, but it can be set to a user defined value:</p> <pre><code>mgmt:\nnetwork: clab_mgmt\nmtu: 2100 # set mtu of the management network to 2100\n</code></pre> <p>This will result in every interface connected to that network to inherit this MTU value.</p>"},{"location":"manual/network/#network-name","title":"network name","text":"<p>The default container network name is <code>clab</code>. To customize this name, users should specify a new value within the <code>network</code> element:</p> <pre><code>mgmt:\nnetwork: myNetworkName\n</code></pre>"},{"location":"manual/network/#default-docker-network","title":"default docker network","text":"<p>To make clab nodes start in the default docker network <code>bridge</code>, which uses the <code>docker0</code> bridge interface, users need to mention this explicitly in the configuration:</p> <pre><code>mgmt:\nnetwork: bridge\n</code></pre> <p>Since <code>bridge</code> network is created by default by docker, using its name in the configuration will make nodes to connect to this network.</p>"},{"location":"manual/network/#bridge-name","title":"bridge name","text":"<p>By default, containerlab will create a linux bridge backing the management docker network with the following name <code>br-&lt;network-id&gt;</code>. The network-id part is coming from the docker network ID that docker manages.</p> <p>We allow our users to change the bridge name that the management network will use. This can be used to connect containers to an already existing bridge with other workloads connected:</p> <pre><code>mgmt:\n# a bridge with a name mybridge will be created or reused\n# as a backing bridge for the management network\nbridge: mybridge\n</code></pre> <p>If the existing bridge has already been addressed with IPv4/6 address, containerlab will respect this address and use it in the IPAM configuration blob of the docker network.</p> <p>If there is no existing IPv4/6 address defined for the custom bridge, docker will assign the first interface from the subnet associated with the bridge.</p> <p>It is possible to set the desired gateway IP (that is the IP assigned to the bridge) with the <code>ipv4-gw/ipv6-gw</code> setting under <code>mgmt</code> container:</p> <pre><code>mgmt:\nnetwork: custom-net\nbridge: mybridge\nipv4_subnet: 10.20.30.0/24 # ip range for the docker network\nipv4-gw: 10.20.30.100 # set custom gateway ip\n</code></pre>"},{"location":"manual/network/#external-access","title":"external access","text":"<p>Starting with <code>0.24.0</code> release containerlab will enable external access to the nodes by default. This means that external systems/hosts will be able to communicate with the nodes of your topology without requiring any manual configuration.</p> <p>To allow external communications containerlab installs a rule in the <code>DOCKER-USER</code> iptables chain, allowing all packets targeting containerlab's management network. The rule looks like follows:</p> <pre><code>\u276f sudo iptables -vnL DOCKER-USER\nChain DOCKER-USER (1 references)\npkts bytes target     prot opt in     out     source               destination         0     0 ACCEPT     all  --  *      br-03d953ed46df  0.0.0.0/0            0.0.0.0/0 # (1)\n768K 4728M RETURN     all  --  *      *       0.0.0.0/0            0.0.0.0/0          </code></pre> <ol> <li>The <code>br-03d953ed46df</code> bridge interface is the interface that backs up the containerlab's management network.</li> </ol> <p>The rule will be removed together with the management network.</p> <p>Should you not want to enable external access to your nodes you can set <code>external-access</code> property to <code>false</code> under the management section of a topology:</p> <pre><code>name: no-ext-access\nmgmt:\nexternal-access: false # (1)\ntopology:\n# your regular topology definition\n</code></pre> <ol> <li>When set to <code>false</code>, containerlab will not touch iptables rules. On most docker installations this will result in restricted external access.</li> </ol> 'missing DOCKER-USER iptables chain' error <p>Containerlab will throw an error \"missing DOCKER-USER iptables chain\" when this chain is not found. This error is typically caused by two factors</p> <ol> <li>Old docker version installed. Typically seen on Centos systems. Minimum required docker version is 17.06.</li> <li>Docker is installed incorrectly. It is recommended to follow the official installation procedures by selecting \"Installation per distro\" menu option.</li> </ol> <p>When docker is correctly installed, additional iptables chains will become available and the error will not appear.</p>"},{"location":"manual/network/#connection-details","title":"connection details","text":"<p>When containerlab needs to create the management network, it asks the docker daemon to do this. Docker will fulfill the request and will create a network with the underlying linux bridge interface backing it. The bridge interface name is generated by the docker daemon, but it is easy to find it:</p> <pre><code># list existing docker networks\n# notice the presence of the `clab` network with a `bridge` driver\n\u276f docker network ls\nNETWORK ID          NAME                DRIVER              SCOPE\n5d60b6ec8420        bridge              bridge              local\nd2169a14e334        clab                bridge              local\n58ec5037122a        host                host                local\n4c1491a09a1a        none                null                local\n# the underlying linux bridge interface name follows the `br-&lt;first_12_chars_of_docker_network_id&gt; pattern\n# to find the network ID use:\n\u276f docker network inspect clab -f {{.ID}} | head -c 12\nd2169a14e334\n\n# now the name is known and its easy to show bridge state\n\u276f brctl show br-d2169a14e334\nbridge name         bridge id           STP enabled   interfaces\nbr-d2169a14e334     8000.0242fe382b74   no            vetha57b950\n                                                      vethe9da10a\n</code></pre> <p>As explained in the beginning of this article, containers will connect to this docker network. This connection is carried out by the <code>veth</code> devices created and attached with one end to bridge interface in the lab host and the other end in the container namespace. This is illustrated by the bridge output above and the diagram at the beginning the of the article.</p>"},{"location":"manual/network/#point-to-point-links","title":"Point-to-point links","text":"<p>Management network is used to provide management access to the NOS containers, it does not carry control or dataplane traffic. In containerlab we create additional point-to-point links between the containers to provide the datapath between the lab nodes.</p> <p>The above diagram shows how links are created in the topology definition file. In this example, the datapath consists of the two virtual point-to-point wires between SR Linux and cEOS containers. These links are created on-demand by containerlab itself.</p> <p>The p2p links are provided by the <code>veth</code> device pairs where each end of the <code>veth</code> pair is attached to a respective container. The MTU on these veth links is set to 9500, so a regular 9212 MTU on the network links shouldn't be a problem.</p>"},{"location":"manual/network/#host-links","title":"host links","text":"<p>It is also possible to interconnect container' data interface not with other container or add it to a bridge, but to attach it to a host's root namespace. This is, for example, needed to create a L2 connectivity between containerlab nodes running on different VMs (aka multi-node labs).</p> <p>This \"host-connectivity\" is achieved by using a reserved node name - <code>host</code> - referenced in the endpoints section. Consider the following example where an SR Linux container has its only data interface connected to a hosts root namespace via veth interface:</p> <pre><code>name: host\ntopology:\nnodes:\nsrl:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nstartup-config: test-srl-config.json\nlinks:\n- endpoints: [\"srl:e1-1\", \"host:srl_e1-1\"]\n</code></pre> <p>With this topology definition, we will have a veth interface with its one end in the container' namespace and its other end in the host namespace. The host will have the interface named <code>srl_e1-1</code> once the lab deployed:</p> <pre><code>ip link\n# SNIP\n433: srl_e1-1@if434: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP mode DEFAULT group default link/ether b2:80:e9:60:c7:9d brd ff:ff:ff:ff:ff:ff link-netns clab-srl01-srl\n</code></pre>"},{"location":"manual/network/#additional-connections-to-management-network","title":"Additional connections to management network","text":"<p>By default every lab node will be connected to the docker network named <code>clab</code> which acts as a management network for the nodes.</p> <p>In addition to that mandatory connection, users can attach additional interfaces to this management network. This might be needed, for example, when data interface of a node needs to talk to the nodes on the management network.</p> <p>For such connections a special form of endpoint definition was created - <code>mgmt-net:$iface-name</code>.</p> <pre><code>name: mgmt\ntopology:\nnodes:\nn1:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nlinks:\n- endpoints:\n- \"n1:e1-1\"\n- \"mgmt-net:n1-e1-1\"\n</code></pre> <p>In the above example the node <code>n1</code> connects with its <code>e1-1</code> interface to the management network. This is done by specifying the endpoint with a reserved name <code>mgmt-net</code> and defining the name of the interface that should be used in that bridge (<code>nq-e1-1</code>).</p> <p>By specifying <code>mgmt-net</code> name of the node in the endpoint definition we tell containerlab to find out which bridge is used by the management network of our lab and use this bridge as the attachment point for our veth pair.</p> <p>This is best illustrated with the following diagram:</p>"},{"location":"manual/network/#dns","title":"DNS","text":"<p>When containerlab finishes the nodes deployment, it also creates static DNS entries inside the <code>/etc/hosts</code> file so that users can access the nodes using their DNS names.</p> <p>The DNS entries are created for each node's IPv4/6 address, and follow the pattern - <code>clab-$labName-$nodeName</code>.</p> <p>For a lab named <code>demo</code> with two nodes named <code>l1</code> and <code>l2</code> containerlab will create the following section inside the <code>/etc/hosts</code> file.</p> <pre><code>###### CLAB-demo-START ######\n172.20.20.2     clab-demo-l1\n172.20.20.3     clab-demo-l2\n2001:172:20:20::2       clab-demo-l1\n2001:172:20:20::3       clab-demo-l2\n###### CLAB-demo-END ######\n</code></pre>"},{"location":"manual/nodes/","title":"Nodes","text":"<p>Node object is one of the containerlab' pillars. Essentially, it is nodes and links what constitute the lab topology. To let users build flexible and customizable labs the nodes are meant to be configurable.</p> <p>The node configuration is part of the topology definition file and may consist of the following fields that we explain in details below.</p> <pre><code># part of topology definition file\ntopology:\nnodes:\nnode1:  # node name\nkind: srl\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\nstartup-config: /root/mylab/node1.cfg\nbinds:\n- /usr/local/bin/gobgp:/root/gobgp\n- /root/files:/root/files:ro\nports:\n- 80:8080\n- 55555:43555/udp\n- 55554:43554/tcp\nuser: test\nenv:\nENV1: VAL1\ncmd: /bin/bash script.sh\n</code></pre>"},{"location":"manual/nodes/#kind","title":"kind","text":"<p>The <code>kind</code> property selects which kind this node is of. Kinds are essentially a way of telling containerlab how to treat the nodes properties considering the specific flavor of the node. We dedicated a separate section to discuss kinds in details.</p> <p>Note</p> <p>Kind must be defined either by setting the kind for a node specifically (as in the example above), or by setting the default kind: <pre><code>topology:\ndefaults:\nkind: srl\nnodes:\nnode1:\n# kind value of `srl` is inherited from defaults section\n</code></pre></p>"},{"location":"manual/nodes/#type","title":"type","text":"<p>With <code>type</code> the user sets a type of the node. Types work in combination with the kinds, such as the type value of <code>ixrd2</code> sets the chassis type for SR Linux node, thus this value only makes sense to nodes of kind <code>srl</code>.</p> <p>Other nodes might treat <code>type</code> field differently, that will depend on the kind of the node. The <code>type</code> values and effects defined in the documentation for a specific kind.</p>"},{"location":"manual/nodes/#group","title":"group","text":"<p><code>group</code> is a freeform string that denotes which group a node belongs to. The grouping is currently only used to sort topology elements on a graph.</p>"},{"location":"manual/nodes/#image","title":"image","text":"<p>The common <code>image</code> attribute sets the container image name that will be used to start the node. The image name should be provided in a well-known format of <code>repository(:tag)</code>.</p> <p>We use <code>&lt;repository&gt;</code> image name throughout the docs articles. This means that the image with <code>&lt;repository&gt;:latest</code> name will be looked up. A user will need to add the latest tag if they want to use the same loose-tag naming:</p> <pre><code># tagging srlinux:20.6.1-286 as srlinux:latest\n# after this change its possible to use `srlinux:latest` or `srlinux` image name\ndocker tag srlinux:20.6.1-286 srlinux:latest\n</code></pre>"},{"location":"manual/nodes/#image-pull-policy","title":"image-pull-policy","text":"<p>With <code>image-pull-policy</code> a user defines the container image pull policy.</p> <p>Valid values are:</p> <ul> <li><code>IfNotPresent</code> - Pull container image if it is not already present. (If e.g. the <code>:latest</code> tag has been updated on a remote registry, containerlab will not re-pull it, if an image with the same tag is already present)</li> <li><code>Never</code> - Do not at all try to pull the image from a registry. An error will be thrown and the execution is stopped if the image is not available locally.</li> <li><code>Always</code> - Always try to pull the new image from a registry. An error will be thrown if pull fails. This will ensure fetching latest image version even if it exists locally.</li> </ul> <p>The default value is <code>IfNotPresent</code>.</p> <pre><code>topology:\nnodes:\nsrl:\nimage: ghcr.io/nokia/srlinux\nimage-pull-policy: Always\n</code></pre>"},{"location":"manual/nodes/#subject-alternative-names-san","title":"subject alternative names (SAN)","text":"<p>With <code>SANs</code> the user sets the Subject Alternative Names that will be added to the node's certificate. Host names that are set by default are:</p> <p>For a topology node named \"srl\" in a lab named \"srl01\", the following SANs are set by default:</p> <ul> <li><code>srl</code></li> <li><code>clab-srl01-srl</code></li> <li><code>srl.srl01.io</code></li> </ul> <pre><code>name: srl01\ntopology:\nkinds:\nsrl:\ntype: ixrd3\nimage: ghcr.io/nokia/srlinux\nnodes:\nsrl:\nkind: srl\nSANs:\n- \"test.com\"\n</code></pre>"},{"location":"manual/nodes/#license","title":"license","text":"<p>Some containerized NOSes require a license to operate or can leverage a license to lift-off limitations of an unlicensed version. With <code>license</code> property a user sets a path to a license file that a node will use. The license file will then be mounted to the container by the path that is defined by the <code>kind/type</code> of the node.</p>"},{"location":"manual/nodes/#startup-config","title":"startup-config","text":"<p>For some kinds it's possible to pass a path to a config file that a node will use on start instead of a bare config. Check documentation for a specific kind to see if <code>startup-config</code> element is supported.</p> <p>Note, that if a config file exists in the lab directory for a given node, then it will take preference over the startup config passed with this setting. If it is desired to discard the previously saved config and use the startup config instead, use the <code>enforce-startup-config</code> setting or deploy a lab with the <code>reconfigure</code> flag.</p>"},{"location":"manual/nodes/#enforce-startup-config","title":"enforce-startup-config","text":"<p>By default, containerlab will use the config file that is available in the lab directory for a given node even if the <code>startup config</code> parameter points to another file. To make a node to boot with the config set with <code>startup-config</code> parameter no matter what, set the <code>enforce-startup-config</code> to <code>true</code>.</p>"},{"location":"manual/nodes/#auto-remove","title":"auto-remove","text":"<p>By default, containerlab will not remove the failed or stopped nodes so that you can read their logs and understand the reason of a failure. If it is required to remove the failed/stopped nodes, use <code>auto-remove: true</code> property.</p> <p>The property can be set on all topology levels.</p>"},{"location":"manual/nodes/#startup-delay","title":"startup-delay","text":"<p>To make certain node(s) to boot/start later than others use the <code>startup-delay</code> config element that accepts the delay amount in seconds.</p> <p>This setting can be applied on node/kind/default levels.</p>"},{"location":"manual/nodes/#binds","title":"binds","text":"<p>Users can leverage the bind mount capability to expose host files to the containerized nodes.</p> <p>Binds instructions are provided under the <code>binds</code> container of a default/kind/node configuration section. The format of those binding instructions follows the same of the docker's --volume parameter.</p> <pre><code>topology:\nnodes:\ntestNode:\nkind: linux\n# some other node parameters\nbinds:\n- /usr/local/bin/gobgp:/root/gobgp # (1)!\n- /root/files:/root/files:ro # (2)!\n- somefile:/somefile # (3)!\n- ~/.ssh/id_rsa:/root/.ssh/id_rsa # (4)!\n</code></pre> <ol> <li>mount a host file found by the path <code>/usr/local/bin/gobgp</code> to a container under <code>/root/gobgp</code> (implicit RW mode)</li> <li>mount a <code>/root/files</code> directory from a host to a container in RO mode</li> <li>when a host path is given in a relative format, the path is considered relative to the topology file and not a current working directory.</li> <li>The <code>~</code> char will be expanded to a user's home directory.</li> </ol> Bind variables <p>By default, binds are either provided as an absolute or a relative (to the current working dir) path. Although the majority of cases can be very well covered with this, there are situations in which it is desirable to use a path that is relative to the node-specific example.</p> <p>Consider a two-node lab <code>mylab.clab.yml</code> with node-specific files, such as state information or additional configuration artifacts. A user could create a directory for such files similar to that:</p> <pre><code>.\n\u251c\u2500\u2500 cfgs\n\u2502   \u251c\u2500\u2500 n1\n\u2502   \u2502   \u2514\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 n2\n\u2502       \u2514\u2500\u2500 conf\n\u2514\u2500\u2500 mylab.clab.yml\n\n3 directories, 3 files\n</code></pre> <p>Then to mount those files to the nodes, the nodes would have been configured with binds like that:</p> <pre><code>name: mylab\ntopology:\nnodes:\nn1:\nbinds:\n- cfgs/n1/conf:/conf\nn2:\nbinds:\n- cfgs/n2/conf:/conf\n</code></pre> <p>while this configuration is correct, it might be considered verbose as the number of nodes grows. To remove this verbosity, the users can use a special variable <code>__clabNodeDir__</code> in their bind paths. This variable will expand to the node-specific directory that containerlab creates for each node.</p> <p>This means that you can create a directory structure that containerlab will create anyhow and put the needed files over there. With the lab named <code>mylab</code> and the nodes named <code>n1</code> and <code>n2</code> the structure containerlab uses is as follows:</p> <pre><code>.\n\u251c\u2500\u2500 clab-mylab\n\u2502   \u251c\u2500\u2500 n1\n\u2502   \u2502   \u2514\u2500\u2500 conf\n\u2502   \u2514\u2500\u2500 n2\n\u2502       \u2514\u2500\u2500 conf\n\u2514\u2500\u2500 mylab.clab.yml\n\n3 directories, 3 files\n</code></pre> <p>With this structure in place, the clab file can leverage the <code>__clabNodeDir__</code> variable:</p> <pre><code>name: mylab\ntopology:\nnodes:\nn1:\nbinds:\n- __clabNodeDir__/conf:/conf\nn2:\nbinds:\n- __clabNodeDir__/conf:/conf\n</code></pre> <p>Notice how <code>__clabNodeDir__</code> hides the directory structure and node names and removes the verbosity of the previous approach.</p> <p>Another special variable the containerlab topology file can use is <code>__clabDir__</code>. In the example above, it would expand into <code>clab-mylab</code> folder. With <code>__clabDir__</code> variable it becomes convenient to bind files like <code>ansible-inventory.yml</code> or <code>topology-data.json</code> that containerlab automatically creates:</p> <pre><code>name: mylab\ntopology:\nnodes:\nansible:\nbinds:\n- __clabDir__/ansible-inventory.yml:/ansible-inventory.yml:ro\ngraphite:\nbinds:\n- __clabDir__/topology-data.json:/htdocs/clab/topology-data.json:ro\n</code></pre> <p>Binds defined on multiple levels (defaults -&gt; kind -&gt; node) will be merged with the duplicated values removed (the lowest level takes precedence).</p>"},{"location":"manual/nodes/#ports","title":"ports","text":"<p>To bind the ports between the lab host and the containers the users can populate the <code>ports</code> object inside the node:</p> <pre><code>ports:\n- 80:8080 # tcp port 80 of the host is mapped to port 8080 of the container\n- 55555:43555/udp\n- 55554:43554/tcp\n</code></pre> <p>The list of port bindings consists of strings in the same format that is acceptable by <code>docker run</code> command's <code>-p/--export</code> flag.</p> <p>This option is only configurable under the node level.</p>"},{"location":"manual/nodes/#env","title":"env","text":"<p>To add environment variables to a node use the <code>env</code> container that can be added at <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p> <p>The variables values are merged when the same vars are defined on multiple levels with nodes level being the most specific.</p> <pre><code>topology:\ndefaults:\nenv:\nENV1: 3 # ENV1=3 will be set if its not set on kind or node level\nENV2: glob # ENV2=glob will be set for all nodes\nkinds:\nsrl:\nenv:\nENV1: 2 # ENV1=2 will be set to if its not set on node level\nENV3: kind # ENV3=kind will be set for all nodes of srl kind\nnodes:\nnode1:\nenv:\nENV1: 1 # ENV1=1 will be set for node1\n# env vars expansion is available, for example\n# ENV2 variable will be set to the value of the environment variable SOME_ENV\n# that is defined for the shell you run containerlab with\nENV2: ${SOME_ENV} </code></pre> <p>You can also specify a magic ENV VAR - <code>__IMPORT_ENVS: true</code> - which will import all environment variables defined in your shell to the relevant topology level.</p>"},{"location":"manual/nodes/#env-files","title":"env-files","text":"<p>To add environment variables defined in a file use the <code>env-files</code> property that can be defined at <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p> <p>The variable defined in the files are merged across all of them wtit more specific definitions overwriting less specific. Node level is the most specific one.</p> <p>Files can either be specified with their absolute path or a relative path. The base path for the relative path resolution is the directory that holds the topology definition file.</p> <pre><code>topology:\ndefaults:\nenv-files:\n- envfiles/defaults\n- /home/user/clab/default-env\nkinds:\nsrl:\nenv-files:\n- envfiles/common\n- ~/spines\nnodes:\nnode1:\nenv-files:\n- /home/user/somefile\n</code></pre>"},{"location":"manual/nodes/#user","title":"user","text":"<p>To set a user which will be used to run a containerized process use the <code>user</code> configuration option. Can be defined at <code>node</code>, <code>kind</code> and <code>global</code> levels.</p> <pre><code>topology:\ndefaults:\nuser: alice # alice user will be used for all nodes unless set on kind or node levels\nkinds:\nsrl:\nuser: bob # bob user will be used for nodes of kind srl unless it is set on node level\nnodes:\nnode1:\nuser: clab # clab user will be used for node1\n</code></pre>"},{"location":"manual/nodes/#entrypoint","title":"entrypoint","text":"<p>Changing the entrypoint of the container is done with <code>entrypoint</code> config option. It accepts the \"shell\" form and can be set on all levels.</p> <pre><code>topology:\ndefaults:\nentrypoint: entrypoint.sh\nkinds:\nsrl:\ncmd: entrypoint.sh\nnodes:\nnode1:\ncmd: entrypoint.sh\n</code></pre>"},{"location":"manual/nodes/#cmd","title":"cmd","text":"<p>It is possible to set/override the command of the container image with <code>cmd</code> configuration option. It accepts the \"shell\" form and can be set on all levels.</p> <pre><code>topology:\ndefaults:\ncmd: bash cmd.sh\nkinds:\nsrl:\ncmd: bash cmd2.sh\nnodes:\nnode1:\ncmd: bash cmd3.sh\n</code></pre>"},{"location":"manual/nodes/#labels","title":"labels","text":"<p>To add container labels to a node use the <code>labels</code> container that can be added at <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p> <p>The label values are merged when the same vars are defined on multiple levels with nodes level being the most specific.</p> <p>Consider the following example, where labels are defined on different levels to show value propagation.</p> <pre><code>topology:\ndefaults:\nlabels:\nlabel1: value1\nlabel2: value2\nkinds:\nsrl:\nlabels:\nlabel1: kind_value1\nlabel3: value3\nnodes:\nnode1:\nlabels:\nlabel1: node_value1\n</code></pre> <p>As a result of such label distribution, node1 will have the following labels:</p> <pre><code>label1: node_value1 # most specific label wins\nlabel2: value2 # inherited from defaults section\nlabel3: value3 # inherited from kinds section\n</code></pre> <p>Note</p> <p>Both user-defined and containerlab-assigned labels also promoted to environment variables prefixed with <code>CLAB_LABEL_</code> prefix.</p>"},{"location":"manual/nodes/#mgmt_ipv4","title":"mgmt_ipv4","text":"<p>To make a node to boot with a user-specified management IPv4 address, the <code>mgmt_ipv4</code> setting can be used. Note, that the static management IP address should be part of the subnet that is used within the lab.</p> <p>Read more about user-defined management addresses here.</p> <pre><code>nodes:\nr1:\nkind: srl\nmgmt_ipv4: 172.20.20.100\n</code></pre>"},{"location":"manual/nodes/#mgmt_ipv6","title":"mgmt_ipv6","text":"<p>To make a node to boot with a user-specified management IPv4 address, the <code>mgmt_ipv6</code> setting can be used. Note, that the static management IP address should be part of the subnet that is used within the lab.</p> <p>Read more about user-defined management addresses here.</p> <pre><code>nodes:\nr1:\nkind: srl\nmgmt_ipv6: 2001:172:20:20::100\n</code></pre>"},{"location":"manual/nodes/#dns","title":"DNS","text":"<p>To influence the DNS configuration a particular node uses, the <code>dns</code> configuration knob should be used. Within this blob, DNS server addresses, options and search domains can be provisioned.</p> <pre><code>topology:\nnodes:\nr1:\nkind: nokia_srlinux\nimage: ghcr.io/nokia/srlinux\ndns:\nservers:\n- 1.1.1.1\n- 8.8.4.4\nsearch:\n- foo.com\noptions:\n- some-opt\n</code></pre>"},{"location":"manual/nodes/#publish","title":"publish","text":"<p>Danger</p> <p>Mysocket.io service used in containerlab was recently rebranded to border0.com, therefore this particular functionality is not available.</p> <p>We have to refactor the publishing feature to use border0 service and would gladly accept PRs.</p> <p>Container lab integrates with mysocket.io service to allow for private, Internet-reachable tunnels created for ports of containerlab nodes. This enables effortless access sharing with customers/partners/colleagues.</p> <p>This integration is extensively covered on Publish ports page.</p> <pre><code>name: demo\ntopology:\nnodes:\nr1:\nkind: srl\npublish:\n- tcp/22     # tcp port 22 will be published\n- tcp/57400  # tcp port 57400 will be published\n- http/8080  # http port 8080 will be published\n</code></pre>"},{"location":"manual/nodes/#network-mode","title":"network-mode","text":"<p>By default containerlab nodes use bridge-mode driver - nodes are created with their first interface connected to a docker network (management network).</p> <p>It is possible to change this behavior using <code>network-mode</code> property of a node.</p>"},{"location":"manual/nodes/#host-mode","title":"host mode","text":"<p>The <code>network-mode</code> configuration option set to <code>host</code> will launch the node in the host networking mode.</p> <pre><code># example node definition with host networking mode\nmy-node:\nimage: alpine:3\nnetwork-mode: host\n</code></pre>"},{"location":"manual/nodes/#container-mode","title":"container mode","text":"<p>Additionally, a node can join network namespace of another container - by referencing the node in the format of <code>container:parent_node_name</code><sup>2</sup>:</p> <pre><code># example node definition with shared network namespace\nmy-node:\nkind: linux\nsidecar-node:\nkind: linux\nnetwork-mode: container:my-node # (1)\nstartup-delay: 10 # (2)\n</code></pre> <ol> <li><code>my-node</code> portion of a <code>network-mode</code> property instructs <code>sidecar-node</code> to join the network namespace of a <code>my-node</code>.</li> <li><code>startup-delay</code> is required in this case in order to properly initialize the namespace of a parent container.</li> </ol> <p>Container name used after <code>container:</code> portion can refer to a node defined in containerlab topology or can refer to a name of a container that was launched outside of containerlab. This is useful when containerlab node needs to connect to a network namespace of a container deployed by 3<sup>rd</sup> party management tool (e.g. k8s kind).</p>"},{"location":"manual/nodes/#runtime","title":"runtime","text":"<p>By default containerlab nodes will be started by <code>docker</code> container runtime. Besides that, containerlab has experimental support for <code>podman</code>, <code>containerd</code>, and <code>ignite</code> runtimes.</p> <p>It is possible to specify a global runtime with a global <code>--runtime</code> flag, or set the runtime on a per-node basis:</p> <p>Options for the runtime parameter are:</p> <ul> <li><code>docker</code></li> <li><code>podman</code></li> <li><code>containerd</code></li> <li><code>ignite</code></li> </ul> <p>The default runtime can also be influenced via the <code>CLAB_RUNTIME</code> environment variable, which takes the same values as mentioned above.</p> <pre><code># example node definition with per-node runtime definition\nmy-node:\nimage: alpine:3\nruntime: containerd\n</code></pre>"},{"location":"manual/nodes/#exec","title":"exec","text":"<p>Containers typically have some process that is launched inside the sandboxed environment. The said process and its arguments are provided via container instructions such as <code>entrypoint</code> and <code>cmd</code> in Docker's case.</p> <p>Quite often, it is needed to run additional commands inside the containers when they finished booting. Instead of modifying the <code>entrypoint</code> and <code>cmd</code> it is possible to use the <code>exec</code> parameter and specify a list of commands to execute:</p> <pre><code># two commands will be executed for node `my-node` once it finishes booting\nmy-node:\nimage: alpine:3\nkind: linux\nbinds:\n- myscript.sh:/myscript.sh\nexec:\n- echo test123\n- bash /myscript.sh\n</code></pre> <p>The <code>exec</code> is particularly helpful to provide some startup configuration for linux nodes such as IP addressing and routing instructions.</p>"},{"location":"manual/nodes/#memory","title":"memory","text":"<p>By default, container runtimes do not impose any memory resource constraints<sup>1</sup>. A container can use too much of the host's memory, making the host OS unstable.</p> <p>The <code>memory</code> parameter can be used to limit the amount of memory a node/container can use.</p> <pre><code># my-node will have access to at most 1Gb of memory.\nmy-node:\nimage: alpine:3\nkind: linux\nmemory: 1Gb\n</code></pre> <p>Supported memory suffixes (case insensitive): <code>b</code>, <code>kib</code>, <code>kb</code>, <code>mib</code>, <code>mb</code>, <code>gib</code>, <code>gb</code>.</p>"},{"location":"manual/nodes/#cpu","title":"cpu","text":"<p>By default, container runtimes do not impose any CPU resource constraints<sup>1</sup>. A container can use as much as the host's scheduler allows.</p> <p>The <code>cpu</code> parameter can be used to limit the number of CPUs a node/container can use.</p> <pre><code># my-node will have access to at most 1.5 of the CPUs\n# available in the host machine.\nmy-node:\nimage: alpine:3\nkind: linux\ncpu: 1.5\n</code></pre>"},{"location":"manual/nodes/#cpu-set","title":"cpu-set","text":"<p>The <code>cpu-set</code> parameter can be used to limit the node CPU usage to specific cores of the host system.</p> <p>Valid syntaxes:</p> <ul> <li><code>0-3</code>: Cores 0, 1, 2 and 3</li> <li><code>0,3</code>: Cores 0 and 3</li> <li><code>0-1,4-5</code>: Cores 0, 1, 4 and 5</li> </ul> <pre><code># my-node will have access to CPU cores 0, 1, 4 and 5.\nmy-node:\nimage: alpine:3\nkind: linux\ncpu-set: 0-1,4-5\n</code></pre>"},{"location":"manual/nodes/#sysctls","title":"sysctls","text":"<p>The sysctl container' setting can be set via the <code>sysctls</code> knob under the <code>defaults</code>, <code>kind</code> and <code>node</code> levels.</p> <p>The sysctl values will be merged. Certain kinds already set up sysctl values in the background, which take precedence over the user-defined values.</p> <p>The following is an example on how to setup the sysctls.</p> <pre><code>topology:\ndefaults:\nsysctls:\nnet.ipv4.ip_forward: 1\nnet.ipv6.icmp.ratelimi: 100\nkinds:\nsrl:\nsysctls:\nnet.ipv4.ip_forward: 0\nnodes:\nnode1:\nsysctls:\nnet.ipv6.icmp.ratelimit: 1000\n</code></pre>"},{"location":"manual/nodes/#wait-for","title":"wait-for","text":"<p>For the explicit definition of startup dependencies between nodes, the <code>wait-for</code> knob under the <code>kind</code> or <code>node</code> level can be used.</p> <p>In the example below node srl3 will wait until srl1 and srl2 are in running state before srl3 gets created. The client node will (via the definition in the linux kind) wait for all three srlX nodes to be created before it gets created.</p> <pre><code>name: waitfor\ntopology:\nkinds:\nsrl:\nimage: ghcr.io/nokia/srlinux\nlinux:\nimage: alpine:3\nwait-for:\n- srl1\n- srl2\n- srl3\nnodes:\nsrl1:\nkind: srl\nsrl2:\nkind: srl\nsrl3:\nkind: srl\nwait-for:\n- srl1\n- srl2\nclient:\nkind: linux\n</code></pre> <p>The built-in Dependency Manger takes care of all the dependencies, both explicitly-defined and implicit ones. It will inspect the dependency graph an make sure it is acyclic. The output of the Dependency Manager graph is visible in the debug mode and looks like the following:</p> <pre><code>DEBU[0004] Dependencies:\nsrl2 -&gt; [  ]\nsrl3 -&gt; [ srl1, srl2 ]\nclient -&gt; [ srl1, srl2, srl3 ]\nsrl1 -&gt; [  ] DEBU[0004] - cycle check round 1 - srl1 &lt;- [ client, srl3 ]\nsrl2 &lt;- [ client, srl3 ]\nsrl3 &lt;- [ client ]\nclient &lt;- [  ] DEBU[0004] - cycle check round 2 - srl1 &lt;- [ srl3 ]\nsrl2 &lt;- [ srl3 ]\nsrl3 &lt;- [  ] DEBU[0004] - cycle check round 3 - srl2 &lt;- [  ]\nsrl1 &lt;- [  ] DEBU[0004] node creation graph is successfully validated as being acyclic </code></pre> <ol> <li> <p>docker runtime resources constraints.\u00a0\u21a9\u21a9</p> </li> <li> <p>this deployment model makes two containers to use a shared network namespace, similar to a Kubernetes pod construct.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/published-ports/","title":"Publish ports","text":"<p>Danger</p> <p>Mysocket.io service used in containerlab was recently rebranded to border0.com, therefore this particular functionality is not available.</p> <p>We have to refactor the publishing feature to use border0 service and would gladly accept PRs.</p> <p>Labs are typically deployed in the isolated environments, such as company's internal network, cloud region or even a laptop. The lab nodes can happily talk to each other and, if needed, can reach Internet in the outbound direction.</p> <p>But sometimes it is really needed to let your lab nodes be reachable over Internet securely and privately in the inbound direction. There are many use cases that warrant such publishing, some of the most common are:</p> <ul> <li>create a lab in your environment and share it with a customer/colleague on-demand</li> <li>make an interactive demo/training where nodes are shared with an audience for hands-on experience</li> <li>share a private lab with someone to collaborate or troubleshoot</li> <li>expose management interfaces (gNMI, NETCONF, SNMP) to test integration with collectors deployed outside of your lab environment</li> </ul> <p>Check out the short video demonstrating the integration:</p> <p>Containerlab made all of these use cases possible by integrating with mysocket.io service. Mysocket.io provides personal and secure tunnels for https/https/tls/tcp ports over global anycast<sup>1</sup> network spanning US, Europe and Asia.</p> <p>To make a certain port of a node available via mysocket.io tunnel provide a <code>publish</code> container under the node/kind/default section of the topology:</p> <pre><code>name: demo\ntopology:\nnodes:\nr1:\nkind: srl\npublish:\n# tcp port 22 will be published and accessible to anyone\n- tcp/22\n# tcp port 57400 will be published for a specific user only\n- tls/57400/user@domain.com\n# http service running over 10200 will be published\n# for any authenticated user within gmail domain\n- http/10200/gmail.com\n</code></pre>"},{"location":"manual/published-ports/#registration","title":"Registration","text":"<p>Tunnels set up by mysocket.io are associated with a user who set them, thus users are required to register within the service. Luckily, the registration is a split second process carried out via a web portal. All it takes is an email and a password.</p>"},{"location":"manual/published-ports/#acquiring-a-token","title":"Acquiring a token","text":"<p>To authenticate with mysocket.io service a user needs to acquire the token by logging into the service. A helper command <code>mysocketio login</code> has been added to containerlab to help with that:</p> <pre><code># Login with password entered from the prompt\ncontainerlab tools mysocketio login -e myemail@dot.com\nPassword:\nINFO[0000] Written mysocketio token to a file /root/containerlab/.mysocketio_token\n</code></pre> <p>The acquired token will be saved under <code>.mysocketio_token</code> filename in the current working directory.</p> <p>Info</p> <p>The token is valid for 5 hours, once the token expires, the already established tunnels will continue to work, but to establish new tunnels a new token must be provided.</p>"},{"location":"manual/published-ports/#specify-what-to-share","title":"Specify what to share","text":"<p>To indicate which ports to publish a users needs to add the <code>publish</code> section under the node/kind or default level of the topology definition file. In the example below, we are publishing SSH and gNMI services of <code>r1</code> node:</p> <pre><code>name: demo\ntopology:\nnodes:\nr1:\nkind: srl\npublish:\n- tcp/22     # tcp port 22 will be exposed\n- tcp/57400  # tcp port 57400 will be exposed\n</code></pre> <p>The <code>publish</code> section holds a list of <code>&lt;type&gt;/&lt;port-number&gt;[/&lt;allowed-domains-and-email&gt;</code> strings, where</p> <ul> <li><code>&lt;type&gt;</code> must be one of the supported mysocket.io socket type<sup>2</sup> - http/https/tls/tcp</li> <li><code>&lt;port&gt;</code> must be a single valid port value</li> <li><code>&lt;allowed-domains-and-email&gt;</code> an optional element restricting access to published ports for a list of users' emails or domains. Read more about in the Identity Aware tunnels section.</li> </ul> <p>Note</p> <p>For a regular mysocketio account the maximum number of tunnels is limited to:   - tcp based tunnels: 5   - http based tunnels: 10 If &gt;5 tcp tunnels are required users should launch a VM in a lab, expose it's SSH service and use this VM as a jumpbox.</p>"},{"location":"manual/published-ports/#add-mysocketio-node","title":"Add mysocketio node","text":"<p>Containerlab integrates with mysocket.io service by leveraging <code>mysocketctl</code> application packaged in a container format. In order for the ports indicated in the <code>publish</code> block to be published, a user needs to add a <code>mysocketio</code> node to the topology. The complete topology file could look like this:</p> <pre><code>name: publish\ntopology:\nnodes:\nr1:\nkind: srl\nimage: ghcr.io/nokia/srlinux\npublish:\n- tcp/22     # tcp port 22 will be exposed\ngrafana:\nkind: linux\nimage: grafana/grafana:7.4.3\npublish:\n- http/3000  # grafana' default http port will be published\n# adding mysocketio container which has mysocketctl client inside\nmysocketio:\nkind: mysocketio\nimage: ghcr.io/hellt/mysocketctl:0.5.0\nbinds:\n- .mysocketio_token:/root/.mysocketio_token # bind mount API token\n</code></pre> <p>The <code>mysocketio</code> node is a tiny linux container with mysocketctl client installed. Containerlab uses this node to create the sockets and start the tunnels as per <code>publish</code> block instructions.</p> <p>Pay specific attention to <code>binds</code> section defined for mysocketio node. With this section we provide a path to the API token that we acquired before launching the lab. This token is used to authenticate with mysocketio API service.</p>"},{"location":"manual/published-ports/#explore-published-ports","title":"Explore published ports","text":"<p>When a user launches a lab with published ports it will be presented with a summary table after the lab deployment process finishes:</p> <pre><code>+---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+\n| # |         Name          | Container ID |              Image              |    Kind    | Group |  State  |  IPv4 Address  |     IPv6 Address     |\n+---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+\n| 1 | clab-sock-r1          | 9cefd6cdb239 | srlinux:20.6.3-145              | srl        |       | running | 172.20.20.2/24 | 2001:172:20:20::2/80 |\n| 2 | clab-sock-mysocketctl | 8f5385beb97e | ghcr.io/hellt/mysocketctl:0.5.0 | mysocketio |       | running | 172.20.20.3/24 | 2001:172:20:20::3/80 |\n+---+-----------------------+--------------+---------------------------------+------------+-------+---------+----------------+----------------------+\nPublished ports:\n\u250c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u252c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2510\n\u2502 SOCKET ID                            \u2502 DNS NAME                             \u2502 PORT(S) \u2502 TYPE \u2502 CLOUD AUTH \u2502 NAME                   \u2502\n\u251c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u253c\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2524\n\u2502 444ed853-d3b6-448c-8f0a-6854b3578848 \u2502 wild-water-9221.edge.mysocket.io     \u2502 80, 443 \u2502 http \u2502 false      \u2502 clab-grafana-http-3000 \u2502\n\u2502 287e5962-29ac-4ca1-8e01-e0333d399070 \u2502 falling-wave-5735.edge.mysocket.io   \u2502 54506   \u2502 tcp  \u2502 false      \u2502 clab-r1-tcp-22         \u2502\n\u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The Published ports table lists the published ports and their corresponding DNS names. Looking at the NAME column users can quickly discover which tunnel corresponds to which node-port. The socket name follows the <code>clab-&lt;node-name&gt;-&lt;type&gt;-&lt;port&gt;</code> pattern.</p> <p>To access the published port, users need to combine the DNS name and the Port to derive the full address. For the exposed SSH port, for example, the ssh client can use the following command to access remote SSH service:</p> <pre><code>ssh user@falling-wave-5735.edge.mysocket.io -p 54506\n</code></pre> <p>Warning</p> <p>When a lab with published ports start, containerlab first removes all previously established tunnels. This means that any manually set up tunnels for this account will get removed.</p>"},{"location":"manual/published-ports/#identity-aware-tunnels","title":"Identity aware tunnels","text":"<p>In the previous examples the published ports were created in a way that makes them accessible to anyone on the Internet who knows the exact domain name and port of a respective tunnel. Although being convenient, this approach is not secure, since there is no control over who can access the ports you published.</p> <p>If additional security is needed, containerlab users should define the published ports using Identity Awareness<sup>3</sup> feature of mysocketio. With Identity aware sockets users are allowed to specify a list of email addresses or domains which will have access to a certain port.</p> <p>Consider the following snippet:</p> <pre><code>topology:\nnodes:\nleaf1:\npublish:\n- tcp/22/dodin.roman@gmail.com,contractor@somewhere.com\nleaf2:\npublish:\n- tcp/22\ngrafana:\npublish:\n- http/3000/gmail.com,nokia.com,colleague@somedomain.com\n</code></pre> <p>Within the same <code>publish</code> block it is possible to provide a list of comma separated emails and/or domains which will be allowed to access the published port in question.</p> <p>Authentication is carried out by OAuth via Google, GitHub, Facebook and Mysocket.io providers. When accessing a secured tunnel, a browser page is opened asking to authenticate:</p> <p> </p> <p>Once authenticated via any of the available providers the sessions will establish.</p>"},{"location":"manual/published-ports/#tcptls","title":"TCP/TLS","text":"<p>With Identity Aware sockets used for SSH<sup>4</sup> service, a client must have mysocketctl client installed and use the following command to establish a connection:</p> <pre><code>ssh &lt;ssh-username&gt;@&lt;mysocket-tunnel-address&gt; \\\n-o 'ProxyCommand=mysocketctl client tls --host %h'\n</code></pre> <p>As with HTTP services, a browser page will appear asking to proceed with authentication. Upon successful authentication, the SSH session will establish.</p>"},{"location":"manual/published-ports/#proxy","title":"Proxy","text":"<p>Mysocketio uses SSH as a dataplane to build tunnels, thus it needs to be able to have external SSH access towards the <code>ssh.mysocket.io</code> SSH server.</p> <p>Chances are high that in your environment external SSH access might be blocked, preventing mysocket to setup tunnels. A possible solution for such environments would be to leverage the ability to tunnel SSH traffic via HTTP(S) proxies.</p> <p>If your HTTP(S) proxy supports CONNECT method and is able to pass non-HTTP payloads, it is quite likely that mysocketio service will work.</p> <p>To configure HTTP(S) proxy for mysocketio use the <code>mysocket-proxy</code> parameter in the <code>extras</code> section of the node definition:</p> <pre><code>    mysocketio:\nkind: mysocketio\nimage: ghcr.io/hellt/mysocketctl:0.5.0\nbinds:\n- .mysocketio_token:/root/.mysocketio_token\nextras:\nmysocket-proxy: http://192.168.0.1:8000\n</code></pre>"},{"location":"manual/published-ports/#troubleshooting","title":"Troubleshooting","text":"<p>To check the health status of the established tunnels execute the following command to check the logs created on mysocketio container:</p> <pre><code>docker exec -it &lt;mysocketio-node-name&gt; /bin/sh -c \"cat socket*\"\n</code></pre> <p>This command will display all the logs for the published ports. If something is not right, you will see the errors in the log.</p> <ol> <li> <p>https://mysocket.readthedocs.io/en/latest/about/about.html#build-on-a-global-anycast-network \u21a9</p> </li> <li> <p>https://mysocket.readthedocs.io/en/latest/about/about.html#features \u21a9</p> </li> <li> <p>Identity aware HTTP and TCP tunnels are available.\u00a0\u21a9</p> </li> <li> <p>Read more about Identity Aware sockets for TCP in the official blog.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/topo-def-file/","title":"Topology definition","text":"<p>Containerlab builds labs based on the topology information that users pass to it. This topology information is expressed as a code contained in the topology definition file which structure is the prime focus of this document.</p>"},{"location":"manual/topo-def-file/#topology-definition-components","title":"Topology definition components","text":"<p>The topology definition file is a configuration file expressed in YAML and has a name pattern of <code>*.clab.yml</code><sup>1</sup>. In this document, we take a pre-packaged Nokia SR Linux and Arista cEOS lab and explain the topology definition structure using its definition file srlceos01.clab.yml which is pasted below:</p> <pre><code>name: srlceos01\ntopology:\nnodes:\nsrl:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nceos:\nkind: ceos\nimage: ceos:4.25.0F\nlinks:\n- endpoints: [\"srl:e1-1\", \"ceos:eth1\"]\n</code></pre> <p>Tip</p> <p>Containerlab provides a JSON schema file for the topology file. The schema is used to live-validate user's input if a code editor supports this feature.</p> <p>Additionally, the auto-generated schema documentation can be explored to understand the full scope of the configuration options containerlab provides. </p> <p>This topology results in the two nodes being started up and interconnected with each other using a single point-po-point interface:</p> <p>Let's touch on the key components of the topology definition file used in this example.</p>"},{"location":"manual/topo-def-file/#name","title":"Name","text":"<p>The topology must have a name associated with it. The name is used to distinct one topology from another, to allow multiple topologies to be deployed on the same host without clashes.</p> <pre><code>name: srlceos01\n</code></pre> <p>Its user's responsibility to give labs unique names if they plan to run multiple labs.</p> <p>The name is a free-formed string, though it is better not to use dashes (<code>-</code>) as they are used to separate lab names from node names.</p> <p>When containerlab starts the containers, their names will be generated using the following pattern: <code>clab-{{lab-name}}-{{node-name}}</code>. The lab name here is used to make the container's names unique between two different labs, even if the nodes are named the same.</p>"},{"location":"manual/topo-def-file/#prefix","title":"Prefix","text":"<p>It is possible to change the prefix that containerlab adds to node names. The <code>prefix</code> parameter is in charge of that. It follows the below-mentioned logic:</p> <ol> <li>When <code>prefix</code> is not present in the topology file, the default prefix logic applies. Containers will be named as <code>clab-&lt;lab-name&gt;-&lt;node-name&gt;</code>.</li> <li>When <code>prefix</code> is set to some value, for example, <code>myprefix</code>, this string is used instead of <code>clab</code>, and the resulting container name will be: <code>myprefix-&lt;lab-name&gt;-&lt;node-name&gt;</code>.</li> <li>When <code>prefix</code> is set to a magic value <code>__lab-name</code> the resulting container name will not have the <code>clab</code> prefix, but will keep the lab name: <code>&lt;lab-name&gt;-&lt;node-name&gt;</code>.</li> <li>When set to an empty string, the node names will not be prefixed at all. If your node is named <code>mynode</code>, you will get the <code>mynode</code> container in your system.</li> </ol> <p>Warning</p> <p>In the case of an empty prefix, you have to keep in mind that nodes need to be named uniquely across all labs.</p> <p>Examples:</p> custom prefixempty prefix <pre><code>name: mylab\nprefix: myprefix\nnodes:\nn1:\n# &lt;some config&gt;\n</code></pre> <p>With a prefix set to <code>myprefix</code> the container name for node <code>n1</code> will be <code>myprefix-mylab-n1</code>.</p> <pre><code>name: mylab\nprefix: \"\"\nnodes:\nn1:\n# &lt;some config&gt;\n</code></pre> <p>When a prefix is set to an empty string, the container name will match the node name - <code>n1</code>.</p> <p>Note</p> <p>Even when you change the prefix, the lab directory is still uniformly named using the <code>clab-&lt;lab-name&gt;</code> pattern.</p>"},{"location":"manual/topo-def-file/#topology","title":"Topology","text":"<p>The topology object inside the topology definition is the core element of the file. Under the <code>topology</code> element you will find all the main building blocks of a topology such as <code>nodes</code>, <code>kinds</code>, <code>defaults</code> and <code>links</code>.</p>"},{"location":"manual/topo-def-file/#nodes","title":"Nodes","text":"<p>As with every other topology the nodes are in the center of things. With nodes we define which lab elements we want to run, in what configuration and flavor.</p> <p>Let's zoom into the two nodes we have defined in our topology:</p> <pre><code>topology:\nnodes:\nsrl:                    # this is a name of the 1st node\nkind: srl\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\nceos:                   # this is a name of the 2nd node\nkind: ceos\nimage: ceos:4.25.0F\n</code></pre> <p>We defined individual nodes under the <code>topology.nodes</code> container. The name of the node is the key under which it is defined. Following the example, our two nodes are named <code>srl</code> and <code>ceos</code> respectively.</p> <p>Each node can have multiple configuration properties which make containerlab quite a flexible tool. The <code>srl</code> node in our example is defined with the a few node-specific properties:</p> <pre><code>srl:\nkind: srl\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\n</code></pre> <p>Refer to the node configuration document to meet all other options a node can have.</p>"},{"location":"manual/topo-def-file/#links","title":"Links","text":"<p>Although it is totally fine to define a node without any links (like in this lab) most of the time we interconnect the nodes to make datapaths. One of containerlab purposes is to make the interconnection of nodes simple.</p> <p>Links are defined under the <code>topology.links</code> container in the following manner:</p> <pre><code># nodes configuration omitted for clarity\ntopology:\nnodes:\nsrl:\nceos:\nlinks:\n- endpoints: [\"srl:e1-1\", \"ceos:eth1\"]\n- endpoints: [\"srl:e1-2\", \"ceos:eth2\"]\n</code></pre> <p>As you see, the <code>topology.links</code> element is a list of individual links. The link itself is expressed as pair of <code>endpoints</code>. This might sound complicated, lets use a graphical explanation:</p> <p>As demonstrated on a diagram above, the links between the containers are the point-to-point links which are defined by a pair of interfaces. The link defined as:</p> <pre><code>endpoints: [\"srl:e1-1\", \"ceos:eth1\"]\n</code></pre> <p>will result in a creation of a p2p link between the node named <code>srl</code> and its <code>e1-1</code> interface and the node named <code>ceos</code> and its <code>eth1</code> interface. The p2p link is realized with a veth pair.</p>"},{"location":"manual/topo-def-file/#kinds","title":"Kinds","text":"<p>Kinds define the behavior and the nature of a node, it says if the node is a specific containerized Network OS, virtualized router or something else. We go into details of kinds in its own document section, so here we will discuss what happens when <code>kinds</code> section appears in the topology definition:</p> <pre><code>topology:\nkinds:\nsrl:\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\nnodes:\nsrl1:\nkind: srl\nsrl2:\nkind: srl\nsrl3:\nkind: srl\n</code></pre> <p>In the example above the <code>topology.kinds</code> element has <code>srl</code> kind referenced. With this, we set some values for the properties of the <code>srl</code> kind. A configuration like that says that nodes of <code>srl</code> kind will also inherit the properties (type, image) defined on the kind level.</p> <p>Essentially, what <code>kinds</code> section allows us to do is to shorten the lab definition in cases when we have a number of nodes of a same kind. All the nodes (<code>srl1</code>, <code>srl2</code>, <code>srl3</code>) will have the same values for their <code>type</code> and <code>image</code> properties.</p> <p>Consider how the topology would have looked like without setting the <code>kinds</code> object:</p> <pre><code>topology:\nnodes:\nsrl1:\nkind: srl\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\nsrl2:\nkind: srl\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\nsrl3:\nkind: srl\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\n</code></pre> <p>A lot of unnecessary repetition which is eliminated when we set <code>srl</code> kind properties on kind level.</p>"},{"location":"manual/topo-def-file/#defaults","title":"Defaults","text":"<p><code>kinds</code> set the values for the properties of a specific kind, whereas with the <code>defaults</code> container it is possible to set values globally.</p> <p>For example, to set the environment variable for all the nodes of a topology:</p> <pre><code>topology:\ndefaults:\nenv:\nMYENV: VALUE\nnodes:\nsrl1:\nsrl2:\nsrl3:\n</code></pre> <p>Now every node in this topology will have environment variable <code>MYENV</code> set to <code>VALUE</code>.</p>"},{"location":"manual/topo-def-file/#generated-topologies","title":"Generated topologies","text":"<p> Advanced topic</p> <p>To further simplify parametrization of the topology files, containerlab allows users to template the topology files using Go Template engine.</p> <p>Using templating approach it is possible to create a lab template and instantiate different labs from it, by simply changing the variables in the variables file.</p> <p>Standard Go templating language has been extended with the functions provided in docs.gomplate.ca project, which opens the doors to a very flexible topology generation workflows.</p> <p>To help you get started, we created the following lab examples which demonstrate how topology templating can be used:</p> <ul> <li>Leaf-Spine topology with parametrized number of leaves/spines</li> <li>5-stage Clos topology with parametrized number of pods and super-spines</li> </ul> <ol> <li> <p>if the filename has <code>.clab.yml</code> or <code>-clab.yml</code> suffix, the YAML file will have autocompletion and linting support in VSCode editor.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/vrnetlab/","title":"VM-based routers integration","text":"<p>Containerlab focuses on containers, but many routing products ship only in virtual machine packaging. Leaving containerlab users without the ability to create topologies with both containerized and VM-based routing systems would have been a shame.</p> <p>Keeping this requirement in mind from the very beginning, we added <code>bridge</code>/<code>ovs-bridge</code> kind that allows bridging your containerized topology with other resources available via a bridged network. For example, a VM based router:</p> <p>With this approach, you could bridge VM-based routing systems by attaching interfaces to the bridge you define in your topology. However, it doesn't allow users to define the VM-based nodes in the same topology file. With <code>vrnetlab</code> integration, containerlab is now capable of launching topologies with VM-based routers defined in the same topology file.</p>"},{"location":"manual/vrnetlab/#vrnetlab","title":"Vrnetlab","text":"<p>Vrnetlab packages a regular VM inside a container and makes it runnable as if it was a container image.</p> <p>To make this work, vrnetlab provides a set of scripts that build the container image out of a user-provided VM disk. This integration enables containerlab to build topologies that consist both of native containerized NOSes and VMs:</p> <p>Warning</p> <p>Ensure that the VM that containerlab runs on has Nested virtualization enabled to support vrnetlab-based containers.</p>"},{"location":"manual/vrnetlab/#compatibility-matrix","title":"Compatibility matrix","text":"<p>To make vrnetlab images to work with container-based networking in containerlab, we needed to fork vrnetlab project and implement the necessary improvements. VM-based routers that you intend to run with containerlab should be built with <code>hellt/vrnetlab</code> project, and not with the upstream <code>vrnetlab/vrnetlab</code>.</p> <p>Containerlab depends on <code>hellt/vrnetlab</code> project, and sometimes features added in containerlab must be implemented in <code>vrnetlab</code> (and vice-versa). This leads to a cross-dependency between these projects.</p> <p>The following table provides a link between the version combinations:</p> containerlab<sup>3</sup> vrnetlab<sup>4</sup> Notes <code>0.10.4</code> <code>0.1.0-cl</code> Initial release. Images: sros, vmx, xrv, xrv9k <code>0.11.0</code> <code>0.2.0</code> added vr-veos, support for boot-delay, SR OS will have a static route to docker network, improved XRv startup chances -- <code>0.2.1</code> added timeout for SR OS images to allow eth interfaces to appear in the container namespace. Other images are not touched. -- <code>0.2.2</code> fixed serial (telnet) access to SR OS nodes -- <code>0.2.3</code> set default cpu/ram for SR OS images <code>0.13.0</code> <code>0.3.0</code> added support for Cisco CSR1000v via <code>vr-csr</code> and MikroTik routeros via <code>vr-ros</code> kind -- <code>0.3.1</code> enhanced SR OS boot sequence -- <code>0.4.0</code> fixed SR OS CPU allocation and added Palo Alto PAN support <code>vr-pan</code> <code>0.16.0</code> <code>0.5.0</code> added support for Cisco Nexus 9000v via <code>vr-n9kv</code> kind, added support for non-continuous interfaces provisioning <code>0.19.0</code> <code>0.6.0</code> added experimental support for Juniper vQFX via <code>vr-vqfx</code> kind, added support Dell FTOS via <code>vr-ftosv</code> <code>0.6.2</code> support for IPv6 management for SR OS; support for RouterOS v7+ <code>0.7.0</code> startup-config support for vqfx and vmx <code>0.32.2</code> <code>0.8.0</code> startup-config support for the rest of the kinds, support for multi line card SR OS <code>0.34.0</code> <code>0.8.2</code> startup-config support for PANOS, ISA support for Nokia VSR-I and MGMT VRF for VMX <code>0.9.0</code> Support for IPInfusion OcNOS with vrnetlab"},{"location":"manual/vrnetlab/#building-vrnetlab-images","title":"Building vrnetlab images","text":"<p>To build a vrnetlab image compatible with containerlab, users first need to ensure that the versions of both projects follow compatibility matrix.</p> <ol> <li>Clone <code>hellt/vrnetlab</code> and checkout to a version compatible with containerlab release:</li> </ol> <pre><code>git clone https://github.com/hellt/vrnetlab &amp;&amp; cd vrnetlab\n\n# assuming we are running containerlab 0.11.0,\n# the latest compatible vrnetlab version is 0.2.3\n# at the moment of this writing\ngit checkout v0.2.3\n</code></pre> <ol> <li>Enter the directory for the image of interest</li> </ol> <pre><code>cd sros\n</code></pre> <ol> <li>Follow the build instructions from the README.md file in the image directory</li> </ol>"},{"location":"manual/vrnetlab/#supported-vm-products","title":"Supported VM products","text":"<p>The images that work with containerlab will appear in the supported list as we implement the necessary integration.</p> Product Kind Demo lab Notes Nokia SR OS vr-sros SRL &amp; SR OS When building SR OS vrnetlab image for use with containerlab, do not provide the license during the image build process. The license shall be provided in the containerlab topology definition file<sup>1</sup>. Juniper vMX vr-vmx SRL &amp; vMX Juniper vQFX vr-vqfx Coming soon Cisco XRv vr-xrv SRL &amp; XRv Cisco XRv9k vr-xrv9k SRL &amp; XRv9k Cisco CSR1000v vr-csr Arista vEOS vr-veos MikroTik RouterOS vr-ros Palo Alto PAN vr-pan Cisco Nexus 9000v vr-n9kv Dell FTOS10v vr-ftosv"},{"location":"manual/vrnetlab/#connection-modes","title":"Connection modes","text":"<p>Containerlab offers several ways of connecting VM-based routers with the rest of the docker workloads. By default, vrnetlab integrated routers will use tc backend<sup>2</sup>, which doesn't require any additional packages to be installed on the container host and supports transparent passage of LACP frames.</p> Any other datapaths? <p>Although <code>tc</code> based datapath should cover all the needed connectivity requirements, if other bridge-like datapaths are needed, Containerlab offers OpenvSwitch and Linux bridge modes. Users can plug in those datapaths by specifying <code>CONNECTION_MODE</code> env variable: <pre><code># the env variable can also be set in the defaults section\nname: myTopo\ntopology:\nnodes:\nsr1:\nkind: vr-sros\nimage: vrnetlab/vr-sros:20.10.R1\nenv:\nCONNECTION_MODE: bridge # use `ovs` for openvswitch datapath\n</code></pre></p>"},{"location":"manual/vrnetlab/#boot-delay","title":"Boot delay","text":"<p>A simultaneous boot of many qemu nodes may stress the underlying system, which sometimes renders in a boot loop or system halt. If the container host doesn't have enough capacity to bear the simultaneous boot of many qemu nodes, it is still possible to successfully run them by scheduling their boot time.</p> <p>Delaying the boot process of specific nodes by a user-defined time will allow nodes to boot successfully while \"gradually\" loading the system. The boot delay can be set with <code>BOOT_DELAY</code> environment variable that supported <code>vr-xxxx</code> kinds will recognize.</p> <p>Consider the following example where the first SR OS nodes will boot immediately, whereas the second node will sleep for 30 seconds and then start the boot process:</p> <pre><code>name: bootdelay\ntopology:\nnodes:\nsr1:\nkind: vr-sros\nimage: vr-sros:21.2.R1\nlicense: license-sros21.txt\nsr2:\nkind: vr-sros\nimage: vr-sros:21.2.R1\nlicense: license-sros21.txt\nenv:\n# boot delay in seconds\nBOOT_DELAY: 30\n</code></pre>"},{"location":"manual/vrnetlab/#memory-optimization","title":"Memory optimization","text":"<p>Typically a lab consists of a few types of VMs which are spawned and interconnected with each other. Consider a lab consisting of 5 interconnected routers; one router uses VM image X, and four routers use VM image Y.</p> <p>Effectively we run just two types of VMs in that lab, and thus we can implement a memory deduplication technique that drastically reduces the memory footprint of a lab. In Linux, this can be achieved with technologies like UKSM/KSM. Refer to this article that explains the methodology and provides steps to get UKSM working on Ubuntu/Fedora systems.</p> <ol> <li> <p>see this example lab with a license path provided in the topology definition file\u00a0\u21a9</p> </li> <li> <p>pros and cons of different datapaths were examined here \u21a9</p> </li> <li> <p>to install a certain version of containerlab, use the instructions from installation doc.\u00a0\u21a9</p> </li> <li> <p>to have a guaranteed compatibility checkout to the mentioned tag and build the images.\u00a0\u21a9</p> </li> </ol>"},{"location":"manual/wireshark/","title":"Packet capture & Wireshark","text":"<p>Every lab must have a packet capturing abilities, without it data plane verification becomes unnecessary complicated.</p> <p> </p> <p>Containerlab is no exception and capturing packets is something you can and should do with the labs launched by containerlab.</p> <p>Consider the following lab topology which highlights the typical points of packet capture.</p> <p>Since containerlab leverages linux network devices, users are free to use whatever tool of choice to sniff from any of them. This article will provide examples for <code>tcpdump</code> and <code>wireshark</code> tools.</p>"},{"location":"manual/wireshark/#packet-capture-namespaces-and-interfaces","title":"Packet capture, namespaces and interfaces","text":"<p>Capturing the packets from an interface requires having that interface name and it's network namespace (netns). And that's it.</p> <p>Keep in mind, that containers employ network isolation by the means of network namespaces. As depicted above, each container has its own network namespace which is named exactly the same. This makes it trivial to pinpoint which namespace to use.</p> <p>If containerlab at the end of a lab deploy reports that it created the containers with the names</p> <ul> <li>clab-lab1-srl</li> <li>clab-lab1-ceos</li> <li>clab-lab1-linux</li> </ul> <p>then the namespaces for each of those containers will be named the same (clab-lab1-srl, etc).</p> <p>To list the interfaces (links) of a given container leverage the <code>ip</code> utility:</p> <pre><code># where $netns_name is the container name of a node\nip netns exec $netns_name ip link\n</code></pre>"},{"location":"manual/wireshark/#capturing-with-tcpdumpwireshark","title":"Capturing with tcpdump/wireshark","text":"<p>Now when it is clear which netns names corresponds to which container and which interfaces are available inside the given lab node, its extremely easy to start capturing traffic.</p>"},{"location":"manual/wireshark/#local-capture","title":"local capture","text":"<p>From the containerlab host to capture from any interface inside a container simply use:</p> <pre><code># where $lab_node_name is the name of the container, which is also the name of the network namespace\n# and $if_name is the interface name inside the container netns\nip netns exec $lab_node_name tcpdump -nni $if_name\n</code></pre>"},{"location":"manual/wireshark/#remote-capture","title":"remote capture","text":"<p>If you want to start capture from a remote machine, then add <code>ssh</code> command to the mix:</p> <pre><code>ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -nni $if_name\"\n</code></pre> <p>Capturing remotely with <code>tcpdump</code> makes little sense, but it makes all the difference when <code>wireshark</code> is concerned.</p> <p>Wireshark normally is not installed on the containerlab host, but it more often than not installed on the users machine/laptop. Thus it is possible to use remote capture capability to let wireshark receive the traffic from the remote containerlab node:</p> <pre><code>ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | wireshark -k -i -\n</code></pre> <p>This will start the capture from a given interface and redirect the received flow to the wireshark input.</p> <p>Note</p> <p>Windows users should use WSL and invoke the command similar to the following: <pre><code>ssh $containerlab_host_address \"ip netns exec $lab_node_name tcpdump -U -nni $if_name -w -\" | /mnt/c/Program\\ Files/Wireshark/wireshark.exe -k -i -\n</code></pre></p>"},{"location":"manual/wireshark/#examples","title":"Examples","text":"<p>Lets take the first diagram of this article and see which commands are used to sniff from the highlighted interfaces.</p> <p>In the examples below the wireshark will be used as a sniffing tool and the following naming simplifications and conventions used:</p> <ul> <li><code>$clab_host</code> - address of the containerlab host</li> <li><code>clab-pcap-srl</code>, <code>clab-pcap-ceos</code>, <code>clab-pcap-linux</code> - container names of the SRL, cEOS and Linux nodes accordingly.</li> </ul> SR Linux [1], [4]cEOS [2]Linux container [3]management bridge [5] <p>SR Linux linecard interfaces are named as <code>e&lt;linecard_num&gt;-&lt;port_num&gt;</code> which translates to <code>ethernet-&lt;linecard_num&gt;/&lt;port_num&gt;</code> name inside the NOS itself. So to capture from <code>ethernet-1/1</code> interface the following command should be used: <pre><code>ssh $clab_host \"ip netns exec $clab-pcap-srl tcpdump -U -nni e1-1 -w -\" | wireshark -k -i -\n</code></pre> The management interface on the SR Linux container is named <code>mgmt0</code>, so the relevant command will look like: <pre><code>ssh $clab_host \"ip netns exec $clab-pcap-srl tcpdump -U -nni mgmt0 -w -\" | wireshark -k -i -\n</code></pre></p> <p>Similarly to SR Linux example, to capture the data interface of cEOS is no different. Just pick the right interface: <pre><code>ssh $clab_host \"ip netns exec $clab-pcap-ceos tcpdump -U -nni eth1 -w -\" | wireshark -k -i -\n</code></pre></p> <p>A bare linux container is no different, its interfaces are named <code>ethX</code> where <code>eth0</code> is the interface connected to the containerlab management network. So to capture from the first data link we will use <code>eth1</code> interface: <pre><code>ssh $clab_host \"ip netns exec $clab-pcap-linux tcpdump -U -nni eth1 -w -\" | wireshark -k -i -\n</code></pre></p> <p>It is also possible to listen for all management traffic that traverses the containerlab's management network. To do that you firstly need to find out the name of the linux bridge and then capture from it: <pre><code>ssh $clab_host \"tcpdump -U -nni brXXXXXX -w -\" | wireshark -k -i -\n</code></pre> Note that in this case you do not need to drill into the network namespace, since management bridge is in the default netns.</p> <p>To simplify wireshark remote capturing process users can create a tiny bash script that will save some typing:</p> <pre><code>#!/bin/sh\n# call this script as `bash script_name.sh &lt;container-name&gt; &lt;interface-name&gt;`\nssh &lt;containerlab_address&gt; \"ip netns exec $1 tcpdump -U -nni $2 -w -\" | wireshark -k -i -\n</code></pre>"},{"location":"manual/kinds/","title":"Kinds","text":"<p>Containerlab launches, wires up and manages container-based labs. The steps required to launch a vanilla <code>debian</code> or <code>centos</code> container image aren't at all different. On the other hand, Nokia SR Linux launching procedure is nothing like the one for Arista cEOS.</p> <p>Things like required syscalls, mounted directories, entrypoint and commands to execute are all different for the containerized NOS'es. To let containerlab understand which launching sequence to use, the notion of a <code>kind</code> was introduced. Essentially <code>kinds</code> abstract away the need to understand certain setup peculiarities of different NOS'es.</p> <p>Given the following topology definition file, containerlab is able to know how to launch <code>node1</code> as an SR Linux container and <code>node2</code> as a cEOS one because they are associated with the kinds:</p> <pre><code>name: srlceos01\ntopology:\nnodes:\nnode1:\nkind: srl              # node1 is of srl kind\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\nnode2:\nkind: ceos             # node2 is of ceos kind\nimage: ceos:4.25F\nlinks:\n- endpoints: [\"node1:e1-1\", \"node2:eth1\"]\n</code></pre> <p>Containerlab supports a fixed number of platforms. Most platforms are identified with both a short and a long <code>kind</code> name; these names can be used interchangeably.</p> <p>Within each predefined kind, we store the necessary information that is used to successfully launch the container. The following kinds are supported by containerlab:</p> Name Short/Long kind name Status Packaging Nokia SR Linux <code>srl/nokia_srlinux</code> supported container Nokia SR OS <code>vr-sros/nokia_sros</code> supported VM Arista cEOS <code>ceos/arista_ceos</code> supported container Arista vEOS <code>vr-veos/vr-arista_veos</code> supported VM Juniper cRPD <code>crpd/juniper_crpd</code> supported container Juniper vMX <code>vr-vmx/vr-juniper_vmx</code> supported VM Juniper vQFX <code>vr-vqfx/vr-juniper_vqfx</code> supported VM Cisco XRv9k <code>vr-xrv9k/vr-cisco_xrv9k</code> supported VM Cisco XRv <code>vr-xrv/vr-cisco_xrv</code> supported VM Cisco Nexus 9000v <code>vr-n9kv/vr-cisco_n9kv</code> supported VM Cumulus VX <code>cvx/cumulus_cvx</code> supported container SONiC <code>sonic</code> supported container Dell FTOS <code>vr-ftosv/vr-dell_ftos</code> supported VM Mikrotik Router OS <code>vr-ros/vr-mikrotik_ros</code> supported VM Palo Alto PAN OS <code>vr-panos/vr-paloalto_panos</code> supported VM IPInfusion OcNOS <code>ipinfusion_ocnos</code> supported VM Keysight ixia-c-one <code>keysight_ixia-c-one</code> supported container Checkpoint Cloudguard <code>checkpoint_cloudguard</code> supported VM Linux container <code>linux</code> supported container Linux bridge <code>bridge</code> supported N/A OvS bridge <code>ovs-bridge</code> supported N/A mysocketio node <code>mysocketio</code> supported N/A <p>Refer to a specific kind documentation article for kind-specific details.</p>","boost":4},{"location":"manual/kinds/bridge/","title":"Linux bridge","text":"","boost":4},{"location":"manual/kinds/bridge/#linux-bridge","title":"Linux bridge","text":"<p>Containerlab can connect its nodes to a Linux bridge instead of interconnecting the nodes directly. This connectivity option is enabled with <code>bridge</code> kind and opens a variety of integrations that containerlab labs can have with workloads of other types.</p> <p>For example, by connecting a lab node to a bridge we can:</p> <ol> <li>allow a node to talk to any workload (VM, container, baremetal) which are connected to that bridge</li> <li>let a node to reach networks which are available via that bridge</li> <li>scale out containerlab labs by running separate labs in different hosts and get network reachability between them</li> <li>wiring nodes' data interfaces via a broadcast domain (linux bridge) and use vlans to making dynamic connections</li> </ol>","boost":4},{"location":"manual/kinds/bridge/#using-bridge-kind","title":"Using bridge kind","text":"<p>Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file, the bridge needs to be created and enabled first.</p> <p>Once the bridge is created, it needs to be referenced as a node inside the topology file:</p> <pre><code># topology documentation: http://containerlab.dev/lab-examples/ext-bridge/\nname: br01\ntopology:\nkinds:\nsrl:\ntype: ixrd2\nimage: ghcr.io/nokia/srlinux\nnodes:\nsrl1:\nkind: srl\nsrl2:\nkind: srl\nsrl3:\nkind: srl\n# note, that the bridge br-clab must be created manually\nbr-clab:\nkind: bridge\nlinks:\n- endpoints: [\"srl1:e1-1\", \"br-clab:eth1\"]\n- endpoints: [\"srl2:e1-1\", \"br-clab:eth2\"]\n- endpoints: [\"srl3:e1-1\", \"br-clab:eth3\"]\n</code></pre> <p>In the example above, node <code>br-clab</code> of kind <code>bridge</code> tells containerlab to identify it as a linux bridge and look for a bridge named <code>br-clab</code>.</p> <p>When connecting other nodes to a bridge, the bridge endpoint must be present in the <code>links</code> section.</p> <p>Note</p> <p>When choosing names of the interfaces that need to be connected to the bridge make sure that these names are not clashing with existing interfaces. In the example above we named interfaces <code>eth1</code>, <code>eth2</code>, <code>eth3</code> accordingly and ensured that none of these interfaces existed before in the root netns.  </p> <p>As a result of such topology definition, you will see bridge <code>br-clab</code> with three interfaces attached to it:</p> <pre><code>bridge name     bridge id               STP enabled     interfaces\nbr-clab         8000.6281eb7133d2       no              eth1\n                                                        eth2\n                                                        eth3\n</code></pre> <p>Containerlab automatically adds an iptables rule for the referenced bridges to allow forwarding over them. Namely, for a given bridge named <code>br-clab</code> containerlab will attempt to call the following iptables command during the lab deployment:</p> <pre><code>iptables -I FORWARD -i br-clab -j ACCEPT\n</code></pre> <p>This will ensure that traffic is forwarded when passing this particular bridge. Note, that once you destroy the lab, the rule will stay.</p> <p>Check out \"External bridge\" lab for a ready-made example on how to use bridges.</p>","boost":4},{"location":"manual/kinds/c8000/","title":"Cisco 8000","text":"<p>Cisco 8000 platform emulator is identified with <code>c8000</code> or <code>cisco_c8000</code> kind in the topology file.</p> <p>The 8000 emulator is an enhanced KVM hypervisor that emulates Cisco boards and chassis. The 8000 emulator VM is launched inside a container for ease of integration with ContainerLab.</p>","boost":4},{"location":"manual/kinds/c8000/#getting-cisco-8000-containerlab-docker-images","title":"Getting Cisco 8000 ContainerLab docker images","text":"<p>Cisco customers can contact their Cisco account team to get access to Cisco 8000 ContainerLab docker images.</p>","boost":4},{"location":"manual/kinds/c8000/#supported-platforms","title":"Supported platforms","text":"<ul> <li>Fixed form platforms   8101-32H, 8102-64H, 8201-32FH, 8201, 8202-32FH-M</li> <li>Modular chassis (8808 and 8804)   8800-LC-36FH, 8800-LC-48H, 8800-LC-36FH-M</li> </ul> <p>Additional platforms will be supported on \"as needed\" basis.</p>","boost":4},{"location":"manual/kinds/c8000/#host-server-requirements","title":"Host server requirements","text":"<ul> <li>Open vSwitch</li> <li>KVM</li> <li>Limit <code>/proc/sys/kernel/pid_max</code> to 1048575 <code>sysctl -w kernel.pid_max=1048575</code></li> </ul>","boost":4},{"location":"manual/kinds/c8000/#hardware-resource-requirements","title":"Hardware resource requirements","text":"<p>Memory and cpu usage depends on XR features enabled and control/data plane traffic</p> <ul> <li> <p>Fixed form platforms   Recommended 20GB memory and 4 cores per router</p> </li> <li> <p>Modular chassis   Recommended 64GB memory and 8 cores per router</p> </li> </ul>","boost":4},{"location":"manual/kinds/c8000/#managing-c8000-nodes","title":"Managing c8000 nodes","text":"<p>Info</p> <p>Cisco 8000 nodes may take a few minutes to come to XR prompt. To monitor boot progress: <pre><code>docker &lt;container-name/id&gt; logs -f\n</code></pre> Wait for <code>Router up</code> message.</p> SSHXR consolebashNetconf <p><code>ssh cisco@&lt;node-mgmt-address&gt;</code> Password: <code>cisco123</code></p> <p>to connect to a XR console (via telnet): <pre><code>docker exec -it &lt;container-name/id&gt; telnet 0 60000\n</code></pre></p> <p>to connect to a <code>bash</code> shell of a running c8000 container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>Netconf server runs on <code>830</code> port: <pre><code>ssh cisco@&lt;node-mgmt-address&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default credentials: <code>cisco:cisco123</code></p>","boost":4},{"location":"manual/kinds/c8000/#interface-naming-convention","title":"Interface naming convention","text":"<p>c8000 container uses the following naming convention for its managment and data interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>Hu0_0_0_X</code> - 100G data interface mapped to <code>HundredGigE0/0/0/X</code> internal interface.</li> <li><code>FH0_0_0_X</code> - 400G data interface mapped to <code>FourHundredGigE0/0/0/X</code> internal interface.</li> </ul> <p>When containerlab launches c8000 node, it will set IPv4 address as assigned by docker to the <code>eth0</code> interface and c8000 node will boot with this address configured for its <code>MgmtEth0</code>.</p> <pre><code>RP/0/RP0/CPU0:r1#sh ip int br\nWed Dec 21 12:04:13.049 UTC\n\nInterface                      IP-Address      Status          Protocol Vrf-Name\nMgmtEth0/RP0/CPU0/0            172.20.20.5     Up              Up       default\n</code></pre>","boost":4},{"location":"manual/kinds/c8000/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/c8000/#default-node-configuration","title":"Default node configuration","text":"<p>It is possible to launch nodes of <code>cisco_c8000</code> kind with a basic config or to provide a custom config file that will be used as a startup config instead.</p> <p>When a node is defined without <code>startup-config</code> statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node.</p>","boost":4},{"location":"manual/kinds/c8000/#user-defined-config","title":"User defined config","text":"<p>With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>name: c8201\ntopology:\nnodes:\nc8000:\nkind: cisco_c8000\nstartup-config: r1.cfg\n</code></pre> <p>When a config file is passed via <code>startup-config</code> parameter it will be used during an initial lab deployment. However, a config file that might be in the lab directory of a node takes precedence over the startup-config<sup>1</sup>.</p> <p>With such topology file containerlab is instructed to take a file <code>r1.cfg</code> from the current working directory and copy it to the lab directory for that specific node under the <code>/first-boot.cfg</code> name. This will result in this config acting as a startup-config for the node.</p> <p>To provide a user-defined config, take the default configuration template and add the necessary configuration commands without changing the rest of the file. This will result in proper automatic assignment of IP addresses to the management interface, as well as applying user-defined commands.</p>","boost":4},{"location":"manual/kinds/c8000/#lab-examples","title":"Lab examples","text":"<p>TBD</p> <ol> <li> <p>if startup config needs to be enforced, either deploy a lab with <code>--reconfigure</code> flag, or use <code>enforce-startup-config</code> setting.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/ceos/","title":"Arista cEOS","text":"<p>Arista cEOS is identified with <code>ceos</code> or <code>arista_ceos</code> kind in the topology file. The <code>ceos</code> kind defines a supported feature set and a startup procedure of a <code>ceos</code> node.</p> <p>cEOS nodes launched with containerlab comes up with</p> <ul> <li>their management interface <code>eth0</code> configured with IPv4/6 addresses as assigned by docker</li> <li>hostname assigned to the node name</li> <li>gNMI, Netconf and eAPI services enabled</li> <li><code>admin</code> user created with password <code>admin</code></li> </ul>","boost":4},{"location":"manual/kinds/ceos/#getting-ceos-image","title":"Getting cEOS image","text":"<p>Arista requires its users to register with arista.com before downloading any images. Once you created an account and logged in, go to the software downloads section and download ceos64 tar archive for a given release.</p> <p>Once downloaded, import the archive with docker:</p> <pre><code># import container image and save it under ceos:4.28.0F name\ndocker import cEOS64-lab-4.28.0F.tar.xz ceos:4.28.0F\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#managing-ceos-nodes","title":"Managing ceos nodes","text":"<p>Arista cEOS node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONFgNMI <p>to connect to a <code>bash</code> shell of a running ceos container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the ceos CLI <pre><code>docker exec -it &lt;container-name/id&gt; Cli\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh root@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>gNMI server is running over port 6030 in non-secure mode using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt;:6030 --insecure \\\n-u admin -p admin \\\ncapabilities\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/ceos/#interfaces-mapping","title":"Interfaces mapping","text":"<p>ceos container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code><sup>5</sup> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface</li> </ul> <p>When containerlab launches ceos node, it will set IPv4/6 addresses as assigned by docker to the <code>eth0</code> interface and ceos node will boot with that addresses configured. Data interfaces <code>eth1+</code> need to be configured with IP addressing manually.</p> ceos interfaces output <p>This output demonstrates the IP addressing of the linux interfaces of ceos node. <pre><code>bash-4.2# ip address\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/24 scope host lo\n    valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n    valid_lft forever preferred_lft forever\n&lt;SNIP&gt;\n5877: eth0@if5878: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default\n    link/ether 02:42:ac:14:14:02 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 172.20.20.2/24 brd 172.20.20.255 scope global eth0\n    valid_lft forever preferred_lft forever\n    inet6 2001:172:20:20::2/80 scope global\n    valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe14:1402/64 scope link\n    valid_lft forever preferred_lft forever\n</code></pre> This output shows how the linux interfaces are mapped into the ceos OS. <pre><code>ceos&gt;sh ip int br\n                                                                            Address\nInterface         IP Address           Status       Protocol           MTU    Owner\n----------------- -------------------- ------------ -------------- ---------- -------\nManagement0       172.20.20.2/24       up           up                1500\n\nceos&gt;sh ipv6 int br\nInterface       Status        MTU       IPv6 Address                     Addr State    Addr Source\n--------------- ------------ ---------- -------------------------------- ---------------- -----------\nMa0             up           1500       fe80::42:acff:fe14:1402/64       up            link local\n                                        2001:172:20:20::2/80             up            config\n</code></pre> As you see, the management interface <code>Ma0</code> inherits the IP address that docker assigned to ceos container management interface.</p>","boost":4},{"location":"manual/kinds/ceos/#user-defined-interface-mapping","title":"User-defined interface mapping","text":"<p>Note</p> <p>Supported in cEOS &gt;= 4.28.0F</p> <p>It is possible to make ceos nodes boot up with a user-defined interface layout. With the <code>binds</code> property, a user sets the path to the interface mapping file that will be mounted to a container and used during bootup. The underlying linux <code>eth</code> interfaces (used in the containerlab topology file) are mapped to cEOS interfaces in this file. The following shows an example of how this mapping file is structured:</p> <pre><code>{\n\"ManagementIntf\": {\n\"eth0\": \"Management1\"\n},\n\"EthernetIntf\": {\n\"eth1\": \"Ethernet1/1\",\n\"eth2\": \"Ethernet2/1\",\n\"eth3\": \"Ethernet27/1\",\n\"eth4\": \"Ethernet28/1\",\n\"eth5\": \"Ethernet3/1/1\",\n\"eth6\": \"Ethernet5/2/1\"\n}\n}\n</code></pre> <p>Linux's <code>eth0</code> interface is always used to map the management interface.</p> <p>With the following topology file, containerlab is instructed to take a <code>mymapping.json</code> file located in the same directory as the topology and mount that to the container as <code>/mnt/flash/EosIntfMapping.json</code>. This will result in this interface mapping being considered during the bootup of the node. The destination for that bind has to be <code>/mnt/flash/EosIntfMapping.json</code>.</p> <ol> <li>Craft a valid interface mapping file.</li> <li> <p>Use <code>binds</code> config option for a ceos node/kind to make this file available in the container's filesystem:</p> <pre><code>name: ceos\ntopology:\nnodes:\nceos1:\nkind: ceos\nimage: ceos:4.28.0F\nbinds:\n- mymapping.json:/mnt/flash/EosIntfMapping.json:ro # (1)!\nceos2: kind: ceos\nimage: ceos:4.28.0F\nbinds:\n- mymapping.json:/mnt/flash/EosIntfMapping.json:ro\nlinks:\n- endpoints: [\"ceos1:eth1\", \"ceos2:eth1\"]\n</code></pre> <ol> <li>If all ceos nodes use the same interface mapping file, it is easier to set the bind instruction on a kind level</li> </ol> <pre><code>    topology:\nkinds:\nceos:\nbinds:\n- mymapping.json:/mnt/flash/EosIntfMapping.json:ro\nnodes:\nceos1:\nkind: ceos\nimage: ceos:4.28.0F\nceos2: kind: ceos\nimage: ceos:4.28.0F\n</code></pre> <p>This way the bind is set only once, and nodes of <code>ceos</code> kind will have these binds applied.</p> </li> </ol>","boost":4},{"location":"manual/kinds/ceos/#additional-interface-naming-considerations","title":"Additional interface naming considerations","text":"<p>While many users will be fine with the default ceos naming of <code>eth</code>, some ceos users may find that they need to name their interfaces <code>et</code>. Interfaces named <code>et</code> provide consistency with the underlying interface mappings within ceos. This enables the correct operation of commands/features which depend on <code>et</code> format interface naming.</p> <p>In order to align interfaces in this manner, the <code>INTFTYPE</code> environment variable must be set to <code>et</code> in the topology definition file and the links which are defined must be named <code>et</code>, as opposed to <code>eth</code>. This naming requirement does not apply to the <code>eth0</code> interface automatically created by containerlab. This is only required for links that are used for interconnection with other elements in a topology.</p> <p>example:</p> <pre><code>topology:\ndefaults:\nenv:\nINTFTYPE: et\nnodes:\n# --snip--\nlinks:\n- endpoints: [\"ceos_rtr1:et1\", \"ceos_rtr2:et1\"]\n- endpoints: [\"ceos_rtr1:et2\", \"ceos_rtr3:et1\"]\n</code></pre> <p>If the only purpose of renaming the interfaces is to add breakouts (\"/1\", etc.) to the interface naming to match the future physical setup, it is possible to use underscores (\"_\") in the interface names.</p> <pre><code>name: ceos\ntopology:\nnodes:\nceos1:\nkind: ceos\nimage: ceos:4.28.0F\nceos2: kind: ceos\nimage: ceos:4.28.0F\nlinks:\n- endpoints: [\"ceos1:eth1_1\", \"ceos2:eth2_1_1\"]\n</code></pre> <p>This topology will be equivalent to <code>ceos1:Ethernet1/1</code> connected to <code>ceos2:Ethernet2/1/1</code>.</p> <p>Note</p> <p>This feature can not be used together with interface mapping. If the interface mapping is in use, all names must be redefined in the map and the underscore naming option will not work. Also, it's only possible to rename Ethernet interfaces this way, not management ports.</p>","boost":4},{"location":"manual/kinds/ceos/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/ceos/#node-configuration","title":"Node configuration","text":"<p>cEOS nodes have a dedicated <code>config</code> directory that is used to persist the configuration of the node. It is possible to launch nodes of <code>ceos</code> kind with a basic config or to provide a custom config file that will be used as a startup config instead.</p>","boost":4},{"location":"manual/kinds/ceos/#default-node-configuration","title":"Default node configuration","text":"<p>When a node is defined without <code>startup-config</code> statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node.</p> <pre><code># example of a topo file that does not define a custom config\n# as a result, the config will be generated from a template\n# and used by this node\nname: ceos\ntopology:\nnodes:\nceos:\nkind: ceos\n</code></pre> <p>The generated config will be saved by the path <code>clab-&lt;lab_name&gt;/&lt;node-name&gt;/flash/startup-config</code>. Using the example topology presented above, the exact path to the config will be <code>clab-ceos/ceos/flash/startup-config</code>.</p> <p>cEOS Ma0 interface will be configured with a random MAC address with <code>00:1c:73</code> OUI part. Containerlab will also create a <code>system_mac_address</code> file in the node's lab directory with the value of a System MAC address. The System MAC address value is calculated as <code>Ma0-MAC-addr + 1</code>.</p> <p>A default ipv4 route is also created with a next-hop of the management network to allow for outgoing connections.</p>","boost":4},{"location":"manual/kinds/ceos/#mgmt-vrf","title":"MGMT VRF","text":"<p>The default empty configuration supports placing the management interface into a VRF to isolate it from the main device routing table.  Passing the environment variable <code>CLAB_MGMT_VRF</code> in either the kind or node definition will activate this behavior, and alter the management services configuration to also reflect the management VRF.  You can duplicate this when using the <code>startup-config</code> by starting from the linked template below.</p> <pre><code># example topo file with management VRF\n# node1 will have vrf MGMT\n# node2 will have vrf FOO\nname: ceos_vrf\ntopology:\nkinds:\nceos:\nenv:\nCLAB_MGMT_VRF: MGMT\nnodes:\nnode1:\nkind: ceos\nnode2:\nkind: ceos\nenv:\nCLAB_MGMT_VRF: FOO\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#user-defined-config","title":"User defined config","text":"<p>It is possible to make ceos nodes to boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup config:</p> <pre><code>name: ceos_lab\ntopology:\nnodes:\nceos:\nkind: ceos\nstartup-config: myconfig.conf\n</code></pre> <p>When a config file is passed via <code>startup-config</code> parameter it will be used during an initial lab deployment. However, a config file that might be in the lab directory of a node takes precedence over the startup-config<sup>3</sup>.</p> <p>With such topology file containerlab is instructed to take a file <code>myconfig.conf</code> from the current working directory, copy it to the lab directory for that specific node under the <code>/flash/startup-config</code> name and mount that dir to the container. This will result in this config to act as a startup config for the node.</p> <p>It is possible to change the default config which every ceos node will start with with the following steps:</p> <ol> <li>Craft a valid startup configuration file<sup>2</sup>.</li> <li> <p>Use this file as a startup-config for ceos kind:</p> <pre><code>name: ceos\ntopology:\nkinds:\nceos:\nstartup-config: ceos-custom-startup.cfg\nnodes:\n# ceos1 will boot with ceos-custom-startup.cfg as set in the kind parameters\nceos1:\nkind: ceos\nimage: ceos:4.25.0F\n# ceos2 will boot with its own specific startup config, as it overrides the kind variables\nceos2: kind: ceos\nimage: ceos:4.25.0F\nstartup-config: node-specific-startup.cfg\nlinks:\n- endpoints: [\"ceos1:eth1\", \"ceos2:eth1\"]\n</code></pre> </li> </ol>","boost":4},{"location":"manual/kinds/ceos/#saving-configuration","title":"Saving configuration","text":"<p>In addition to cli commands such as <code>write memory</code> user can take advantage of the <code>containerlab save</code> command. It saves running cEOS configuration into a startup config file effectively calling the <code>write</code> CLI command.</p>","boost":4},{"location":"manual/kinds/ceos/#container-configuration","title":"Container configuration","text":"<p>To start an Arista cEOS node containerlab uses the configuration instructions described in Arista Forums<sup>1</sup>. The exact parameters are outlined below.</p> Startup commandEnvironment variables <p><code>/sbin/init systemd.setenv=INTFTYPE=eth systemd.setenv=ETBA=1 systemd.setenv=SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT=1 systemd.setenv=CEOS=1 systemd.setenv=EOS_PLATFORM=ceoslab systemd.setenv=container=docker systemd.setenv=MAPETH0=1 systemd.setenv=MGMT_INTF=eth0</code></p> <p><code>CEOS:1</code> <code>EOS_PLATFORM\":ceoslab</code> <code>container:docker</code> <code>ETBA:1</code> <code>SKIP_ZEROTOUCH_BARRIER_IN_SYSDBINIT:1</code> <code>INTFTYPE:eth</code> <code>MAPETH0:1</code> <code>MGMT_INTF:eth0</code></p>","boost":4},{"location":"manual/kinds/ceos/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a node directory for storing configuration artifacts. For <code>ceos</code> kind containerlab creates <code>flash</code> directory for each ceos node and mounts these folders by <code>/mnt/flash</code> paths.</p> <pre><code>\u276f tree clab-srlceos01/ceos\nclab-srlceos01/ceos\n\u2514\u2500\u2500 flash\n    \u251c\u2500\u2500 AsuFastPktTransmit.log\n    \u251c\u2500\u2500 debug\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 proc\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 modules\n    \u251c\u2500\u2500 fastpkttx.backup\n    \u251c\u2500\u2500 Fossil\n    \u251c\u2500\u2500 kickstart-config\n    \u251c\u2500\u2500 persist\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 local\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 messages\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 persistentRestartLog\n    \u2502\u00a0\u00a0 \u251c\u2500\u2500 secure\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 sys\n    \u251c\u2500\u2500 schedule\n    \u2502\u00a0\u00a0 \u2514\u2500\u2500 tech-support\n    \u2502\u00a0\u00a0     \u2514\u2500\u2500 ceos_tech-support_2021-01-14.0907.log.gz\n    \u251c\u2500\u2500 SsuRestoreLegacy.log\n    \u251c\u2500\u2500 SsuRestore.log\n    \u251c\u2500\u2500 system_mac_address\n    \u2514\u2500\u2500 startup-config\n\n9 directories, 11 files\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#copy-to-flash","title":"Copy to <code>flash</code>","text":"<p>If there is a need to copy ceos-specific configuration or override files to the ceos node in the topology use <code>.extras.ceos-copy-to-flash</code> config option. These files will be copied to the node's flash directory and evaluated on startup.</p> <pre><code>name: ceos\ntopology:\nnodes:\nceos1:\nkind: ceos\n...\nextras:\nceos-copy-to-flash:\n- ceos-config # (1)!\n- toggle_override\n</code></pre> <ol> <li>Paths are relative to the topology file. Absolute paths like <code>~/some/path</code> or <code>/some/path</code> are also possible.</li> </ol>","boost":4},{"location":"manual/kinds/ceos/#lab-examples","title":"Lab examples","text":"<p>The following labs feature a cEOS node:</p> <ul> <li>SR Linux and cEOS</li> </ul>","boost":4},{"location":"manual/kinds/ceos/#known-issues-or-limitations","title":"Known issues or limitations","text":"","boost":4},{"location":"manual/kinds/ceos/#cgroups-v1","title":"cgroups v1","text":"<p>In versions prior to EOS-4.28.0F, the ceos-lab image requires a cgroups v1 environment. For many users, this should not require any changes to the runtime environment. However, some Linux distributions (ref: #467) may be configured to use cgroups v2 out-of-the-box<sup>4</sup>, which will prevent ceos-lab image from booting. In such cases, the users will need to configure their system to utilize a cgroups v1 environment.  </p> <p>Consult your distribution's documentation for details regarding configuring cgroups v1 in case you see similar startup issues as indicated in #467.</p> <p>Starting with EOS-4.28.0F, ceos-lab will automatically determine whether the container host is using cgroups v1 or cgroups v2 and act appropriately. No configuration is required.</p> Switching to cgroup v1 in Ubuntu 21.04 <p>To switch back to cgroup v1 in Ubuntu 21+ users need to add a kernel parameter <code>systemd.unified_cgroup_hierarchy=0</code> to GRUB config. Below is a snippet of <code>/etc/default/grub</code> file with the added <code>systemd.unified_cgroup_hierarchy=0</code> parameter.</p> <p>Note that <code>sudo update-grub</code> is needed once changes are made to the file.</p> <pre><code># If you change this file, run 'update-grub' afterwards to update\n# /boot/grub/grub.cfg.\n# For full documentation of the options in this file, see:\n#   info -f grub -n 'Simple configuration'\nGRUB_DEFAULT=0\nGRUB_TIMEOUT_STYLE=hidden\nGRUB_TIMEOUT=0\nGRUB_DISTRIBUTOR=`lsb_release -i -s 2&gt; /dev/null || echo Debian`\nGRUB_CMDLINE_LINUX_DEFAULT=\"transparent_hugepage=never quiet splash systemd.unified_cgroup_hierarchy=0\"\nGRUB_CMDLINE_LINUX=\"\"\n</code></pre>","boost":4},{"location":"manual/kinds/ceos/#wsl","title":"WSL","text":"<p>When running under WSL2 ceos datapath might appear not working. As of Feb 2022 users would need to manually enter the following iptables rules inside ceos container:</p> <pre><code>sudo iptables -P INPUT ACCEPT\nsudo ip6tables -P INPUT ACCEPT\n</code></pre> <ol> <li> <p>https://eos.arista.com/ceos-lab-topo/ \u21a9</p> </li> <li> <p>feel free to omit the IP addressing for Management interface, as it will be configured by containerlab when ceos node boots.\u00a0\u21a9</p> </li> <li> <p>if startup config needs to be enforced, either deploy a lab with <code>--reconfigure</code> flag, or use <code>enforce-startup-config</code> setting.\u00a0\u21a9</p> </li> <li> <p>for example, Ubuntu 21.04 comes with cgroup v2 by default.\u00a0\u21a9</p> </li> <li> <p>interface name can also be <code>et</code> instead of <code>eth</code>.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/checkpoint_cloudguard/","title":"Check Point Cloudguard","text":"<p>Check Point Cloudguard virtualized security appliance is identified with <code>checkpoint_cloudguard</code> kind in the topology file. It is built using boxen project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/checkpoint_cloudguard/#getting-cloudguard-image","title":"Getting Cloudguard image","text":"<p>Users can obtain the qcow2 disk image for Check Point Cloudguard VM from the official download site. To build a containerlab-compatible container use boxen project.</p>","boost":4},{"location":"manual/kinds/checkpoint_cloudguard/#managing-check-point-cloudguard-nodes","title":"Managing Check Point Cloudguard nodes","text":"<p>Note</p> <p>Containers with Check Point Cloudguard VM inside will take ~5min to fully boot. You can monitor the progress with</p> <ul> <li><code>docker logs -f &lt;container-name&gt;</code> for boxen status reports</li> <li>and <code>docker exec -it &lt;container-name&gt; tail -f /console.log</code> to see the boot log messages.</li> </ul> <p>Check Point Cloudguard node launched with containerlab can be managed via the following interfaces:</p> bashCLIHTTPS <p>to connect to a <code>bash</code> shell of a running checkpoint_cloudguard container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>Note</p> <p>The shell access gives you access to the container that hosts the Qemu VM.</p> <p>to connect to the Cloudguard CLI <pre><code>ssh admin@&lt;container-name/id/IP-addr&gt;\n</code></pre></p> <p>Cloudguard OS comes with HTTPS server running on boot. You can access the Web UI using https schema <pre><code>curl https://&lt;container-name/id/IP-addr&gt;\n</code></pre></p> <p>You can expose container's 443 port with <code>ports</code> setting in containerlab and get access to the Web UI using your containerlab host IP.</p> <p>Info</p> <p>Default login credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/checkpoint_cloudguard/#interfaces-mapping","title":"Interfaces mapping","text":"<p>Check Point Cloudgard starts up with 8 available interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to the first data port of the VM</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches Cloudguard node, it assigns a static <code>10.0.0.5</code> IPv4 address to the VM's <code>eth0</code> interface. This interface is transparently stitched with container's <code>eth0</code> interface such that users can reach the management plane of the Cloudguard using containerlab's assigned IP.</p> <p>Data interfaces <code>eth1+</code> need to be configured with IP addressing manually using CLI or other available management interfaces.</p>","boost":4},{"location":"manual/kinds/crpd/","title":"Juniper cRPD","text":"<p>Juniper cRPD is identified with <code>crpd</code> or <code>juniper_crpd</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a <code>crpd</code> node.</p> <p>cRPD nodes launched with containerlab comes up pre-provisioned with SSH service enabled, <code>root</code> user created and NETCONF enabled.</p>","boost":4},{"location":"manual/kinds/crpd/#managing-crpd-nodes","title":"Managing cRPD nodes","text":"<p>Juniper cRPD node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONF <p>to connect to a <code>bash</code> shell of a running cRPD container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the cRPD CLI <pre><code>docker exec -it &lt;container-name/id&gt; cli\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh root@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>root:clab123</code></p>","boost":4},{"location":"manual/kinds/crpd/#interfaces-mapping","title":"Interfaces mapping","text":"<p>cRPD container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface</li> </ul> <p>When containerlab launches cRPD node, it will assign IPv4/6 address to the <code>eth0</code> interface. Data interface <code>eth1</code> needs to be configured with IP addressing manually.</p> cRPD interfaces output <p>This output demonstrates the IP addressing of the linux interfaces of cRPD node. <pre><code>\u276f docker exec -it clab-crpd-crpd bash\n\n===&gt;\n        Containerized Routing Protocols Daemon (CRPD)\nCopyright (C) 2020, Juniper Networks, Inc. All rights reserved.\n                                                                    &lt;===\n\nroot@crpd:/# ip a\n1: lo: &lt;LOOPBACK,UP,LOWER_UP&gt; mtu 65536 qdisc noqueue state UNKNOWN group default qlen 1000\n    link/loopback 00:00:00:00:00:00 brd 00:00:00:00:00:00\n    inet 127.0.0.1/8 scope host lo\n    valid_lft forever preferred_lft forever\n    inet6 ::1/128 scope host\n    valid_lft forever preferred_lft forever\n\n&lt;SNIP&gt;\n\n5767: eth0@if5768: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1450 qdisc noqueue state UP group default\n    link/ether 02:42:ac:14:14:03 brd ff:ff:ff:ff:ff:ff link-netnsid 0\n    inet 172.20.20.3/24 brd 172.20.20.255 scope global eth0\n    valid_lft forever preferred_lft forever\n    inet6 2001:172:20:20::3/80 scope global nodad\n    valid_lft forever preferred_lft forever\n    inet6 fe80::42:acff:fe14:1403/64 scope link\n    valid_lft forever preferred_lft forever\n5770: eth1@if5769: &lt;BROADCAST,MULTICAST,UP,LOWER_UP&gt; mtu 1500 qdisc noqueue state UP group default\n    link/ether b6:d3:63:f1:cb:7b brd ff:ff:ff:ff:ff:ff link-netnsid 1\n    inet6 fe80::b4d3:63ff:fef1:cb7b/64 scope link\n    valid_lft forever preferred_lft forever\n</code></pre> This output shows how the linux interfaces are mapped into the cRPD OS. <pre><code>root@crpd&gt; show interfaces routing\nInterface        State Addresses\nlsi              Up\ntunl0            Up    ISO   enabled\nsit0             Up    ISO   enabled\n                    INET6 ::172.20.20.3\n                    INET6 ::127.0.0.1\nlo.0             Up    ISO   enabled\n                    INET6 fe80::1\nip6tnl0          Up    ISO   enabled\n                    INET6 fe80::42a:e9ff:fede:a0e3\ngretap0          Down  ISO   enabled\ngre0             Up    ISO   enabled\neth1             Up    ISO   enabled\n                    INET6 fe80::b4d3:63ff:fef1:cb7b\neth0             Up    ISO   enabled\n                    INET  172.20.20.3\n                    INET6 2001:172:20:20::3\n                    INET6 fe80::42:acff:fe14:1403\n</code></pre> As you see, the management interface <code>eth0</code> inherits the IP address that docker assigned to cRPD container.</p>","boost":4},{"location":"manual/kinds/crpd/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/crpd/#node-configuration","title":"Node configuration","text":"<p>cRPD nodes have a dedicated <code>config</code> directory that is used to persist the configuration of the node. It is possible to launch nodes of <code>crpd</code> kind with a basic \"empty\" config or to provide a custom config file that will be used as a startup config instead.</p>","boost":4},{"location":"manual/kinds/crpd/#default-node-configuration","title":"Default node configuration","text":"<p>When a node is defined without <code>config</code> statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node.</p> <pre><code># example of a topo file that does not define a custom config\n# as a result, the config will be generated from a template\n# and used by this node\nname: crpd\ntopology:\nnodes:\ncrpd:\nkind: crpd\n</code></pre> <p>The generated config will be saved by the path <code>clab-&lt;lab_name&gt;/&lt;node-name&gt;/config/juniper.conf</code>. Using the example topology presented above, the exact path to the config will be <code>clab-crpd/crpd/config/juniper.conf</code>.</p>","boost":4},{"location":"manual/kinds/crpd/#user-defined-config","title":"User defined config","text":"<p>It is possible to make cRPD nodes to boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property of the node/kind a user sets the path to the config file that will be mounted to a container:</p> <pre><code>name: crpd_lab\ntopology:\nnodes:\ncrpd:\nkind: crpd\nstartup-config: myconfig.conf\n</code></pre> <p>With such topology file containerlab is instructed to take a file <code>myconfig.conf</code> from the current working directory, copy it to the lab directory for that specific node under the <code>/config/juniper.conf</code> name and mount that dir to the container. This will result in this config to act as a startup config for the node.</p>","boost":4},{"location":"manual/kinds/crpd/#saving-configuration","title":"Saving configuration","text":"<p>With <code>containerlab save</code> command it's possible to save running cRPD configuration into a file. The configuration will be saved by <code>/config/juniper.conf</code> path in the relevant node directory.</p>","boost":4},{"location":"manual/kinds/crpd/#license","title":"License","text":"<p>cRPD containers require a license file to have some features to be activated. With a <code>license</code> directive it's possible to provide a path to a license file that will be copied over to the nodes configuration directory by the <code>/config/license/safenet/junos_sfnt.lic</code> path and will get applied automatically on boot.</p>","boost":4},{"location":"manual/kinds/crpd/#container-configuration","title":"Container configuration","text":"<p>To launch cRPD, containerlab uses the deployment instructions that are provided in the TechLibrary as well as leveraging some setup steps outlined by Matt Oswalt in this blog post.</p> <p>The SSH service is already enabled for root login, so nothing is needed to be done additionally.</p> <p>The <code>root</code> user is created already with the <code>clab123</code> password.</p>","boost":4},{"location":"manual/kinds/crpd/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a node directory for storing configuration artifacts. For <code>crpd</code> kind containerlab creates <code>config</code> and <code>log</code> directories for each crpd node and mounts these folders by <code>/config</code> and <code>/var/log</code> paths accordingly.</p> <pre><code>\u276f tree clab-crpd/crpd\nclab-crpd/crpd\n\u251c\u2500\u2500 config\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 juniper.conf\n\u2502\u00a0\u00a0 \u251c\u2500\u2500 license\n\u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 safenet\n\u2502\u00a0\u00a0 \u2514\u2500\u2500 sshd_config\n\u2514\u2500\u2500 log\n    \u251c\u2500\u2500 cscript.log\n    \u251c\u2500\u2500 license\n    \u251c\u2500\u2500 messages\n    \u251c\u2500\u2500 mgd-api\n    \u251c\u2500\u2500 na-grpcd\n    \u251c\u2500\u2500 __policy_names_rpdc__\n    \u2514\u2500\u2500 __policy_names_rpdn__\n\n4 directories, 9 files\n</code></pre>","boost":4},{"location":"manual/kinds/crpd/#lab-examples","title":"Lab examples","text":"<p>The following labs feature cRPD node:</p> <ul> <li>SR Linux and cRPD</li> </ul>","boost":4},{"location":"manual/kinds/cvx/","title":"Cumulus VX","text":"<p>Cumulus VX is identified with <code>cvx</code> or <code>cumulus_cvx</code> kind in the topology file. The <code>cvx</code> kind defines a supported feature set and a startup procedure of a <code>cvx</code> node.</p> <p>CVX nodes launched with containerlab come up with:</p> <ul> <li>the management interface <code>eth0</code> is configured with IPv4/6 addresses as assigned by either the container runtime or DHCP</li> <li><code>root</code> user created with password <code>root</code></li> </ul>","boost":4},{"location":"manual/kinds/cvx/#mode-of-operation","title":"Mode of operation","text":"<p>CVX supports two modes of operation:</p> <ul> <li>Using only the container runtime -- this mode runs Cumulus VX container image directly inside the container runtime (e.g. Docker). Due to the lack of Cumulus VX kernel modules, some features are not supported, most notable one being MLAG. In order to use this mode, add <code>runtime: docker</code> under the cvx node definition (see also this example).</li> <li> <p>Using Firecracker micro-VMs -- this mode runs Cumulus VX inside a micro-VM on top of the native Cumulus kernel. This mode uses <code>ignite</code> runtime and is the default way of running CVX nodes.</p> <p>Warning</p> <p>This mode was broken in containerlab between v0.27.1 and v0.32.1 due to dependencies issues in ignite<sup>2</sup>.</p> </li> </ul> <p>Note</p> <p>When running in the default <code>ignite</code> runtime mode, the only host OS dependency is <code>/dev/kvm</code><sup>1</sup> required to support hardware-assisted virtualisation. Firecracker VMs are spun up inside a special \"sandbox\" container that has all the right tools and dependencies required to run micro-VMs.</p> <p>Additionally, containerlab creates a number of directories under <code>/var/lib/firecracker</code> for nodes running in <code>ignite</code> runtime to store runtime metadata; these directories are managed by containerlab.</p>","boost":4},{"location":"manual/kinds/cvx/#managing-cvx-nodes","title":"Managing cvx nodes","text":"<p>Cumulus VX node launched with containerlab can be managed via the following interfaces:</p> bashSSHgNMI <p>to attach to a <code>bash</code> shell of a running cvx container (only container ID is supported): <pre><code>docker attach &lt;container-id&gt;\n</code></pre> Use Docker's detach sequence (Ctrl+P+Q) to disconnect.</p> <p>SSH server is running on port 22 <pre><code>ssh root@&lt;container-name&gt;\n</code></pre></p> <p>gNMI server will be added in future releases.</p> <p>Info</p> <p>Default user credentials: <code>root:root</code></p>","boost":4},{"location":"manual/kinds/cvx/#user-defined-config","title":"User-defined config","text":"<p>It is possible to make cvx nodes to boot up with a user-defined config by passing any number of files along with their desired mount path:</p> <pre><code>name: cvx_lab\ntopology:\nnodes:\ncvx:\nkind: cvx\nbinds:\n- cvx/interfaces:/etc/network/interfaces\n- cvx/daemons:/etc/frr/daemons\n- cvx/frr.conf:/etc/frr/frr.conf\n</code></pre>","boost":4},{"location":"manual/kinds/cvx/#configuration-persistency","title":"Configuration persistency","text":"<p>When running inside the <code>ignite</code> runtime, all mount binds work one way -- from host OS to the cvx node, but not the other way around. Currently, it's up to a user to manually update individual files if configuration updates need to be persisted. This will be addressed in the future releases.</p>","boost":4},{"location":"manual/kinds/cvx/#lab-examples","title":"Lab examples","text":"<p>The following labs feature CVX node:</p> <ul> <li>Cumulus and FRR</li> <li>Cumulus in Docker runtime and Host</li> <li>Cumulus Linux Test Drive</li> <li>EVPN with MLAG and multi-homing scenarios</li> </ul>","boost":4},{"location":"manual/kinds/cvx/#known-issues-or-limitations","title":"Known issues or limitations","text":"<ul> <li>CVX in Ignite is always attached to the default docker bridge network</li> </ul> <ol> <li> <p>this device is already part of the linux kernel, therefore this can be read as \"no external dependencies are needed for running cvx with <code>ignite</code> runtime\".\u00a0\u21a9</p> </li> <li> <p>see https://github.com/srl-labs/containerlab/pull/1037 and https://github.com/srl-labs/containerlab/issues/1039 \u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/ext-container/","title":"External Container","text":"<p>Regular containerlab-managed nodes can be connected to externally managed containers. For instance, users may want to connect Network OS nodes launched by containerlab to some containers that are managed by other container orchestration tools to create advanced topologies.</p> <p>This connectivity option is enabled by adding nodes of <code>ext-container</code> kind to the topology.</p>","boost":4},{"location":"manual/kinds/ext-container/#using-ext-container-kind","title":"Using <code>ext-container</code> kind","text":"<p>Containerlab doesn't create nodes of type <code>ext-container</code>, it uses those nodes to let users create links to those externally managed containers from the nodes scheduled by containerlab.</p> <p>The below topology demonstrates how the node named <code>srl</code> created by containerlab can be connected to the external container named <code>external-node1</code> that is created by some other tool.</p> <pre><code>name: ext-cont\ntopology:\nnodes:\nsrl:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nexternal-node1: #(1)!\nkind: ext-container\nlinks:\n- endpoints: [\"srl:e1-1\", \"external-node1:eth1\"]\n</code></pre> <ol> <li>The name of the node of <code>ext-container</code> kind should match the container name as displayed by the <code>docker ps</code> command.</li> </ol> <p>By specifying the node <code>external-node1</code> as part of the containerlab topology, users can use this node name in the links section of the file and create links between containerlab-managed and externally-managed nodes.</p>","boost":4},{"location":"manual/kinds/ext-container/#interacting-with-external-container-nodes","title":"Interacting with External Container nodes","text":"<p>Even though External Container nodes are not scheduled by containerlab, it is possible to configure or interact with them using containerlab topology definition options.</p> <p>For example, when deploying containerlab topology, users can execute commands in the external containers using <code>exec</code> configuration option:</p> <pre><code>topology:\nnodes:\nsrl:\nkind: srl\nimage: ghcr.io/nokia/srlinux\nexternal-node1: #(1)!\nkind: ext-container\nexec:\n- ip address add 192.168.0.1/24 dev eth1\n</code></pre> <ol> <li><code>external-node1</code> is the name of a container launched outside of containerlab. In the case of a Docker runtime, this is a name displayed by <code>docker ps</code> command.</li> </ol>","boost":4},{"location":"manual/kinds/ipinfusion-ocnos/","title":"IPInfusion OcNOS","text":"<p>IPInfusion OcNOS virtualized router is identified with <code>ipinfusion_ocnos</code> kind in the topology file. It is built using either boxen project or hellt/vrnetlab and essentially is a Qemu VM packaged in a docker container format.</p> <p>ipinfusion_ocnos nodes launched with containerlab come up pre-provisioned with SSH, and NETCONF services enabled.</p> <p>Warning</p> <p>OcNOS VM disk images need to be altered to support telnet serial access and ethX interfaces name style. This can be done by modifying the grub config file, as shown here](https://github.com/hellt/vrnetlab/pull/99).</p>","boost":4},{"location":"manual/kinds/ipinfusion-ocnos/#managing-ipinfusion_ocnos-nodes","title":"Managing ipinfusion_ocnos nodes","text":"<p>Note</p> <p>Containers with OcNOS inside will take ~3min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code> and <code>docker exec -it &lt;container-name&gt; tail -f /console.log</code>.</p> <p>IPInfusion OcNOS node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONF <p>to connect to a <code>bash</code> shell of a running ipinfusion_ocnos container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the OcNOS CLI <pre><code>ssh ocnos@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh ocnos@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>ocnos:ocnos</code></p>","boost":4},{"location":"manual/kinds/ipinfusion-ocnos/#interfaces-mapping","title":"Interfaces mapping","text":"<p>ipinfusion_ocnos container can have up to 144 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of OcNOS line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches ipinfusion_ocnos node, it will assign IPv4 address to the <code>eth0</code> interface. This address can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> need to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/","title":"Keysight IXIA-C One","text":"<p>Keysight ixia-c-one is a single-container distribution of ixia-c, which in turn is Keysight's reference implementation of Open Traffic Generator API (OTG).</p> <p>What is IXIA-C?</p> <p>Ixia-c is a modern, powerful and API-driven traffic generator designed to cater to the needs of hyperscalers, network hardware vendors and hobbyists alike.</p> <p>It is available for free and distributed / deployed as a multi-container application consisting of a controller, a traffic-engine and an app-usage-reporter.</p> <p>Users can pull ixia-c-one container image from Github Container Registry.</p> <p>The corresponding node in containerlab is identified with <code>keysight_ixia-c-one</code> kind in the topology file. Upon boot up, it comes up with:</p> <ul> <li>management interface <code>eth0</code> configured with IPv4/6 addresses as assigned by the container runtime</li> <li>hostname assigned to the node name</li> <li>HTTPS service enabled on port 443 (for client SDK to push configuration and fetch metrics)</li> </ul>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#managing-ixia-c-one-nodes","title":"Managing ixia-c-one nodes","text":"<p>ixia-c-one is a \"docker in docker\" container hosting two kinds of ixia-c containers internally:</p> <ul> <li>API endpoint container that also manages configuration across multiple test ports. This container has the name <code>ixia-c-controller</code>.</li> <li>A set of containers acting as test ports (i.e., for generating or processing traffic, emulating protocols, etc.). Each container represents a traffic generator's port and gets created for each endpoint defined in the containerlab file.   These containers are named <code>ixia-c-port-dp-ethX</code>, where <code>X</code> matches the interface number given in the clab file.</li> </ul> <p>Request and response to the API endpoint are driven by Open Traffic Generator API and can be exercised in the following two ways:</p> Using SDKUsing <code>curl</code> <p>Using SDK is the preferred way of interacting with OTG devices. Implementations listed in the SDK chapter below provide references to SDK clients in different languages along with examples.</p> <p>Test case designers create test cases using SDK in one of the supported languages and leverage native language toolchain to test/execute the tests. Being API-first, Open Traffic Generator compliant implementations provide full configuration flexibility over the API.</p> <p>SDK clients use HTTPS to interface with the OTG API.</p> <pre><code># fetch configuration that was last pushed to ixia-c-one\n# clab-ixia-c-ixia-c-one is a container name allocated by clab for ixia node\ncurl -kL https://clab-ixia-c-ixia-c-one/config\n\n# fetch flow metrics\ncurl -kL https://clab-ixia-c-ixia-c-one/results/metrics -d '{\"choice\": \"flow\"}'\n</code></pre>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#sdk","title":"SDK","text":"<p>Client SDK for configuring ixia-c is available in various languages, most prevalent being gosnappi and snappi.</p> <p>The example below demonstrates how gosnappi - A Go-based SDK client - can be used to configure/run a traffic test and evaluate results.</p> <p>IPv4 Forwarding test is setup in the following way:</p> <p>Endpoints: <code>OTG 1.1.1.1 -----&gt; 1.1.1.2 DUT 2.2.2.1 ------&gt; OTG 2.2.2.2</code> Static Route on DUT: <code>20.20.20.0/24 -&gt; 2.2.2.2</code> TCP flow from OTG: <code>10.10.10.1 -&gt; 20.20.20.1+</code></p> <ol> <li>install go and init a module <pre><code>mkdir tests &amp;&amp; cd tests\ngo mod init tests\n</code></pre></li> <li>Download gosnappi release. gosnappi version needs to be compatible to a given release of ixia-c and can be checked at https://github.com/open-traffic-generator/ixia-c/releases <pre><code>go get github.com/open-traffic-generator/snappi/gosnappi@v0.7.18\n</code></pre></li> <li>download a basic IPv4 forwarding test <pre><code>curl -LO https://raw.githubusercontent.com/open-traffic-generator/snappi-tests/main/scripts/ipv4_forwarding.go\n</code></pre></li> <li>run the test with the MAC address of the <code>1.1.1.2</code> interface, which is located on the DUT as per the test diagram. <pre><code>go run ipv4_forwarding.go -dstMac=\"&lt;MAC address&gt;\"\n</code></pre></li> </ol>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#interfaces-mapping","title":"Interfaces mapping","text":"<p>ixia-c-one container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li>The other interfaces (eth1, eth2, etc) are the data interfaces acting as traffic generation ports</li> </ul> <p>When containerlab launches ixia-c-one node, it will set IPv4/6 addresses as assigned by docker to the <code>eth0</code> interface and ixia-c-one node will boot with that addresses configured.</p> <p>Data interfaces <code>eth1+</code> have to be configured with IP addressing manually if needed (as in the Layer3 forwarding test example). IP addresses are required when the test port needs to reply to ARP/ND queries from the Device Under Test. These addresses can be configured on non-<code>eth0</code> ports.</p> <p>Examples below show how test designer can configure IP address on eth1 data port of a parent container named <code>ixiac-one</code>:</p> IPv4 configurationIPv6 configuration <pre><code># set ipv4 addr\ndocker exec -it ixiac-one bash -c \"./ifcfg add eth2 2.2.2.2 24\" # (1)!\n# remove ipv4 addr\ndocker exec -it ixiac-one bash -c \"./ifcfg del eth2 2.2.2.2 24\"\n</code></pre> <ol> <li>Note the relative path for <code>./ifcfg</code> script. This is important, as there is a global script that is available by the <code>ifcfg</code> path.</li> </ol> <pre><code># set ipv6 addr on eth1 iface\ndocker exec -it ixiac-one bash -c \"./ifcfg add eth1 11::1 64\"\n# remove ipv6 addr on eth1 iface\ndocker exec -it ixiac-one bash -c \"./ifcfg del eth1 11::1 64\"\n</code></pre>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#features-and-options","title":"Features and options","text":"<p>The free version of Ixia-c supports generation of L2 and L3 traffic to test forwarding of Ethernet, IPv4 and IPv6 traffic by switches and routers. For technical support and queries, please log requests at open-traffic-generator/ixia-c or contact us via Slack.</p> <p>The commercial version of Ixia-c supports ARP/ND/Auto destination MAC resolution in data traffic, IPv4 and IPv6 BGP with IPv4 and IPv6 Routes and ISIS with IPv4 and IPv6 routes<sup>1</sup>.</p>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#lab-examples","title":"Lab examples","text":"<p>The following labs feature Keysight ixia-c-one node:</p> <ul> <li>Keysight Ixia-c and Nokia SR Linux</li> </ul>","boost":4},{"location":"manual/kinds/keysight_ixia-c-one/#known-issues-or-limitations","title":"Known issues or limitations","text":"<ol> <li>For L3 traffic tests using the free version , there is no in-built support of ARP and ND.    This can be worked around by manually setting IP address on the receive interface (as explained in Interfaces mapping section above) and by learning the MAC of the connected DUT using external means such as gnmi/ssh/reading it from CLI and using it when generating packets.    This limitation will be removed in the ixia-c-one free version in future releases where it is planned to support ARP/ND Request and Reply for emulated interfaces.  </li> </ol> <ol> <li> <p>Please contact Keysight support for further information regarding this if needed.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/linux/","title":"Linux container","text":"","boost":4},{"location":"manual/kinds/linux/#linux-container","title":"Linux container","text":"<p>Labs deployed with containerlab are endlessly flexible, mostly because containerlab can spin up and wire regular containers as part of the lab topology.</p> <p>Nowadays more and more workloads are packaged into containers, and containerlab users can nicely integrate them in their labs following a familiar docker' compose-like syntax. As long as the networking domain is considered, the most common use case for bare linux containers is to introduce \"clients\" or traffic generators which are connected to the network nodes or host telemetry/monitoring stacks.</p> <p>But, of course, you are free to choose which container to add into your lab, there is not restriction to that!</p>","boost":4},{"location":"manual/kinds/linux/#using-linux-containers","title":"Using linux containers","text":"<p>As with any other node, the linux container is a node of a specific kind, <code>linux</code> in this case.</p> <pre><code># a simple topo of two alpine containers connected with each other\nname: demo\ntopology:\nnodes:\nn1:\nkind: linux\nimage: alpine:latest\nn2:\nkind: linux\nimage: alpine:latest\nlinks:\n- endpoints: [\"n1:eth1\",\"n2:eth1\"]\n</code></pre> <p>With a topology file like that, the nodes will start and both containers will have <code>eth1</code> link available.</p> <p>Containerlab tries to deliver the same level of flexibility in container configuration as docker-compose has. With linux containers it is possible to use the following node configuration parameters:</p> <ul> <li>image - to set an image source for the container</li> <li>binds - to mount files from the host to a container</li> <li>ports - to expose services running in the container to a host</li> <li>env - to set environment variables</li> <li>user - to set a user that will be used inside the container system</li> <li>cmd - to provide a command that will be executed when the container is started</li> <li>publish - to expose container' service via mysocket.io integration</li> </ul> <p>Note</p> <p>Nodes of <code>linux</code> kind will have a <code>on-failure</code> restart policy when run with docker runtime. This means that if container fails/exits with a non zero return code, docker will restart this container automatically. When restarted, the container will loose all non-<code>eth0</code> interfaces. These can be re-added manually with tools veth command.</p>","boost":4},{"location":"manual/kinds/ovs-bridge/","title":"Openvswitch bridge","text":"<p>Similar to linux bridge capability, containerlab allows to connect nodes to an Openvswitch (Ovs) bridge. Ovs bridges offers even more connectivity options compared to classic Linux bridge, as well as it allows to create stretched L2 domain by means of tunneled interfaces (vxlan).</p>","boost":4},{"location":"manual/kinds/ovs-bridge/#using-ovs-bridge-kind","title":"Using ovs-bridge kind","text":"<p>Containerlab doesn't create bridges on users behalf, that means that in order to use a bridge in the topology definition file, the Ovs bridge needs to be created first.</p> <p>Once the bridge is created, it has to be referenced as a node inside the topology file:</p> <pre><code>name: ovs\ntopology:\nnodes:\nmyovs:\nkind: ovs-bridge\nceos:\nkind: ceos\nimage: ceos:latest\nlinks:\n- endpoints: [\"myovs:ovsp1\", \"srl:eth1\"]\n</code></pre> <p>In the example above, node <code>myovs</code> of kind <code>ovs-bridge</code> tells containerlab to identify it as a Ovs bridge and look for a bridge named <code>myovs</code>.</p> <p>When connecting other nodes to a bridge, the bridge endpoint must be present in the <code>links</code> section.</p> <p>Note</p> <p>When choosing names of the interfaces that need to be connected to the bridge make sure that these names are not clashing with existing interfaces. In the example above we attach a single interfaces named <code>ovsp1</code> to the Ovs bridge with a name <code>myovs</code>. Before that, we ensured that no other interfaces named <code>ovsp1</code> existed.</p> <p>As a result of such topology definition, you will see the Ovs bridge <code>br-clab</code> with three interfaces attached to it:</p> <pre><code>\u276f ovs-vsctl show\n918a3466-4368-4167-9162-f2cf80a0c106\n    Bridge myovs\n        Port myovs\n            Interface myovs\n                type: internal\n        Port eth1\n            Interface ovsp1\n    ovs_version: \"2.13.1\"\n</code></pre>","boost":4},{"location":"manual/kinds/sonic-vs/","title":"SONiC","text":"<p>SONiC is identified with <code>sonic-vs</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a <code>sonic-vs</code> node.</p> <p>Note</p> <p>To build a <code>sonic-vs</code> docker image:</p> <ol> <li>Leverage automated scripts provided by @antongisli</li> <li>or consult with the SONiC build documentation and create the docker images with <code>PLATFORM=vs</code> yourself.</li> </ol> <p>sonic-vs nodes launched with containerlab come without any additional configuration.</p>","boost":4},{"location":"manual/kinds/sonic-vs/#getting-sonic-images","title":"Getting Sonic images","text":"<p>Apparently, it is still tricky to find sonic container images, as their prime distribution format is a VM. Several discussions happened around creating a container build pipeline and a few projects were created.</p> <p>One of the latest known working images can be found here.</p>","boost":4},{"location":"manual/kinds/sonic-vs/#managing-sonic-vs-nodes","title":"Managing sonic-vs nodes","text":"<p>SONiC node launched with containerlab can be managed via the following interfaces:</p> bashCLI <p>to connect to a <code>bash</code> shell of a running sonic-vs container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the sonic-vs CLI (vtysh) <pre><code>docker exec -it &lt;container-name/id&gt; vtysh\n</code></pre></p>","boost":4},{"location":"manual/kinds/sonic-vs/#interfaces-mapping","title":"Interfaces mapping","text":"<p>sonic-vs container uses the following mapping for its linux interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data (front-panel port) interface</li> </ul> <p>When containerlab launches sonic-vs node, it will assign IPv4/6 address to the <code>eth0</code> interface. Data interface <code>eth1</code> mapped to <code>Ethernet0</code> port and needs to be configured with IP addressing manually. See Lab examples for exact configurations.</p>","boost":4},{"location":"manual/kinds/sonic-vs/#lab-examples","title":"Lab examples","text":"<p>The following labs feature sonic-vs node:</p> <ul> <li>SR Linux and sonic-vs</li> </ul>","boost":4},{"location":"manual/kinds/srl/","title":"Nokia SR Linux","text":"<p>Nokia SR Linux NOS is identified with <code>srl</code> or <code>nokia_srlinux</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a node.</p>","boost":4},{"location":"manual/kinds/srl/#getting-sr-linux-image","title":"Getting SR Linux image","text":"<p>Nokia SR Linux is the first commercial Network OS with a free and open distribution model. Everyone can pull SR Linux container from a public registry:</p> <pre><code># pull latest available release\ndocker pull ghcr.io/nokia/srlinux\n</code></pre> <p>To pull a specific version, use tags that match the released version and are listed in the srlinux-container-image repo.</p>","boost":4},{"location":"manual/kinds/srl/#managing-sr-linux-nodes","title":"Managing SR Linux nodes","text":"<p>There are many ways to manage SR Linux nodes, ranging from classic CLI management all the way up to the gNMI programming.</p> bashCLI/SSHgNMIJSON-RPC <p>to connect to a <code>bash</code> shell of a running SR Linux container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the SR Linux CLI <pre><code>docker exec -it &lt;container-name/id&gt; sr_cli\n</code></pre> or with SSH <code>ssh admin@&lt;container-name&gt;</code></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --skip-verify \\\n-u admin -p admin \\\n-e json_ietf \\\nget --path /system/name/host-name\n</code></pre></p> <p>SR Linux has a JSON-RPC interface running over ports 80/443 for HTTP/HTTPS schemas.</p> <p>HTTPS server uses the same TLS certificate as gNMI server.</p> <p>Here is an example of getting version information with JSON-RPC: <pre><code>curl http://admin:admin@clab-srl-srl/jsonrpc -d @- &lt;&lt; EOF\n{\n    \"jsonrpc\": \"2.0\",\n    \"id\": 0,\n    \"method\": \"get\",\n    \"params\":\n    {\n        \"commands\":\n        [\n            {\n                \"path\": \"/system/information/version\",\n                \"datastore\": \"state\"\n            }\n        ]\n    }\n}\nEOF\n</code></pre></p> <p>Info</p> <p>Default credentials<sup>4</sup>: <code>admin:NokiaSrl1!</code> Containerlab will automatically enable public-key authentication for <code>root</code>, <code>admin</code> and <code>linuxadmin</code> users if public key files are found at <code>~/.ssh</code> directory<sup>1</sup>.</p>","boost":4},{"location":"manual/kinds/srl/#interfaces-mapping","title":"Interfaces mapping","text":"<p>SR Linux system expects interfaces inside the container to be named in a specific way - <code>eX-Y</code> - where <code>X</code> is the line card index, <code>Y</code> is the port.</p> <p>With that naming convention in mind:</p> <ul> <li><code>e1-1</code> - first ethernet interface on line card #1</li> <li><code>e1-2</code> - second interface on line card #1</li> <li><code>e2-1</code> - first interface on line card #2</li> </ul> <p>These interface names are seen in the Linux shell; however, when configuring the interfaces via SR Linux CLI, the interfaces should be named as <code>ethernet-X/Y</code> where <code>X/Y</code> is the <code>linecard/port</code> combination.</p> <p>Interfaces can be defined in a non-sequential way, for example:</p> <pre><code>  links:\n# srlinux port ethernet-1/5 is connected to sros port 2\n- endpoints: [\"srlinux:e1-5\", \"sros:eth2\"]\n</code></pre>","boost":4},{"location":"manual/kinds/srl/#breakout-interfaces","title":"Breakout interfaces","text":"<p>If the interface is intended to be configured as a breakout interface, its name must be changed accordingly.</p> <p>The breakout interfaces will have the name <code>eX-Y-Z</code> where <code>Z</code> is the breakout port number. For example, if interface <code>ethernet-1/3</code> on an IXR-D3 system is meant to act as a breakout 100Gb to 4x25Gb, then the interfaces in the topology must use the following naming:</p> <pre><code>  links:\n# srlinux's first breakout port ethernet-1/3/1 is connected to sros port 2\n- endpoints: [\"srlinux:e1-3-1\", \"sros:eth2\"]\n</code></pre>","boost":4},{"location":"manual/kinds/srl/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/srl/#types","title":"Types","text":"<p>For SR Linux nodes <code>type</code> defines the hardware variant that this node will emulate.</p> <p>The available type values are: <code>ixrd1</code>, <code>ixrd2</code>, <code>ixrd3</code>, <code>ixrd2l</code>, <code>ixrd3l</code>, <code>ixrd5</code>, <code>ixrh2</code> and <code>ixrh3</code>, which correspond to a hardware variant of Nokia 7220 IXR chassis.</p> <p>Nokia 7250 IXR chassis identified with types <code>ixr6e</code> and <code>ixr10e</code> require a valid license to boot.</p> <p>If type is not set in the clab file <code>ixrd2</code> value will be used by containerlab.</p> <p>Based on the provided type, containerlab will generate the topology file that will be mounted to the SR Linux container and make it boot in a chosen HW variant.</p>","boost":4},{"location":"manual/kinds/srl/#node-configuration","title":"Node configuration","text":"<p>SR Linux uses a <code>/etc/opt/srlinux/config.json</code> file to persist its configuration. By default, containerlab starts nodes of <code>srl</code> kind with a basic \"default\" config, and with the <code>startup-config</code> parameter, it is possible to provide a custom config file that will be used as a startup one.</p>","boost":4},{"location":"manual/kinds/srl/#default-node-configuration","title":"Default node configuration","text":"<p>When a node is defined without the <code>startup-config</code> statement present, containerlab will make additional configurations on top of the factory config:</p> <pre><code># example of a topo file that does not define a custom startup-config\n# as a result, the default configuration will be used by this node\nname: srl_lab\ntopology:\nnodes:\nsrl1:\nkind: srl\ntype: ixrd3\n</code></pre> <p>The generated config will be saved by the path <code>clab-&lt;lab_name&gt;/&lt;node-name&gt;/config/config.json</code>. Using the example topology presented above, the exact path to the config will be <code>clab-srl_lab/srl1/config/config.json</code>.</p> <p>Additional configurations that containerlab adds on top of the factory config:</p> <ul> <li>enabling interfaces (<code>admin-state enable</code>) referenced in the topology's <code>links</code> section</li> <li>enabling LLDP</li> <li>enabling gNMI/JSON-RPC</li> <li>creating tls server certificate</li> </ul>","boost":4},{"location":"manual/kinds/srl/#user-defined-startup-config","title":"User defined startup config","text":"<p>It is possible to make SR Linux nodes boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property of the node/kind a user sets the path to the local config file that will be used as a startup config.</p> <p>The startup configuration file can be provided in two formats:</p> <ul> <li>full SR Linux config in JSON format</li> <li>partial config in SR Linux CLI format</li> </ul>","boost":4},{"location":"manual/kinds/srl/#cli","title":"CLI","text":"<p>A typical lab scenario is to make nodes boot with a pre-configured use case. The easiest way to do that is to capture the intended changes as CLI commands.</p> <p>On SR Linux, users can configure the system and capture the changes in the form of CLI instructions using the <code>info</code> command. These CLI commands can be saved in a file<sup>3</sup> and used as a startup configuration.</p> CLI config examples <p>these snippets can be the contents of <code>myconfig.cli</code> file referenced in the topology below</p> Regular configFlat config <pre><code>network-instance default {\ninterface ethernet-1/1.0 {\n}\ninterface ethernet-1/2.0 {\n}\nprotocols {\nbgp {\nadmin-state enable\nautonomous-system 65001\nrouter-id 10.0.0.1\n            group rs {\npeer-as 65003\nipv4-unicast {\nadmin-state enable\n}\n}\nneighbor 192.168.13.2 {\npeer-group rs\n            }\n}\n}\n}\n</code></pre> <pre><code>set / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp router-id 10.10.10.1\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp group ibgp export-policy export-lo\nset / network-instance default protocols bgp neighbor 192.168.1.2 admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-group ibgp\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-as 65001\n</code></pre> <pre><code>name: srl_lab\ntopology:\nnodes:\nsrl1:\nkind: srl\ntype: ixrd3\nimage: ghcr.io/nokia/srlinux\n# a path to the partial config in CLI format relative to the current working directory\nstartup-config: myconfig.cli\n</code></pre> <p>In that case, SR Linux will first boot with the default configuration, and then the CLI commands from the <code>myconfig.cli</code> will be applied. Note, that no entering into the candidate config, nor explicit commit is required to be part of the CLI configuration snippets.</p>","boost":4},{"location":"manual/kinds/srl/#json","title":"JSON","text":"<p>SR Linux persists its configuration as a JSON file that can be found by the <code>/etc/opt/srlinux/config.json</code> path. Users can use this file as a startup configuration like that:</p> <pre><code>name: srl_lab\ntopology:\nnodes:\nsrl1:\nkind: srl\ntype: ixrd3\nimage: ghcr.io/nokia/srlinux\n# a path to the full config in JSON format relative to the current working directory\nstartup-config: myconfig.json\n</code></pre> <p>Containerlab will take the <code>myconfig.json</code> file, copy it to the lab directory for that specific node under the <code>config.json</code> name, and mount that directory to the container. This will result in this config acting as a startup-config for the node.</p>","boost":4},{"location":"manual/kinds/srl/#saving-configuration","title":"Saving configuration","text":"<p>As was explained in the Node configuration section, SR Linux containers can make their config persistent because config files are provided to the containers from the host via the bind mount.</p> <p>When a user configures the SR Linux node, the changes are saved into the running configuration stored in memory. To save the running configuration as a startup configuration, the user needs to execute the <code>tools system configuration save</code> CLI command. This command will write the config to the <code>/etc/opt/srlinux/config.json</code> file that holds the startup-config and is exposed to the host.</p> <p>SR Linux node also supports the <code>containerlab save -t &lt;topo-file&gt;</code> command, which will execute the command to save the running-config on all lab nodes. For SR Linux node, the <code>tools system configuration save</code> will be executed:</p> <pre><code>\u276f containerlab save -t quickstart.clab.yml\nINFO[0000] Parsing &amp; checking topology file: quickstart.clab.yml\nINFO[0001] saved SR Linux configuration from leaf1 node. Output:\n/system:\n    Saved current running configuration as initial (startup) configuration '/etc/opt/srlinux/config.json'\n\nINFO[0001] saved SR Linux configuration from leaf2 node. Output:\n/system:\n    Saved current running configuration as initial (startup) configuration '/etc/opt/srlinux/config.json'\n</code></pre>","boost":4},{"location":"manual/kinds/srl/#user-defined-custom-agents-for-sr-linux-nodes","title":"User defined custom agents for SR Linux nodes","text":"<p>SR Linux supports custom \"agents\", i.e. small independent pieces of software that extend the functionality of the core platform and integrate with the CLI and the rest of the system. To deploy an agent, a YAML configuration file must be placed under <code>/etc/opt/srlinux/appmgr/</code>. This feature adds the ability to copy agent YAML file(s) to the config directory of a specific SRL node, or all such nodes.</p> <pre><code>name: srl_lab_with_custom_agents\ntopology:\nnodes:\nsrl1:\nkind: srl\n...\nextras:\nsrl-agents:\n- path1/my_custom_agent.yml\n- path2/my_other_agent.yml\n</code></pre>","boost":4},{"location":"manual/kinds/srl/#tls","title":"TLS","text":"<p>By default, containerlab will generate TLS certificates and keys for each SR Linux node of a lab. The TLS-related files that containerlab creates are located in the TLS directory, which can be found by the <code>&lt;lab-directory&gt;/.tls/</code> path. Here is a list of files that containerlab creates relative to the TLS directory:</p> <ol> <li>CA certificate - <code>./ca/ca.pem</code></li> <li>CA private key - <code>./ca/ca.key</code></li> <li>Node certificate - <code>./&lt;node-name&gt;/&lt;node-name&gt;.pem</code></li> <li>Node private key - <code>./&lt;node-name&gt;/&lt;node-name&gt;.key</code></li> </ol> <p>The generated TLS files will persist between lab deployments. This means that if you destroyed a lab and deployed it again, the TLS files from the initial lab deployment will be used.</p> <p>In case user-provided certificates/keys need to be used, the <code>ca.pem</code>, <code>&lt;node-name&gt;.pem</code> and <code>&lt;node-name&gt;.key</code> files must be copied by the paths outlined above for containerlab to take them into account when deploying a lab.</p> <p>In case only <code>ca.pem</code> and <code>ca.key</code> files are provided, the node certificates will be generated using these CA files.</p> <p>Nokia SR Linux nodes support setting of SANs.</p>","boost":4},{"location":"manual/kinds/srl/#license","title":"License","text":"<p>SR Linux container can run without a license . In that license-less mode, the datapath is limited to 1000 PPS and the sr_linux process will restart once a week.</p> <p>The license file lifts these limitations and a path to it can be provided with <code>license</code> directive.</p>","boost":4},{"location":"manual/kinds/srl/#container-configuration","title":"Container configuration","text":"<p>To start an SR Linux NOS containerlab uses the configuration that is described in SR Linux Software Installation Guide</p> Startup commandSyscallsEnvironment variables <p><code>sudo bash -c /opt/srlinux/bin/sr_linux</code></p> <pre><code>net.ipv4.ip_forward = \"0\"\nnet.ipv6.conf.all.disable_ipv6 = \"0\"\nnet.ipv6.conf.all.accept_dad = \"0\"\nnet.ipv6.conf.default.accept_dad = \"0\"\nnet.ipv6.conf.all.autoconf = \"0\"\nnet.ipv6.conf.default.autoconf = \"0\"\n</code></pre> <p><code>SRLINUX=1</code></p>","boost":4},{"location":"manual/kinds/srl/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a lab directory for storing configuration artifacts. For <code>srl</code> kind, containerlab creates directories for each node of that kind.</p> <pre><code>~/clab/clab-srl02\n\u276f ls -lah srl1\ndrwxrwxrwx+ 6 1002 1002   87 Dec  1 22:11 config\n-rw-r--r--  1 root root  233 Dec  1 22:11 topology.clab.yml\n</code></pre> <p>The <code>config</code> directory is mounted to container's <code>/etc/opt/srlinux/</code> path in <code>rw</code> mode. It will contain configuration that SR Linux runs of as well as the files that SR Linux keeps in its <code>/etc/opt/srlinux/</code> directory:</p> <pre><code>\u276f ls srl1/config\nbanner  cli  config.json  devices  tls  ztp\n</code></pre> <p>The topology file that defines the emulated hardware type is driven by the value of the kinds <code>type</code> parameter. Depending on a specified <code>type</code>, the appropriate content will be populated into the <code>topology.yml</code> file that will get mounted to <code>/tmp/topology.yml</code> directory inside the container in <code>ro</code> mode.</p>","boost":4},{"location":"manual/kinds/srl/#authorized-keys","title":"authorized keys","text":"<p>Additionally, containerlab will mount the <code>authorized_keys</code> file that will have contents of every public key found in <code>~/.ssh</code> directory as well as the contents of a <code>~/.ssh/authorized_keys</code> file if it exists<sup>2</sup>. This file will be mounted to <code>~/.ssh/authorized_keys</code> path for the following users:</p> <ul> <li><code>root</code></li> <li><code>linuxadmin</code></li> <li><code>admin</code></li> </ul> <p>This will enable passwordless access for the users above if any public key is found in the user's directory.</p>","boost":4},{"location":"manual/kinds/srl/#host-requirements","title":"Host Requirements","text":"<p>SR Linux is a containerized NOS, therefore it depends on the host's kernel and CPU. It is recommended to run a kernel v4 and newer, though it might also run on the older kernels.</p>","boost":4},{"location":"manual/kinds/srl/#ssse3-cpu-set","title":"SSSE3 CPU set","text":"<p>SR Linux XDP - the emulated datapath based on DPDK - requires SSSE3 instructions to be available. This instruction set is present on most modern CPUs, but it is missing in the basic emulated CPUs created by hypervisors like QEMU, Proxmox. When this instruction set is not present in the host CPU set, containerlab will abort the lab deployment if it has SR Linux nodes defined.</p> <p>The easiest way to enable SSSE3 instruction set is to configure the hypervisor to use the <code>host</code> CPU type, which exposes all available instructions to the guest. For Proxmox, this can be set in the GUI:</p> <p></p> <p>Or it's also possible via the proxmox configuration file <code>/etc/pve/qemu-server/vmid.conf</code>.</p> <ol> <li> <p>The <code>authorized_keys</code> file will be created with the content of all found public keys. This file will be bind-mounted using the respecting paths inside SR Linux to enable password-less access. Experimental feature.\u00a0\u21a9</p> </li> <li> <p>If running with <code>sudo</code>, add <code>-E</code> flag to sudo to preserve user' home directory for this feature to work as expected.\u00a0\u21a9</p> </li> <li> <p>CLI configs can be saved also in the \"flat\" format using <code>info flat</code> command.\u00a0\u21a9</p> </li> <li> <p>Prior to SR Linux 22.11.1, the default credentials were <code>admin:admin</code>.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/vr-csr/","title":"Cisco CSR1000v","text":"<p>Cisco CSR1000v virtualized router is identified with <code>vr-csr</code> or <code>vr-cisco_csr1000v</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-csr nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-csr/#managing-vr-csr-nodes","title":"Managing vr-csr nodes","text":"<p>Note</p> <p>Containers with CSR1000v inside will take ~6min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Cisco CSR1000v node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONF <p>to connect to a <code>bash</code> shell of a running vr-csr container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the CSR1000v CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-csr/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-csr container can have up to 144 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of CSR1000v line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-csr node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-csr/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-csr/#node-configuration","title":"Node configuration","text":"<p>vr-csr nodes come up with a basic configuration where only <code>admin</code> user and management interfaces such as NETCONF provisioned.</p>","boost":4},{"location":"manual/kinds/vr-csr/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make CSR1000V nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-csr\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-ftosv/","title":"Dell FTOSv (OS10) / ftosv","text":"<p>Dell FTOSv (OS10) virtualized router/switch is identified with <code>vr-ftosv</code> or <code>vr-dell_ftosv</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-ftosv nodes launched with containerlab comes up pre-provisioned with SSH and SNMP services enabled.</p>","boost":4},{"location":"manual/kinds/vr-ftosv/#managing-vr-ftosv-nodes","title":"Managing vr-ftosv nodes","text":"<p>Note</p> <p>Containers with FTOS10v inside will take ~2-4min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Dell FTOS10v node launched with containerlab can be managed via the following interfaces:</p> bashCLI <p>to connect to a <code>bash</code> shell of a running vr-ftosv container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the Dell FTOSv CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-ftosv/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-ftosv container can have different number of available interfaces which depends on platform used under FTOS10 virtualization .qcow2 disk and container image built using vrnetlab project. Interfaces uses the following mapping rules (in topology file):</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of FTOS10v line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-ftosv node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-ftosv/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-ftosv/#node-configuration","title":"Node configuration","text":"<p>vr-ftosv nodes come up with a basic configuration where only <code>admin</code> user and management interfaces such as SSH provisioned.</p>","boost":4},{"location":"manual/kinds/vr-ftosv/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vMX nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-ftosv\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-n9kv/","title":"Cisco Nexus 9000v","text":"<p>Cisco Nexus9000v virtualized router is identified with <code>vr-n9kv</code> or <code>vr-cisco_n9kv</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-n9kv nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF, NXAPI and gRPC services enabled.</p>","boost":4},{"location":"manual/kinds/vr-n9kv/#managing-vr-n9kv-nodes","title":"Managing vr-n9kv nodes","text":"<p>Note</p> <p>Containers with Nexus 9000v inside will take ~8-10min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Cisco Nexus 9000v node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONFgRPC <p>to connect to a <code>bash</code> shell of a running vr-n9kv container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the Nexus 9000v CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>gRPC server is running over port 50051</p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-n9kv/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-n9kv container can have up to 128 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of Nexus 9000v line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-n9kv node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-n9kv/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-n9kv/#node-configuration","title":"Node configuration","text":"<p>vr-n9kv nodes come up with a basic configuration where only <code>admin</code> user and management interfaces such as NETCONF, NXAPI and GRPC provisioned.</p>","boost":4},{"location":"manual/kinds/vr-n9kv/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make n9kv nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-n9kv\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-nxos/","title":"Cisco NXOS","text":"<p>Cisco NXOS virtual appliance is identified with <code>vr-nxos</code> or <code>vr-cisco_nxos</code> kind in the topology file. It is built using hellt/vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Note</p> <p>This is a Titanium based system, which is an older version of NX-OS.</p> <p>vr-nxos nodes launched with containerlab come up pre-provisioned with SSH service enabled.</p>","boost":4},{"location":"manual/kinds/vr-nxos/#managing-vr-nxos-nodes","title":"Managing vr-nxos nodes","text":"<p>Cisco NXOS node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSH <p>to connect to a <code>bash</code> shell of a running vr-nxos container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the NX-OS CLI <pre><code>ssh clab@&lt;container-name/id&gt;\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-nxos/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-nxos container can have up to 90 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of NX-OS line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-nxos node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-nxos/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-nxos/#node-configuration","title":"Node configuration","text":"<p>vr-nxos nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>clab</code> user.</p>","boost":4},{"location":"manual/kinds/vr-nxos/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make NXOS nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-nxos\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-pan/","title":"Palo Alto PA-VM","text":"<p>Palo Alto PA-VM virtualized firewall is identified with <code>vr-pan</code> or <code>vr-paloalto_panos</code> kind in the topology file. It is built using boxen project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-pan nodes launched with containerlab come up pre-provisioned with SSH, and HTTPS services enabled.</p>","boost":4},{"location":"manual/kinds/vr-pan/#managing-vr-pan-nodes","title":"Managing vr-pan nodes","text":"<p>Note</p> <p>Containers with Palo Alto PA-VM inside will take ~8min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Palo Alto PA-VM node launched with containerlab can be managed via the following interfaces:</p> bashCLIHTTPS <p>to connect to a <code>bash</code> shell of a running vr-pan container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the Palo Alto PA-VM CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>HTTPS server is running over port 443 -- connect with any browser normally.</p> <p>Info</p> <p>Default user credentials: <code>admin:Admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-pan/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-pan container supports up to 24 interfaces (plus mgmt) and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of PAN VM</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-pan node, it will assign IPv4/6 address to the <code>mgmt</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> need to be configured with IP addressing manually using CLI/management protocols.</p> <p>Info</p> <p>Interfaces will not show up in the cli (<code>show interfaces all</code>) until some configuration is made to the interface!</p>","boost":4},{"location":"manual/kinds/vr-pan/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-pan/#node-configuration","title":"Node configuration","text":"<p>vr-pan nodes come up with a basic configuration where only <code>admin</code> user and management interface is provisioned.</p>","boost":4},{"location":"manual/kinds/vr-pan/#user-defined-config","title":"User defined config","text":"<p>It is possible to make <code>vr-pan</code> nodes to boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup config:</p> <pre><code>name: lab\ntopology:\nnodes:\nceos:\nkind: vr-paloalto_panos\nstartup-config: myconfig.conf\n</code></pre>","boost":4},{"location":"manual/kinds/vr-ros/","title":"MikroTik RouterOS Cloud-hosted router","text":"<p>MikroTik RouterOS cloud hosted router is identified with <code>vr-ros</code> or <code>vr-mikrotik_ros</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p>","boost":4},{"location":"manual/kinds/vr-ros/#managing-vr-ros-nodes","title":"Managing vr-ros nodes","text":"<p>MikroTik RouterOS node launched with containerlab can be managed via the following interfaces:</p> bashCLITelnet <p>to connect to a <code>bash</code> shell of a running vr-ros container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vr-ros CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>serial port (console) is exposed over TCP port 5000: <pre><code># from container host\ntelnet &lt;node-name&gt; 5000\n</code></pre> You can also connect to the container and use <code>telnet localhost 5000</code> if telnet is not available on your container host.</p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-ros/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-ros container can have up to 30 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to the <code>ether2</code> interface of the RouterOS</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-ros node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-ros/#node-configuration","title":"Node configuration","text":"<p>vr-ros nodes come up with a basic \"blank\" configuration where only the management interface and user is provisioned.</p>","boost":4},{"location":"manual/kinds/vr-ros/#user-defined-config","title":"User defined config","text":"<p>It is possible to make ROS nodes to boot up with a user-defined startup config instead of a built-in one. With a <code>startup-config</code> property of the node/kind a user sets the path to the config file that will be mounted to a container and used as a startup config:</p> <pre><code>name: ros_lab\ntopology:\nnodes:\nros:\nkind: vr-ros\nstartup-config: myconfig.txt\n</code></pre> <p>With such topology file containerlab is instructed to take a file <code>myconfig.txt</code> from the current working directory, copy it to the lab directory for that specific node under the <code>/ftpboot/config.auto.rsc</code> name and mount that dir to the container. This will result in this config to act as a startup config for the node via FTP. Mikrotik will automatically import any file with the .auto.rsc suffix.</p>","boost":4},{"location":"manual/kinds/vr-ros/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a node directory for storing configuration artifacts. For <code>vr-ros</code> kind containerlab creates <code>ftpboot</code> directory where the config file will be copied as config.auto.rsc.</p>","boost":4},{"location":"manual/kinds/vr-sros/","title":"Nokia SR OS","text":"<p>Nokia SR OS virtualized router is identified with <code>vr-sros</code> or <code>vr-nokia_sros</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-sros nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-sros/#managing-vr-sros-nodes","title":"Managing vr-sros nodes","text":"<p>Note</p> <p>Containers with SR OS inside will take ~3min to fully boot. You can monitor the progress with <code>watch docker ps</code> waiting till the status will change to <code>healthy</code>.</p> <p>Nokia SR OS node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONFgNMITelnet <p>to connect to a <code>bash</code> shell of a running vr-sros container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the SR OS CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh root@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --insecure \\\n-u admin -p admin \\\ncapabilities\n</code></pre></p> <p>serial port (console) is exposed over TCP port 5000: <pre><code># from container host\ntelnet &lt;node-name&gt; 5000\n</code></pre> You can also connect to the container and use <code>telnet localhost 5000</code> if telnet is not available on your container host.</p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-sros/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-sros container uses the following mapping for its interfaces:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to the first data port of SR OS line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>Interfaces can be defined in a non-sequential way, for example:</p> <pre><code>  links:\n# sr1 port 3 is connected to sr2 port 5\n- endpoints: [\"sr1:eth3\", \"sr2:eth5\"]\n</code></pre> <p>When containerlab launches vr-sros node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> need to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-sros/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-sros/#variants","title":"Variants","text":"<p>Virtual SR OS simulator can be run in multiple HW variants as explained in the vSIM installation guide.</p> <p><code>vr-sros</code> container images come with pre-packaged SR OS variants as defined in the upstream repo as well as support custom variant definition. The pre-packaged variants are identified by the variant name and come up with cards and mda already configured. On the other hand, custom variants give users total flexibility in emulated hardware configuration, but cards and MDAs must be configured manually.</p> <p>To make vr-sros to boot in one of the packaged variants, set the type to one of the predefined variant values:</p> <pre><code>topology:\nnodes:\nsros:\nkind: vr-sros\nimage: vrnetlab/vr-sros:20.10.R1\ntype: sr-1s # if omitted, the default sr-1 variant will be used\nlicense: license-sros20.txt\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#custom-variants","title":"Custom variants","text":"<p>A custom variant can be defined by specifying the TIMOS line for the control plane and line card components:</p> <pre><code>type: &gt;- # (1)!\ncp: cpu=2 ram=4 chassis=ixr-e slot=A card=cpm-ixr-e ___\nlc: cpu=2 ram=4 max_nics=34 chassis=ixr-e slot=1 card=imm24-sfp++8-sfp28+2-qsfp28 mda/1=m24-sfp++8-sfp28+2-qsfp28\n</code></pre> <ol> <li> <p>for distributed chassis CPM and IOM are indicated with markers <code>cp:</code> and <code>lc:</code>.</p> <p>notice the delimiter string <code>___</code> that must be present between CPM and IOM portions of a custom variant string.</p> <p><code>max_nics</code> value must be set in the <code>lc</code> part and specifies a maximum number of network interfaces this card will be equipped with.</p> <p>Memory <code>mem</code> is provided in GB.</p> </li> </ol> <p>It is possible to define a custom variant with multiple line cards; just repeat the <code>lc</code> portion of a type. Note that each line card is a separate VM, increasing pressure on the host running such a node. You may see some issues starting multi line card nodes due to the VMs being moved between CPU cores unless a cpu-set is used.</p> distributed chassis with multiple line cards<pre><code>type: &gt;-\ncp: cpu=2 min_ram=4 chassis=sr-7 slot=A card=cpm5 ___\nlc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=1 card=iom4-e mda/1=me6-10gb-sfp+ ___\nlc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=2 card=iom4-e mda/1=me6-10gb-sfp+\n</code></pre> How to define links in a multi line card setup? <p>When a node uses multiple line cards users should pay special attention to the way links are defined in the topology file. As explained in the interface mapping section, SR OS nodes use <code>ethX</code> notation for their interfaces, where <code>X</code> denotes a port number on a line card/MDA.</p> <p>Things get a little more tricky when multiple line cards are provided. First, every line card must be defined with a <code>max_nics</code> property that serves a simple purpose - identify how many ports at maximum this line card can bear. In the example above both line cards are equipped with the same IOM/MDA and can bear 6 ports at max. Thus, <code>max_nics</code> is set to 6.</p> <p>Another significant value of a line card definition is the <code>slot</code> position. Line cards are inserted into slots, and slot 1 comes before slot 2, and so on.</p> <p>Knowing the slot number and the maximum number of ports a line card has, users can identify which indexes they need to use in the <code>link</code> portion of a topology to address the right port of a chassis. Let's use the following example topology to explain how this all maps together:</p> <pre><code>topology:\nnodes:\nR1:\nkind: vr-sros\nimage: vr-sros:22.7.R2\ntype: &gt;-\ncp: cpu=2 min_ram=4 chassis=sr-7 slot=A card=cpm5 ___\nlc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=1 card=iom4-e mda/1=me6-10gb-sfp+ ___\nlc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=2 card=iom4-e mda/1=me6-10gb-sfp+\nR2:\nkind: vr-sros\nimage: sros:22.7.R2\ntype: &gt;-\ncp: cpu=2 min_ram=4 chassis=sr-7 slot=A card=cpm5 ___\nlc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=1 card=iom4-e mda/1=me6-10gb-sfp+ ___\nlc: cpu=4 min_ram=4 max_nics=6 chassis=sr-7 slot=2 card=iom4-e mda/1=me6-10gb-sfp+\nlinks:\n- endpoints: [\"R1:eth1\", \"R2:eth3\"]\n- endpoints: [\"R1:eth7\", \"R2:eth8\"]\n</code></pre> <p>Starting with the first pair of endpoints <code>R1:eth1 &lt;--&gt; eth3:R2</code>; we see that port1 of R1 is connected with port3 of R2. Looking at the slot information and <code>max_nics</code> value of 6 we see that the linecard in slot 1 can host maximum 6 ports. This means that ports from 1 till 6 belong to the line card equipped in slot=1. Consequently, links ranging from <code>eth1</code> to <code>eth6</code> will address the ports of that line card.</p> <p>The second pair of endpoints <code>R1:eth7 &lt;--&gt; eth8:R2</code> addresses the ports on a line card equipped in the slot 2. This is driven by the fact that the first six interfaces belong to line card in slot 1 as we just found out. This means that our second line card that sits in slot 2 and has as well six ports, will be addressed by the interfaces <code>eth7</code> till <code>eth12</code>, where <code>eth7</code> is port1 and <code>eth12</code> is port6.</p> <p>An integrated variant is provided with a simple TIMOS line:</p> <pre><code>type: \"cpu=2 ram=4 slot=A chassis=ixr-r6 card=cpiom-ixr-r6 mda/1=m6-10g-sfp++4-25g-sfp28\" # (1)!\n</code></pre> <ol> <li>No <code>cp</code> nor <code>lc</code> markers are needed to define an integrated variant.</li> </ol>","boost":4},{"location":"manual/kinds/vr-sros/#node-configuration","title":"Node configuration","text":"<p>vr-sros nodes come up with a basic \"blank\" configuration where only the card/mda are provisioned, as well as the management interfaces such as Netconf, SNMP, gNMI.</p>","boost":4},{"location":"manual/kinds/vr-sros/#user-defined-config","title":"User-defined config","text":"<p>It is possible to make SR OS nodes to boot up with a user-defined startup config instead of a built-in one. With a <code>startup-config</code> property of the node/kind a user sets the path to the config file that will be mounted to a container and used as a startup config:</p> <pre><code>name: sros_lab\ntopology:\nnodes:\nsros:\nkind: vr-sros\nstartup-config: myconfig.txt\n</code></pre> <p>With such topology file containerlab is instructed to take a file <code>myconfig.txt</code> from the current working directory, copy it to the lab directory for that specific node under the <code>/tftpboot/config.txt</code> name and mount that dir to the container. This will result in this config to act as a startup config for the node.</p>","boost":4},{"location":"manual/kinds/vr-sros/#configuration-save","title":"Configuration save","text":"<p>Containerlab's <code>save</code> command will perform a configuration save for <code>vr-sros</code> nodes via Netconf. The configuration will be saved under <code>config.txt</code> file and can be found at the node's directory inside the lab parent directory:</p> <pre><code># assuming the lab name is \"cert01\"\n# and node name is \"sr\"\ncat clab-cert01/sr/tftpboot/config.txt\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#boot-options-file","title":"Boot Options File","text":"<p>By default <code>vr_nokia_sros</code> nodes boot up with a pre-defined \"Boot Options File\" (BOF). This file includes boot settings including:</p> <ul> <li>license file location</li> <li>config file location</li> </ul> <p>When the node is up and running you can make changes to this BOF. One popular example of such changes is the addition of static-routes to reach external networks from within the SR OS node. Although you can save the BOF from within the SROS system, the location the file is written to is not persistent across container restarts. It is also not possible to define a BOF target location. A workaround for this limitation is to automatically execute a CLI script that configures BOF once the system boots.</p> <p>SR OS has an option (introduced in SR OS 16.0.R1) to automatically execute a script upon successful boot. This option is accessible in SR OS by the <code>/configure system boot-good-exec</code> MD-CLI path:</p> <pre><code>[pr:/configure]\nA:admin@sros1# system boot-good-exec ?\n\nboot-good-exec &lt;string&gt;\n &lt;string&gt;  - &lt;1..180 characters&gt;\n\nCLI script file to execute following successful boot-up\n</code></pre> <p>By mounting a script to SR OS container node and using the <code>boot-good-exec</code> option, users can make changes to the BOF the second the node boots and thus complete the task of having a somewhat persistent BOF.</p> <p>As an example the following SR OS MD-CLI script was created to persist custom static routes to the BOF:</p> <pre><code>########################################\n# Configuring static management routes\n########################################\n/bof private\nrouter \"management\" static-routes route 10.0.0.0/24 next-hop 172.31.255.29\nrouter \"management\" static-routes route 10.0.1.0/24 next-hop 172.31.255.29\nrouter \"management\" static-routes route 192.168.0.0/24 next-hop 172.31.255.29\nrouter \"management\" static-routes route 172.20.20.0/24 next-hop 172.31.255.29\ncommit\nexit all\n</code></pre> <p>This script is then placed somewhere on the disk, for example in the containerlab's topology root directory, and mounted to <code>vr-nokia_sros</code> node tftpboot directory using binds property:</p> <pre><code>  nodes:\nsros1:\nmgmt_ipv4: [mgmt-ip]\nkind: vr-sros\nimage: [container-image-repo]\ntype: sr-1s\nlicense: license-sros.txt\nbinds:\n- post-boot-exec.cfg:/tftpboot/post-boot-exec.cfg #(1)!\n</code></pre> <ol> <li><code>post-boot-exec.cfg</code> file contains the script referenced above and it is mounted to <code>/tftpboot</code> directory that is available in SR OS node.</li> </ol> <p>Once the script is mounted to the node, users need to instruct SR OS to execute the script upon successful boot. This is done by adding the following configuration line on SR OS MD-CLI:</p> <pre><code>[pr:/configure system]\nA:admin@sros1# info | match boot-goo\n    boot-good-exec \"tftp://172.31.255.29/post-boot-exec.cfg\" #(1)!\n</code></pre> <ol> <li>The tftpboot location is always at <code>tftp://172.31.255.29/</code> address and the name of the file needs to match the file you used in the binds instruction.</li> </ol> <p>By combining file bindings and the automatic script execution of SROS it is possible to create a workaround for persistent BOF settings.</p>","boost":4},{"location":"manual/kinds/vr-sros/#license","title":"License","text":"<p>Path to a valid license must be provided for all vr-sros nodes with a <code>license</code> directive.</p> <p>If your SR OS license file is issued for a specific UUID, you can define it with custom type definition:</p> <pre><code># note, typically only the cp needs the UUID defined.\ntype: \"cp: uuid=00001234-5678-9abc-def1-000012345678 cpu=4 ram=6 slot=A chassis=SR-12 card=cpm5 ___ lc: cpu=4 ram=6 max_nics=36 slot=1 chassis=SR-12 card=iom3-xp-c mda/1=m10-1gb+1-10gb\"\n</code></pre>","boost":4},{"location":"manual/kinds/vr-sros/#file-mounts","title":"File mounts","text":"<p>When a user starts a lab, containerlab creates a node directory for storing configuration artifacts. For <code>vr-sros</code> kind containerlab creates <code>tftpboot</code> directory where the license file will be copied.</p>","boost":4},{"location":"manual/kinds/vr-sros/#lab-examples","title":"Lab examples","text":"<p>The following labs feature vr-sros node:</p> <ul> <li>SR Linux and vr-sros</li> </ul>","boost":4},{"location":"manual/kinds/vr-veos/","title":"Arista vEOS","text":"<p>Arista vEOS virtualized router is identified with <code>vr-veos</code> or <code>vr-arista_veos</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-veos nodes launched with containerlab comes up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-veos/#managing-vr-veos-nodes","title":"Managing vr-veos nodes","text":"<p>Note</p> <p>Containers with vEOS inside will take ~4min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Arista vEOS node launched with containerlab can be managed via the following interfaces:</p> bashCLINETCONFgNMI <p>to connect to a <code>bash</code> shell of a running vr-veos container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vEOS CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt;:6030 --insecure \\\n-u admin -p admin \\\ncapabilities\n</code></pre> Note, gNMI service runs over 6030 port.</p> <p>Info</p> <p>Default user credentials: <code>admin:admin</code></p>","boost":4},{"location":"manual/kinds/vr-veos/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-veos container can have up to 144 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of vEOS line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-veos node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-veos/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-veos/#node-configuration","title":"Node configuration","text":"<p>vr-veos nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>admin</code> user and management interfaces such as NETCONF, SNMP, gNMI.</p>","boost":4},{"location":"manual/kinds/vr-veos/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vEOS nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-veos\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vmx/","title":"Juniper vMX","text":"<p>Juniper vMX virtualized router is identified with <code>vr-vmx</code> or <code>vr-juniper_vmx</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-vmx nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI services enabled.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#managing-vr-vmx-nodes","title":"Managing vr-vmx nodes","text":"<p>Note</p> <p>Containers with vMX inside will take ~7min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Juniper vMX node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONFgNMI <p>to connect to a <code>bash</code> shell of a running vr-vmx container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vMX CLI <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh admin@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --insecure \\\n-u admin -p admin@123 \\\ncapabilities\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-vmx/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-vmx container can have up to 90 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to a first data port of vMX line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-vmx node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach the management plane of the router.</p> <p>Data interfaces <code>eth1+</code> need to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-vmx/#node-configuration","title":"Node configuration","text":"<p>vr-vmx nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>admin</code> users and management interfaces such as NETCONF, SNMP, gNMI.</p> <p>Starting with hellt/vrnetlab v0.8.2 VMX will make use of the management VRF<sup>1</sup>.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vMX nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-vmx\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vmx/#lab-examples","title":"Lab examples","text":"<p>The following labs feature vr-vmx node:</p> <ul> <li>SR Linux and Juniper vMX</li> </ul>","boost":4},{"location":"manual/kinds/vr-vmx/#known-issues-and-limitations","title":"Known issues and limitations","text":"<ul> <li>when listing docker containers, vr-vmx containers will always report unhealthy status. Do not rely on this status.</li> <li>vMX requires Linux kernel 4.17+</li> <li>To check the boot log, use <code>docker logs -f &lt;node-name&gt;</code>.</li> </ul> <ol> <li> <p>https://github.com/hellt/vrnetlab/pull/98 \u21a9</p> </li> </ol>","boost":4},{"location":"manual/kinds/vr-vqfx/","title":"Juniper vQFX","text":"<p>Juniper vQFX virtualized router is identified with <code>vr-vqfx</code> or <code>vr-juniper_vqfx</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>Note</p> <p>vQFX images built with hellt/vrnetlab have experimental support for vQFX version v18 and newer.</p>","boost":4},{"location":"manual/kinds/vr-vqfx/#managing-vr-vqfx-nodes","title":"Managing vr-vqfx nodes","text":"<p>Note</p> <p>Containers with vQFX inside will take ~7min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Juniper vQFX node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONF <p>to connect to a <code>bash</code> shell of a running vr-vqfx container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the vQFX CLI (password <code>admin@123</code>) <pre><code>ssh admin@&lt;container-name/id&gt;\n</code></pre></p> <p>Coming soon</p> <p>Info</p> <p>Default user credentials: <code>admin:admin@123</code></p>","boost":4},{"location":"manual/kinds/vr-vqfx/#interfaces-mapping","title":"Interfaces mapping","text":"<ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of vQFX line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-vqfx node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-vqfx/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-vqfx/#node-configuration","title":"Node configuration","text":"<p>vr-vqfx nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>admin</code> user with the provided password.</p>","boost":4},{"location":"manual/kinds/vr-vqfx/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make vQFX nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-vqfx\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-vqfx/#lab-examples","title":"Lab examples","text":"<p>Coming soon.</p>","boost":4},{"location":"manual/kinds/vr-xrv/","title":"Cisco XRv","text":"<p>Cisco XRv virtualized router is identified with <code>vr-xrv</code> or <code>vr-cisco_xrv</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-xrv nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled.</p> <p>Warning</p> <p>XRv image is discontinued by Cisco and supreceded by XRv 9000 image. It was added to containerlab because the image is lightweight, compared to XRv9k. If recent features are needed, use vr-xrv9k kind.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#managing-vr-xrv-nodes","title":"Managing vr-xrv nodes","text":"<p>Note</p> <p>Containers with XRv inside will take ~5min to fully boot. You can monitor the progress with <code>docker logs -f &lt;container-name&gt;</code>.</p> <p>Cisco XRv node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONFgNMI <p>to connect to a <code>bash</code> shell of a running vr-xrv container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the XRv CLI <pre><code>ssh clab@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh clab@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --insecure \\\n-u clab -p clab@123 \\\ncapabilities\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>clab:clab@123</code></p>","boost":4},{"location":"manual/kinds/vr-xrv/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-xrv container can have up to 90 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of XRv line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-xrv node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-xrv/#node-configuration","title":"Node configuration","text":"<p>vr-xrv nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>clab</code> user and management interfaces such as NETCONF, SNMP, gNMI.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make XRv nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-xrv\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-xrv/#lab-examples","title":"Lab examples","text":"<p>The following labs feature vr-xrv node:</p> <ul> <li>SR Linux and Cisco XRv</li> </ul>","boost":4},{"location":"manual/kinds/vr-xrv/#known-issues-and-limitations","title":"Known issues and limitations","text":"<ul> <li>LACP and BPDU packets are not propagated to/from vrnetlab based routers launched with containerlab.</li> </ul>","boost":4},{"location":"manual/kinds/vr-xrv9k/","title":"Cisco XRv9k","text":"<p>Cisco XRv9k virtualized router is identified with <code>vr-xrv9k</code> or <code>vr-cisco_xrv9k</code> kind in the topology file. It is built using vrnetlab project and essentially is a Qemu VM packaged in a docker container format.</p> <p>vr-xrv9k nodes launched with containerlab come up pre-provisioned with SSH, SNMP, NETCONF and gNMI (if available) services enabled.</p> <p>Warning</p> <p>XRv9k node is a resource hungry image. As of XRv9k 7.2.1 version the minimum resources should be set to 2vcpu/14GB. These are the default setting set with containerlab for this kind. Image will take 25 minutes to fully boot, be patient. You can monitor the loading status with <code>docker logs -f &lt;container-name&gt;</code>.</p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#managing-vr-xrv9k-nodes","title":"Managing vr-xrv9k nodes","text":"<p>Cisco XRv9k node launched with containerlab can be managed via the following interfaces:</p> bashCLI via SSHNETCONFgNMI <p>to connect to a <code>bash</code> shell of a running vr-xrv9k container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p>to connect to the XRv9kCLI <pre><code>ssh clab@&lt;container-name/id&gt;\n</code></pre></p> <p>NETCONF server is running over port 830 <pre><code>ssh clab@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>using the best in class gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt; --insecure \\\n-u clab -p clab@123 \\\ncapabilities\n</code></pre></p> <p>Info</p> <p>Default user credentials: <code>clab:clab@123</code></p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#interfaces-mapping","title":"Interfaces mapping","text":"<p>vr-xrv9k container can have up to 90 interfaces and uses the following mapping rules:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>eth1</code> - first data interface, mapped to first data port of XRv9k line card</li> <li><code>eth2+</code> - second and subsequent data interface</li> </ul> <p>When containerlab launches vr-xrv9k node, it will assign IPv4/6 address to the <code>eth0</code> interface. These addresses can be used to reach management plane of the router.</p> <p>Data interfaces <code>eth1+</code> needs to be configured with IP addressing manually using CLI/management protocols.</p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/vr-xrv9k/#node-configuration","title":"Node configuration","text":"<p>vr-xrv9k nodes come up with a basic configuration where only the control plane and line cards are provisioned, as well as the <code>clab</code> user and management interfaces such as NETCONF, SNMP, gNMI.</p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#startup-configuration","title":"Startup configuration","text":"<p>It is possible to make XRv9k nodes boot up with a user-defined startup-config instead of a built-in one. With a <code>startup-config</code> property of the node/kind user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>topology:\nnodes:\nnode:\nkind: vr-xrv9k\nstartup-config: myconfig.txt\n</code></pre> <p>With this knob containerlab is instructed to take a file <code>myconfig.txt</code> from the directory that hosts the topology file, and copy it to the lab directory for that specific node under the <code>/config/startup-config.cfg</code> name. Then the directory that hosts the startup-config dir is mounted to the container. This will result in this config being applied at startup by the node.</p> <p>Configuration is applied after the node is started, thus it can contain partial configuration snippets that you desire to add on top of the default config that a node boots up with.</p>","boost":4},{"location":"manual/kinds/vr-xrv9k/#lab-examples","title":"Lab examples","text":"<p>The following labs feature vr-xrv9k node:</p> <ul> <li>SR Linux and Cisco XRv9k</li> </ul>","boost":4},{"location":"manual/kinds/xrd/","title":"Cisco XRd","text":"<p>Cisco XRd Network OS is identified with <code>xrd</code> or <code>cisco_xrd</code> kind in the topology file. A kind defines a supported feature set and a startup procedure of a node.</p> <p>XRd comes in two variants:</p> <ul> <li>control-plane</li> <li>vrouter</li> </ul> <p>Containerlab supports only the control-plane flavor of XRd, as it allows to build topologies using virtual interfaces, whereas vrouter requires PCI interfaces to be attached to it.</p> <p>Tip</p> <p>Consult with XRd Tutorials series to get an in-depth understanding of XRd requirements and capabilities.</p>","boost":4},{"location":"manual/kinds/xrd/#getting-xrd","title":"Getting XRd","text":"<p>XRd image is available for download only for users who have an active service account<sup>1</sup>.</p>","boost":4},{"location":"manual/kinds/xrd/#managing-xrd-nodes","title":"Managing XRd nodes","text":"<p>There are several management interfaces supported by XRd nodes:</p> CLIbashSSHgNMINetconf <p>to connect to a XR CLI shell of a running XRd container: <pre><code>docker exec -it &lt;container-name/id&gt; /pkg/bin/xr_cli.sh\n</code></pre></p> <p>to connect to a <code>bash</code> shell of a running XRd container: <pre><code>docker exec -it &lt;container-name/id&gt; bash\n</code></pre></p> <p><code>ssh clab@&lt;container-name&gt;</code> Password: <code>clab@123</code></p> <p>gNMI server runs on <code>9339</code> port in the insecure mode (no TLS). Using gnmic gNMI client as an example: <pre><code>gnmic -a &lt;container-name/node-mgmt-address&gt;:9339 --insecure \\\n-u clab -p clab@123 \\\ncapabilities\n</code></pre></p> <p>Netconf server runs on <code>830</code> port: <pre><code>ssh clab@&lt;container-name&gt; -p 830 -s netconf\n</code></pre></p> <p>Info</p> <p>Default credentials: <code>clab:clab@123</code></p>","boost":4},{"location":"manual/kinds/xrd/#interfaces-mapping","title":"Interfaces mapping","text":"<p>XRd container uses the following mapping for its Linux interfaces<sup>2</sup>:</p> <ul> <li><code>eth0</code> - management interface connected to the containerlab management network</li> <li><code>Gi0-0-0-0</code> - first data interface mapped to <code>Gi0/0/0/0</code> internal interface.</li> <li><code>Gi0-0-0-N</code> - Nth data interface mapped to <code>Gi0/0/0/N</code> internal interface.</li> </ul> <p>When containerlab launches XRd node, it will set IPv4/6 addresses as assigned by docker to the <code>eth0</code> interface and XRd node will boot with these addresses configured for its <code>MgmtEth0</code>. Data interfaces <code>Gi0/0/0/N</code> need to be configured with IP addressing manually.</p> <pre><code>RP/0/RP0/CPU0:xrd#sh ip int br\nWed Dec 21 12:04:13.049 UTC\n\nInterface                      IP-Address      Status          Protocol Vrf-Name\nMgmtEth0/RP0/CPU0/0            172.20.20.5     Up              Up       default\n</code></pre>","boost":4},{"location":"manual/kinds/xrd/#features-and-options","title":"Features and options","text":"","boost":4},{"location":"manual/kinds/xrd/#node-configuration","title":"Node configuration","text":"<p>XRd nodes have a dedicated <code>config</code> directory that is used to persist the configuration of the node and expose internal directories of the NOS.</p> <p>For XRd nodes, containerlab exposes the following file layout of the node's lab directory:</p> <ul> <li><code>xr-storage</code> (dir): a directory that is mounted to <code>/xr-storage</code> path of the NOS and is used to persist changes made to the node as well as provides access to the logs and various runtime files.</li> <li><code>first-boot.cfg</code> - a configuration file in Cisco IOS-XR CLI format that the node boots with.</li> </ul>","boost":4},{"location":"manual/kinds/xrd/#default-node-configuration","title":"Default node configuration","text":"<p>It is possible to launch nodes of <code>cisco_xrd</code> kind with a basic config or to provide a custom config file that will be used as a startup config instead.</p> <p>When a node is defined without <code>startup-config</code> statement present, containerlab will generate an empty config from this template and copy it to the config directory of the node.</p> <pre><code># example of a topo file that does not define a custom config\n# as a result, the config will be generated from a template\n# and used by this node\nname: xrd\ntopology:\nnodes:\nxrd:\nkind: cisco_xrd\n</code></pre>","boost":4},{"location":"manual/kinds/xrd/#user-defined-config","title":"User defined config","text":"<p>It is possible to make XRd nodes to boot up with a user-defined config instead of a built-in one. With a <code>startup-config</code> property a user sets the path to the config file that will be mounted to a container and used as a startup-config:</p> <pre><code>name: xrd\ntopology:\nnodes:\nxrd:\nkind: cisco_xrd\nstartup-config: xrd.cfg\n</code></pre> <p>When a config file is passed via <code>startup-config</code> parameter it will be used during an initial lab deployment. However, a config file that might be in the lab directory of a node takes precedence over the startup-config<sup>3</sup>.</p> <p>With such topology file containerlab is instructed to take a file <code>xrd.cfg</code> from the current working directory and copy it to the lab directory for that specific node under the <code>/first-boot.cfg</code> name. This will result in this config acting as a startup-config for the node.</p> <p>To provide a user-defined config, take the default configuration template and add the necessary configuration commands without changing the rest of the file. This will result in proper automatic assignment of IP addresses to the management interface, as well as applying user-defined commands.</p> <p>Tip</p> <p>Check SR Linux and XRd lab example where startup configuration files are provided to both nodes to see it in action.</p>","boost":4},{"location":"manual/kinds/xrd/#configuration-persistency","title":"Configuration persistency","text":"<p>XRd nodes persist their configuration in <code>&lt;lab-directory&gt;/&lt;node-name&gt;/xr-storage</code> directory. When a user commits changes to XRd nodes using one of the management interfaces, they are kept in the configuration DB (but not exposed as a configuration file).</p> <p>This capability allows users to configure the XRd node, commit the changes, then destroy the lab (without using <code>--cleanup</code> flag to keep the lab dir intact) and on a subsequent deploy action, the node will boot with the previously saved configuration.</p>","boost":4},{"location":"manual/kinds/xrd/#lab-examples","title":"Lab examples","text":"<p>The following labs feature XRd nodes:</p> <ul> <li>SR Linux and XRd</li> </ul> <ol> <li> <p>https://xrdocs.io/virtual-routing/tutorials/2022-08-22-xrd-images-where-can-one-get-them/ \u21a9</p> </li> <li> <p>It is not yet possible to manually assign interface mapping rules in containerlab for XRd nodes. PRs are welcome.\u00a0\u21a9</p> </li> <li> <p>if startup config needs to be enforced, either deploy a lab with <code>--reconfigure</code> flag, or use <code>enforce-startup-config</code> setting.\u00a0\u21a9</p> </li> </ol>","boost":4},{"location":"rn/0.11.0/","title":"Release 0.11.0","text":"<p> 2021-03-09</p>"},{"location":"rn/0.11.0/#multi-node-labs-with-vxlan-tunneling","title":"Multi-node labs with VxLAN tunneling","text":"<p>Most containerlab users are fine with running labs within a single server/VM. Truthfully speaking, a VM with 32GB RAM can host a very decent lab with dozens of nodes<sup>1</sup>.</p> <p>But yet, it is not always the case, sometimes we have a shortage of resources or the workloads are quite heavyweighters themselves. In all these scenarios it would be really nice to scope containerlab out of a single server and make it reach far horizons.</p> <p>And there are several way of making containerlab nodes to reach nodes/systems outside of its container host:</p> <ul> <li>Exposing services/ports</li> <li>Exposing management network with routing</li> <li>Connecting remote nodes with bridging</li> <li>Creating VxLAN tunnels across the network</li> </ul> <p>To help you navigate these options we've created a multi-node labs documentation article that explains each approach in details.</p> <p>With 0.11.0 specifically, we add the last option in that list, which is most flexible and far-reaching - VxLAN Tunneling.</p> <p>To help containerlab users to provision VxLAN tunnels the helper commands have been created - vxlan create, vxlan delete.</p> <p>The multi-node lab provides a step-by-step coverage of how this all nicely play together and builds a playground to demonstrate the multi-node capabilities.</p>"},{"location":"rn/0.11.0/#openvswitch-ovs-bridges-support","title":"Openvswitch (ovs) bridges support","text":"<p>The versatility of containerlab is in its ability to run anywhere where linux runs and wire up anything that can be packaged in a docker container. But it is quite hard to package in a container a piece of a real hardware. Still, we need it more often than not to connect devices like traffic generators or physical chassis to containerlab labs.</p> <p>It was possible before with linux bridges, now we are adding support for ovs bridges that will allow for even more elaborated and flexible connectivity options.</p>"},{"location":"rn/0.11.0/#enhanced-topology-checks","title":"Enhanced topology checks","text":"<p>Gradually we add checks that will make containerlab fail early if anything in the topology file or the host system is not right.</p> <p>In this release containerlab will additionally perform these checks:</p> <ol> <li>if node name matches already existing container the deployment won't start</li> <li>if vrnetlab nodes are defined, but virtualization is not available the deployment won't start</li> <li>if &lt;2 vCPU is available a warning will be emitted. Some containerized NOSes demand 2vCPU, so running with 1vCPU is discouraged</li> <li>abrupt deployment if host link referenced in topo file already exists in the host</li> </ol>"},{"location":"rn/0.11.0/#shell-completions","title":"Shell completions","text":"<p>Thanks to our contributor @steiler we now have shell completions, so you can navigate containerlab CLI with guidance and confidence.</p>"},{"location":"rn/0.11.0/#manual-installation","title":"Manual installation","text":"<p>All these time we relied on package managers to handle containerlab installation. Having support for deb/rpm packages is enough for most of us, but those nerds on Arch and Gentoo .</p> <p>For minorities we added </p>"},{"location":"rn/0.11.0/#vrnetlab-changes","title":"Vrnetlab changes","text":"<p>Vrnetlab version <code>0.2.0</code> has been issued to freeze the code compatible with containerlab 0.11.0.</p> <p>If stopped working when you upgraded to 0.11.0, then re-build the images with vrnetlab 0.2.0</p>"},{"location":"rn/0.11.0/#boot-delay-for-vrnetlab-routers","title":"Boot delay for vrnetlab routers","text":"<p>To schedule the startup of vrnetlab routers the BOOT_DELAY env variable may be used. It takes a  </p>"},{"location":"rn/0.11.0/#added-arista-veos-support","title":"Added Arista vEOS support","text":"<p>Support for virtualized EOS from Arista has been added. Now it is possible to use containerized cEOS and virtualized vEOS.</p>"},{"location":"rn/0.11.0/#sr-os-route-to-management-network","title":"SR OS route to management network","text":"<p>SR OS routers will now have a static route in their Management routing instance to reach the docker management network. This enables SR OS initiated requests to reach services runnign on the management network.</p>"},{"location":"rn/0.11.0/#miscellaneous","title":"Miscellaneous","text":"<ol> <li>Default MTU on the management network/bridge is changed to 1500 from 1450. If you run containerlab in the environment where management network MTU is less than 1500, consider setting the needed MTU in the topo file.</li> <li>Fixed multiple cEOS links creation.</li> <li>Add bridge and linux kinds docs.</li> <li>To allow for multi-node labs and also to make it possible to connect nodes with the container host a new endpoint connection was created - host links.</li> <li>MTU on data links is set to 65000 from 1500. This will allow for jumbo frames safe passage.</li> <li>Additional explanations provided for vrnetlab integration and inter-dependency between projects.</li> </ol> <ol> <li> <p>especially if memory optimization techniques are enabled\u00a0\u21a9</p> </li> </ol>"},{"location":"rn/0.12.0/","title":"Release 0.12.0","text":"<p> 2021-03-28</p>"},{"location":"rn/0.12.0/#identity-aware-sockets","title":"Identity aware sockets","text":"<p>A major improvement to our \"published ports\" feature has landed in 0.12.0. Now it is possible to create Identity Aware sockets for any port of your lab.</p> <p>Identity Aware sockets is a feature of mysocketio service that allows you to let in only authorized users. Authorization is possible via multiple OAuth providers and can be configured to let in users with specific emails and/or with specific domains.</p> <p>Check out how easy it is to create identity aware tunnels with containerlab:</p> <p>With this enhancement it is now possible to create long-running secure tunnels which only the authorized users will be able to access.</p>"},{"location":"rn/0.12.0/#on-demand-veth-plumbing","title":"On-demand veth plumbing","text":"<p>Sometimes it is needed to add some additional connections after the lab has been deployed. Although the labs are quickly to re-spin, sometimes one find themselves in the middle of the use case configuration and there is a need to add another connection between the nodes.</p> <p>With <code>tools veth</code> command it is now possible to add veth pairs between container&lt;-&gt;container, containers&lt;-&gt;host and containers&lt;-&gt;bridge nodes. Now you can add modify your lab connections without redeploying the entire lab[^2].</p>"},{"location":"rn/0.12.0/#safer-ways-to-write-clab-files","title":"Safer ways to write clab files","text":"<p>Containerlab got its own JSON Schema that governs the structure of the topology definition files. If you name your topo file as <code>*.clab.yml</code> then some editors like VS Code will automatically provide auto-suggestions and linting for your clab files.</p> <p>Yes, from now on we will call our topo files as clab-files.</p>"},{"location":"rn/0.12.0/#create-tls-certificates-effortlessly","title":"Create TLS certificates effortlessly","text":"<p>With the new commands <code>tools cert ca create</code> and <code>tools cert sign</code> it is now possible to create CA and node certificates with just two commands embedded into containerlab. Start from here if you always wanted to be able to reduce the number of openssl commands.</p> <p>We also added a lab that pictures the net positive effect of having those commands when creating TLS certificates for secured gNMI connectivity.</p>"},{"location":"rn/0.12.0/#ansible-inventory","title":"Ansible inventory","text":"<p>It is imperative to create a nice automation flow that goes from infra deployment to the subsequent configuration automation. When containerlab finishes its deployment job we now create an ansible inventory file for a deployed lab.</p> <p>With this inventory file the users can start their configuration management playbooks and configure the lab for a use case in mind.</p>"},{"location":"rn/0.12.0/#smart-mtu-for-management-network","title":"Smart MTU for management network","text":"<p>Default MTU value for the management network will now be inherited from the MTU of the <code>docker0</code> bridge interface. Thus, if you configured your docker daemon for a custom MTU, it will be respected by containerlab.</p>"},{"location":"rn/0.12.0/#running-nodes-in-bridge-network","title":"Running nodes in <code>bridge</code> network","text":"<p>When management network name is set to <code>bridge</code>, containerlab nodes will be run in the default docker network named <code>bridge</code>. This is the network where containers end up connecting when you do <code>docker run</code>, so running the lab in the default docker network makes it easy to connect your lab with the workloads that have been started by someone else.</p>"},{"location":"rn/0.12.0/#releases-notification","title":"Releases notification","text":"<p>When a new release comes out we let you know next time you deploy a lab, a tiny message will pop up in the log saying that a new one is ready to make your labs even more efficient and easy.</p> <pre><code>INFO[0001] \ud83c\udf89 New containerlab version 0.12.0 is available! Release notes: https://containerlab.dev/rn/0.12.0\nRun 'containerlab version upgrade' to upgrade or go check other installation options at https://containerlab.dev/install/ \n</code></pre>"},{"location":"rn/0.12.0/#saving-sr-os-config","title":"Saving SR OS config","text":"<p>SR OS nodes which are launched with <code>vr-sros</code> kind now have support for saving their configuration with <code>containerlab save</code> command.</p> <p>This is implemented via Netconf's <code>&lt;copy-config&gt;</code> RPC that is executed against SR OS node.</p>"},{"location":"rn/0.12.0/#miscellaneous","title":"Miscellaneous","text":"<ol> <li>Added support for user-defined node labels which can convey metadata for a given node.</li> <li>Container node needs to support live interface attachment.</li> <li>New TLS certificate logic for SR Linux nodes. If the CA files and node cert exist, the re-deployment of a lab won't issue new certificates and will reuse the existing ones. </li> <li>Additional node property <code>network-mode</code> has been added which allows to deploy the node in the host networking mode.</li> <li>If the changes containerlab makes to LLDP/TX-offload on the management bridge fail, they won't prevent the lab from proceed deploying.</li> </ol>"},{"location":"rn/0.13.0/","title":"Release 0.13.0","text":"<p> 2021-04-13</p>"},{"location":"rn/0.13.0/#cisco-csr1000v-support","title":"Cisco CSR1000v support","text":"<p>Added support for Cisco CSR1000v system via <code>vr-csr</code> kind.</p>"},{"location":"rn/0.13.0/#routeros-support","title":"RouterOS support","text":"<p>With <code>vr-ros</code> kind added support for Mikrotik RouterOS system.</p>"},{"location":"rn/0.13.0/#arista-ceos-improvements","title":"Arista cEOS improvements","text":"<p>This patch release brings the following improvements to Arista cEOS:</p> <ol> <li>Arista Ma0 MAC address is now having Arista OUI, instead of docker generated local MAC. Additionally, System MAC address is now generated as the next MAC address from Ma0 interface.</li> <li>ETBA environment variable is set to <code>4</code>, from its original value of <code>1</code>.</li> <li>Default cEOS configuration now has the following config line to allow for enhanced BGP daemon to run on startup:     <pre><code>service routing protocols model multi-agent\n</code></pre></li> </ol>"},{"location":"rn/0.13.0/#attachments-to-management-network","title":"Attachments to management network","text":"<p>With a new reserved endpoint definition it is now possible to attach data interface of a node to the management network.</p>"},{"location":"rn/0.13.0/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>a warning message will be displayed if container host has less than 1GB of free memory</li> <li>a lab won't deploy if a user specified <code>eth0</code> interface in the links section as this is not a possible configuration</li> <li>Now it is possible to safely use <code>-</code> (dashes) in lab name.</li> </ul>"},{"location":"rn/0.13.0/#new-contributors","title":"New contributors","text":"<p>Thanks to @kellerza, @burnyd, @dharmbhai, @dpnetca for providing some of these enhancements and joining our contributors ranks!</p>"},{"location":"rn/0.14.0/","title":"Release 0.14.0","text":"<p> 2021-05-19</p>"},{"location":"rn/0.14.0/#container-runtime-support","title":"Container runtime support","text":"<p>Michael Kashin (@networkop) delivered a massive infrastructure improvement by adding the foundation that allows containerlab to run on multiple container runtimes such as <code>containerd</code>, <code>podman</code> and the likes.</p> <p>For the end users of containerlab that will give more flexibility on platforms selection where containerlab can run.</p>"},{"location":"rn/0.14.0/#arista-et-interfaces","title":"Arista <code>et</code> interfaces","text":"<p>Steve Ulrich (@sulrich) added support for synchronization the ENV vars passed to cEOS node and the respective container command. This makes it possible to set the cEOS specific env vars and be sure that they will be mirrored in the CMD instruction of the container.</p> <p>This allowed for users to, for example, overwrite the <code>INTFTYPE</code> env var to allow for using <code>et</code> interfaces with cEOS. This is documented in the ceos kind docs.</p>"},{"location":"rn/0.14.0/#nodedir-path-variable","title":"<code>nodeDir</code> path variable","text":"<p>Markus Vahlenkamp (@steiler) added support for <code>$nodeDir</code> variable that you can now use in the bind paths. This is useful to simplify the configuration artifacts mapping when they are kept in the node specific directories. Read more on this in the nodes/binds documentation section.</p>"},{"location":"rn/0.14.0/#improved-sr-os-vr-sros-boot-procedure","title":"Improved SR OS (<code>vr-sros</code>) boot procedure","text":"<p>With hellt/vrnetlab v0.3.1 we added a hardened process of SR OS boot sequence. Before that fix the vr-sros nodes might get problems in attaching container interfaces in time. Starting with v0.3.1 that issue is no more and vr-sros nodes will wait till the dataplane interfaces will show up in the container namespace.</p>"},{"location":"rn/0.14.0/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>fixed bridge attachment issue</li> <li>fixed docker repo naming resolution which prevented pulling certainly formatted repositories</li> <li>fixed Arista cEOS configuration regeneration and management interface addressing.</li> <li>@networkop added support for predefined mac addresses that containerlab uses for veth interfaces. The MAC OUI is <code>00:c1:ab</code> for all containerlab interfaces.</li> <li>@networkop added support for max-workers argument for <code>delete</code> command.</li> </ul>"},{"location":"rn/0.14.0/#new-contributors","title":"New contributors","text":"<p>Thanks to @sulrich, @blinklet and @networkop for providing some of these enhancements/fixes and joining our contributors ranks!</p>"},{"location":"rn/0.14.1/","title":"Release 0.14.1","text":"<p> 2021-05-20</p>"},{"location":"rn/0.14.1/#fix-lab-deletion-data-race","title":"fix lab deletion data race","text":"<p>This patch release fixes data race which could occur during the deletion of a lab with many nodes.</p>"},{"location":"rn/0.14.2/","title":"Release 0.14.2","text":"<p> 2021-05-27</p>"},{"location":"rn/0.14.2/#fix-additional-interfaces-to-management-network","title":"fix additional interfaces to management network","text":"<p>This patch release fixes issues with containerlab failing to add additional data interfaces to management network as described here.</p>"},{"location":"rn/0.14.3/","title":"Release 0.14.3","text":"<p> 2021-05-28</p>"},{"location":"rn/0.14.3/#fixes-and-improvements","title":"Fixes and improvements","text":"<ul> <li>fixed missing IPv4/6 information in <code>containerlab inspect --all</code> output</li> <li>prevented deployment of the lab if the same-named lab has already been deployed</li> <li>install script now checks if the released version set by the user actually exists</li> </ul>"},{"location":"rn/0.14.4/","title":"Release 0.14.4","text":"<p> 2021-06-06</p>"},{"location":"rn/0.14.4/#fixes-and-improvements","title":"Fixes and improvements","text":"<ul> <li>fixed generate command</li> <li>added \"Y/n\" prompt when doing <code>containerlab version upgrade</code>.</li> <li>version command will return a link to the relevant release notes</li> </ul>"},{"location":"rn/0.15/","title":"Release 0.15","text":"<p> 2021-07-16</p> <p>Warning</p> <p>This release adds a breaking change in the clab file schema. Starting from this release version, a path to a startup config file needs to be provided with <code>startup-config</code> key. Previously it was done with <code>config</code> key.  </p> <p>Use <code>sed -i s/config:/startup-config:/g &lt;topo-file&gt;</code> script to auto-substitute the values in the <code>&lt;topo-file&gt;</code> file.</p>"},{"location":"rn/0.15/#ignite-runtime-support-and-cumulus-vx","title":"Ignite runtime support and Cumulus VX","text":"<p>@networkop embarked on a journey of adding a Cumulus VX to containerlab, and he chose the doing-this-the-hard-way by running Cumulus VX as a container<sup>1</sup>. To make that happen, he added a new <code>ignite</code> runtime that allows us to launch containers with custom kernels.</p> <p>With that work done, containerlab is now supporting a new kind - <code>cvx</code> - that defines the Cumulus VX NOS and that can run in different modes<sup>2</sup>.</p>"},{"location":"rn/0.15/#containerd-runtime-support","title":"Containerd runtime support","text":"<p>@steiler added experimental support for <code>containerd</code> runtime. As the name implies, this makes it possible to run containerlab having only containerd installed and the necessary CNI plugins.</p> <p>Please bear in mind that not all containerlab features are available for containerd runtime, but the core feature set is there.</p> <p>To start containers with containerd runtime, the <code>--runtime | -r</code> global flag has been introduced.</p>"},{"location":"rn/0.15/#the-big-refactoring","title":"The BIG refactoring","text":"<p>With @karimra superpowers, containerlab has undergone a major refactoring of its code base. Now we have a strong and flexible foundation to build new features on top.</p> <p>This was the biggest architectural change since the beginning, so some rough edges might eventually show up, but we will iron them out.</p>"},{"location":"rn/0.15/#community","title":"Community","text":"<p>We strive to support the growing containerlab community to the best of our abilities. While github discussions is a nice place to ask formulated questions sometimes it is really nice to have a chat with the community members.</p> <p>With that in mind we launched the containerlab's own discord server that all are welcome to join!</p>"},{"location":"rn/0.15/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>kind base configuration templates are now embedded in the code. This means that you can download containerlab binary from Releases and launch the labs, without doing the installation from packages.     The reason you might want to do this is if you want to install containerlab on a system which doesn't have rpm/deb based package manager.     Note, that the package-based installation is still the recommended way.</li> <li>the <code>exec</code> command now has a <code>--format</code> flag that is capable of nicely handling the JSON outputs.</li> <li>deployment of SR Linux nodes will now proceed even if license file is not present<sup>3</sup></li> <li>a new <code>--keep-mgmt-net</code> flag has been added to <code>destroy</code> command to prevent the management network deletion attempt</li> <li>the MTU on the veth links created between the containers has been lowered to 9500 from its original 65000 value</li> </ul>"},{"location":"rn/0.15/#patches","title":"Patches","text":""},{"location":"rn/0.15/#0151","title":"0.15.1","text":"<ul> <li>Fixed directories creation for <code>ignite</code> runtimes</li> <li>Fixed release notes links in <code>containerlab version</code> command</li> </ul>"},{"location":"rn/0.15/#0152","title":"0.15.2","text":"<ul> <li>Fixed startup-config provisioning for ceos nodes #526</li> </ul>"},{"location":"rn/0.15/#0153","title":"0.15.3","text":"<ul> <li>Allow ceos startup config to receive ipv4/6 addresses  #528</li> <li>Added JSON-RPC server to SR Linux default configuration</li> </ul>"},{"location":"rn/0.15/#0154","title":"0.15.4","text":"<ul> <li>Fixed disabling tx checksum offload for linux nodes #543</li> <li>Fixed <code>tools disable-tx-offload</code> command #543</li> </ul> <ol> <li> <p>as opposed to a VM-way by running Cumulus with vrnetlab \u21a9</p> </li> <li> <p>both with <code>ignite</code> runtime or <code>docker</code> runtime\u00a0\u21a9</p> </li> <li> <p>starting with SR Linux 21.3, the license file is optional\u00a0\u21a9</p> </li> </ol>"},{"location":"rn/0.16/","title":"Release 0.16","text":"<p> 2021-08-03</p> <p>Warning</p> <p>This release adds a new way of writing containerlab hosts information into the <code>/etc/hosts</code> file.</p> <p>Before upgrading to this release destroy all labs and ensure no stale entries are present in the <code>/etc/hosts</code> file, then do the upgrade.</p>"},{"location":"rn/0.16/#improved-interfaces-detection-for-vr-based-nodes","title":"Improved interfaces detection for vr-based nodes","text":"<p>With the enhancements made by @carlmontanari in vrnetlab and @networkop in containerlab we improved the way vr-based nodes boot.</p> <p>The improvements are related to the way VMs inside the containers detect the presence of the dataplane interfaces. Starting with <code>hellt/vrnetlab v0.5.0</code> and <code>containerlab v0.16.0</code> the nodes will know exactly how many interfaces a user defined in the lab file, and therefore will wait for those interfaces to appear before booting the qemu VM.</p>"},{"location":"rn/0.16/#non-sequential-interfaces-for-vr-based-nodes","title":"Non-sequential interfaces for vr-based nodes","text":"<p>Thanks to @carlmontanari work in vrnetlab#55 it is now possible to define node's interfaces in a non-sequential way. This means, that a user now can define the links as follows:</p> <pre><code>topology:\nnodes:\nsr1:\nkind: vr-sros\nimage: vr-sros:21.5.R2\nsr2:\nkind: vr-sros\nimage: vr-sros:21.5.R2\nlinks:\n# sr1 port 3 is connected to sr2 port 5\n- endpoints: [\"sr1:eth3\", \"sr2:eth5\"]\n</code></pre>"},{"location":"rn/0.16/#new-dns-entries-format","title":"New DNS entries format","text":"<p>Containerlab creates nodes DNS entries when the lab is deployed. In this release we improved the visual layout of those entries so they can be introspected easier.</p>"},{"location":"rn/0.16/#ansible-inventory-improvements","title":"Ansible inventory improvements","text":"<p>A user can now specify an extra ansible group the node should be part of in addition to the default grouping by kind.</p> <p>Another change is that we now create an empty ansible inventory file when the deployment starts and write data to it once it is available. This approach makes it possible to create a clab file that mounts the <code>ansible-inventory.yml</code> file that will be created during the deploy command.</p> <pre><code># mounting the ansible inventory file to a node of a lab\nbinds:\n- clab-demo/ansible-inventory.yml:/tmp/inv\n</code></pre>"},{"location":"rn/0.16/#new-node-kinds","title":"New node kinds","text":"<p>@carlmontanari added support for Cisco Nexus 9000v and Palo Alto PAN nodes.</p>"},{"location":"rn/0.16/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>IPv6 LLA is now enabled on veth interfaces #551</li> <li>SR Linux bind mounts were refined and now only config.json and topology.yml files are mounted #564</li> <li>SR Linux default type has changed from IXR-6 to IXR-D2 to support EVPN services by default</li> </ul>"},{"location":"rn/0.16/#new-contributors","title":"New contributors","text":"<p>Welcome @carlmontanari, @sc68cal and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.16/#patches","title":"Patches","text":""},{"location":"rn/0.16/#0161","title":"0.16.1","text":"<ul> <li>Fixed mysocketio integration #574</li> </ul>"},{"location":"rn/0.16/#0162","title":"0.16.2","text":"<ul> <li>@karimra added another cool addition to containerlab: it is possible to customize the lab prefix or remove it entirely. Read more about it here.</li> <li>We had to bring back directory based mounts for SR Linux nodes for the reasons explained in #579</li> </ul>"},{"location":"rn/0.17/","title":"Release 0.17","text":"<p> 2021-08-24</p>"},{"location":"rn/0.17/#environment-variables-expansion","title":"Environment variables expansion","text":"<p>A new feature contributed by @GrigoriyMikhalkin allows you to perform environment variables expansion throughout the whole clab topology file.</p> <p>This allows you to modify topology definition at runtime using the env vars you have defined in your environment. Awesome addition to make clab play nice in CI/CD systems!</p>"},{"location":"rn/0.17/#local-config-takes-precedence-over-startup-config","title":"Local config takes precedence over startup config","text":"<p>A major change has been made to the config load order. With this release the following order of configuration load is in place:</p> <ol> <li>config file found in lab directory</li> <li>startup config set with <code>startup-config</code> setting</li> <li>generated config from a template that comes within containerlab</li> </ol> <p>The change is with the local config to be preferred over a startup-config. The idea here is simple: when a user deploys a lab from with a <code>startup-config</code> pointing to a certain file the lab boots with this config.</p> <p>Later, users typically build on top of that startup config some additional use cases. And in the case they save their changes to the startup config the changes will be persisted in a lab directory of this node. Now, if a users destroys a lab and deploys it again, if the file in the lab dir is found, it will be used, and startup-config will be ignored.</p> <p>If it is needed to make a config file referenced in <code>startup-config</code> to be used, the deploy command should be augmented with <code>reconfigure</code> flag or a new parameter can be used in the topo file - enforce-startup-config.</p>"},{"location":"rn/0.17/#arista-ceos-persistency-improvements","title":"Arista cEOS persistency improvements","text":"<p>A big improvement in the way management interface is configured for ceos kinds.</p> <p>With a new scrapligo feature we were able to configure ceos nodes over <code>docker exec</code> command after the node finishes booting. That allowed us to always ensure that ceos nodes will have management interface addressed with the IP that docker assigned to it, regardless of which IP was already set in startup-config file.</p> <p>That made it possible to remove a lot of workarounds and restrictions that were in place before.</p>"},{"location":"rn/0.17/#mixed-mode-labs-with-static-and-dynamic-ip-addresses","title":"Mixed mode labs with Static and Dynamic IP addresses","text":"<p>Containerlab allows users to set static IP address for the node management interface. This is done with <code>mgmt_ipv4/6</code> setting of a node.</p> <p>It was possible to set static address for part of the nodes of a lab, in that case some nodes will get a static address, the rest would receive an address as assigned by Docker daemon. The issue that users might see with such labs is that docker assigns dynamically an IP that was set as a static IP for another node.</p> <p>To overcome this race condition the nodes with Static IP address will now be scheduled first, and the dynamic IP allocated nodes will follow.</p>"},{"location":"rn/0.17/#startup-delay-for-nodes","title":"Startup delay for nodes","text":"<p>Nodes can be artificially delayed if they have <code>startup-delay</code> field set to a value greater than zero. This indicates amount in seconds that this node will wait till it will be scheduled for creation by containerlab.</p>"},{"location":"rn/0.17/#new-contributors","title":"New contributors","text":"<p>Welcome @GrigoriyMikhalkin and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.18/","title":"Release 0.18","text":"<p> 2021-09-14</p>"},{"location":"rn/0.18/#exec-parameter","title":"exec parameter","text":"<p>The new <code>exec</code> node parameter allows users to specify a list of commands that will run once the nodes are created.</p> <p>A typical application of this parameter is to call some boot script that configures something on the node, for example its IP address.</p>"},{"location":"rn/0.18/#entrypoint","title":"Entrypoint","text":"<p>With the new <code>entrypoint</code> node parameter it is possible to change the entrypoint of the container.</p>"},{"location":"rn/0.18/#sr-linux-agents","title":"SR Linux agents","text":"<p>A new <code>srl-agents</code> parameter of the node's extra config will allow to copy SR Linux agent definition file to the container's <code>appmgr</code> dir.</p>"},{"location":"rn/0.18/#dell-ftos-support","title":"Dell FTOS support","text":"<p>Thanks to @log1cb0mb contribution, containerlab now knows how to start Dell FTOS systems.</p>"},{"location":"rn/0.18/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Better support for Mikrotik ROS system by @nlgotz</li> <li>SR Linux nodes will have their <code>/etc/hosts</code> file populated with name-IP pair of the other nodes of the lab</li> <li>Fixed panic for labs which used import of env vars #609</li> <li><code>save</code> command executed on Juniper cRPD will save config to the startup file</li> <li>fixed <code>save</code> command executed in vr-vmx nodes.</li> <li><code>srl</code> nodes additional config is now pushed once the nodes are started, instead of templating the config beforehand. This allows to follow the factory config of the release, without keeping the outdated template as it was before.</li> </ul>"},{"location":"rn/0.18/#new-contributors","title":"New contributors","text":"<p>Welcome @nlgotz, @log1cb0mb and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.19/","title":"Release 0.19","text":"<p> 2021-10-03</p>"},{"location":"rn/0.19/#containerlab-container","title":"Containerlab container","text":"<p>In this release our build pipeline finally started to produce container images with containerlab inside.</p> <p>This allows you to use containerlab on systems that have container runtime installed without requiring any installation whatsoever.</p> <p>Yes, a container with containerlab inside, so that you can launch containers from a container.</p>"},{"location":"rn/0.19/#experimental-vqfx-support","title":"Experimental vQFX support","text":"<p>With the help of @chriscummings-esnet we added experimental support for Juniper vQFX images built with vrnetlab. It may be rough around the edges, but should be a good start for future enhancements and improvements.</p>"},{"location":"rn/0.19/#mysocket-support-for-http-proxies","title":"Mysocket support for HTTP proxies","text":"<p>Our famous mysocketio integration that allows you to share labs effortlessly and secure has been enhanced with proxy support.</p> <p>With HTTP proxy support it is now possible to share lab access in the environments that have external SSH access blocked.</p>"},{"location":"rn/0.19/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>SR Linux variants have been expanded with support for IXR-H2 and IXR-H3 chassis.</li> <li>The 5 stage Clos lab example has been extended with a full blown configuration of the fabric using gnmic as a vehicle to push configs to all the nodes of this lab.</li> </ul>"},{"location":"rn/0.19/#new-contributors","title":"New contributors","text":"<p>Welcome @chriscummings-esnet, @sacckth, @siva19susi, @marcosfsch and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.19/#patches","title":"Patches","text":""},{"location":"rn/0.19/#0191","title":"0.19.1","text":"<ul> <li>fixed ovs-bridge kind name</li> <li>increased readiness timeout for SR Linux nodes to 120s to allow for slow boot on busy VMs</li> <li>increased external API timeouts (towards docker API) to 120s</li> <li>increased SSH allocated terminal width for config engine to deal with long cfg lines on SR Linux</li> <li>changed default license path on cRPD to make license to apply on boot</li> </ul>"},{"location":"rn/0.19/#0192","title":"0.19.2","text":"<ul> <li>fixed <code>tools veth create</code> command #667</li> <li>fixed <code>save</code> command for <code>vr-csr</code> nodes</li> <li>added config engine example</li> </ul>"},{"location":"rn/0.20/","title":"Release 0.20","text":"<p> 2021-11-17</p>"},{"location":"rn/0.20/#templated-topologies","title":"Templated topologies","text":"<p>In #658 @karimra added support for templated topologies. This feature allows users to generate topology files based on the variables defined in a file. With templates, users can create a template once and then deploy different flavors of the topologies by using variables files, which are naturally smaller and way more human-readable.</p> <p>Check the lab examples for addition details.</p>"},{"location":"rn/0.20/#limiting-cpu-and-memory-resources-for-nodes","title":"Limiting CPU and Memory resources for nodes","text":"<p>In #679 @karimra adds resource limitation capabilities for the nodes. It is possible to limit the amount of available CPU, Memory and allocate specific cores to the nodes.</p>"},{"location":"rn/0.20/#containerlab-in-the-media","title":"Containerlab in the media","text":"<p>Containerlab has recently been vocal in the network engineering community by participating in various podcasts and conferences. Don't worry if you missed some, now we have every notable material aggregated In The Media section.</p> <p>For example, here is our NANOG83 talk:</p>"},{"location":"rn/0.20/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Ignite runtime support for containerized clab execution (#670)</li> <li>Containerlab' config engine will not error on the nodes for which credentials were not found (#675)</li> <li>SONiC boot will not block in case of failures (#678)</li> </ul>"},{"location":"rn/0.20/#patches","title":"Patches","text":""},{"location":"rn/0.20/#0201","title":"0.20.1","text":"<ul> <li>Fix panic when copy files (#684)</li> </ul>"},{"location":"rn/0.20/#new-contributors","title":"New contributors","text":"<p>Welcome @LimeHat, and thank you for your contributions to containerlab!</p>"},{"location":"rn/0.21/","title":"Release 0.21","text":"<p> 2021-12-02</p>"},{"location":"rn/0.21/#sr-linux-startup-config-in-cli-format","title":"SR Linux startup-config in CLI format","text":"<p>SR Linux nodes have had support for startup configuration for a very long time. The nuance was that users had to dump the whole running config in JSON format to use it as a startup.</p> <p>While this works, it is not the most straightforward way. Usually, users create some use case by dabbing commands over the CLI, and it would be great to offer them a chance to apply that extra config on top of the factory config when SR Linux node boots.</p> <p>And that is exactly what has been added in v0.21.0! Please meet startup-config in CLI format.</p> <p>Now, if you have a file with CLI commands, you can apply this short snippet to the factory config making the use case ready when SR Linux NOS starts.</p> CLI ConfigTopology <pre><code># contents of myconfig.cli\nset / network-instance default protocols bgp admin-state enable\nset / network-instance default protocols bgp router-id 10.10.10.1\nset / network-instance default protocols bgp autonomous-system 65001\nset / network-instance default protocols bgp group ibgp ipv4-unicast admin-state enable\nset / network-instance default protocols bgp group ibgp export-policy export-lo\nset / network-instance default protocols bgp neighbor 192.168.1.2 admin-state enable\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-group ibgp\nset / network-instance default protocols bgp neighbor 192.168.1.2 peer-as 65001\n</code></pre> <pre><code>name: srl_lab\ntopology:\nnodes:\nsrl1:\nkind: srl\ntype: ixrd3\nimage: ghcr.io/nokia/srlinux\n# a path to the partial config in CLI format relative to the current working directory\nstartup-config: myconfig.cli\n</code></pre> <p>With that addition, you can ship your use cases without carrying the full SR Linux config files, just by checking in the small CLI-styled snippets.</p>"},{"location":"rn/0.21/#auto-enabled-sr-linux-interfaces","title":"Auto enabled SR Linux interfaces","text":"<p>Thanks to @jbemmel for suggesting enabling SR Linux interfaces referenced in the <code>links</code> section of a topology file.</p> <p>The interfaces will come up with Admin Up status when SR Linux finishes booting.</p>"},{"location":"rn/0.21/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Arista cEOS nodes will keep the same System MAC upon re-deployments. This is done to persist the same System MAC for systems like CVP.</li> <li>Fixes to containerlab JSON schema</li> <li>hellt/vrnetlab 0.6.2 added with support for Mikrotik RouterOS v7</li> </ul>"},{"location":"rn/0.22/","title":"Release 0.22","text":"<p> 2021-12-22</p>"},{"location":"rn/0.22/#sr-linux-password-less-login","title":"SR Linux password-less login","text":"<p>Containerlab now generates yet another file that will be stored in a lab directory - <code>authorized_keys</code>. This file will catenate all public keys found in <code>~/.ssh</code> directory.</p> <p>SR Linux nodes will mount this file for <code>admin</code>, <code>linuxadmin</code>, and <code>root</code> users; this will allow for password-less SSH access </p>"},{"location":"rn/0.22/#containerlab-schema","title":"Containerlab schema","text":"<p>To help users navigate in the sheer sea of configuration options containerlab has, we cleaned up the JSON schema and made a web doc generated from it so that you can see what you can configure and where.</p>"},{"location":"rn/0.22/#reworked-prefix-logic","title":"Reworked prefix logic","text":"<p>In 0.16, we added a new top-level field - <code>prefix</code> - to let users decide if they want to have containers prefixed with a string other than <code>clab</code>. Now we got a few requests to make containers ditch the prefixes altogether, such as if you named a node <code>mynode</code> it will be created as a <code>mynode</code> container.</p> <p>Your wish came true. Now, if you have an empty string <code>prefix</code>, the container name will be stripped of everything but name. The magic <code>__lab-name</code> prefix value will add lab name to the container name; leaving the prefix out in the topo file will set the container name to <code>clab-&lt;lab-name&gt;-&lt;node-name&gt;</code>. Read more here.</p>"},{"location":"rn/0.22/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>new SR Linux variants <code>ixrd2l</code> and <code>ixrd3l</code> have been added in #726</li> <li>containerlab assigned SR Linux MACs will have <code>1a:b0:</code> prefix and the node index will be the rightmost byte. This makes it easier to identify the macs in the outputs. #713</li> <li>our beloved users created some awesome blogs and streams about containerlab! We featured them on our community page.</li> <li>fixed doubled dot in the container's fqdn #724</li> <li><code>tools cert</code> command gained more checks #725</li> </ul>"},{"location":"rn/0.23/","title":"Release 0.23","text":"<p> 2022-01-25</p>"},{"location":"rn/0.23/#podman","title":"Podman \u03b2","text":"<p>Containerlab' ties to docker are quite substantial. Basically, containerlab assumes that users have Docker runtime installed to launch containerized labs.</p> <p>But docker is not the only runtime that can be used to launch containers. Containerd, Podman and a few other alternative high level runtimes have been available for quite a while. In containerlab 0.23.0 we add beta support for Podman runtime, thanks to @LimeHat contribution!</p> <p>Why podman? Well, a few reasons:</p> <ol> <li>Podman offers the most similar experience to Docker while not using docker at all; one can make an alias <code>podman-&gt;docker</code> and use the same docker-cli commands with the podman runtime.</li> <li>It is easier to install and sometimes comes as a default choice on redhat based distributions.</li> <li>Supporting an alternative runtime can make containerlab usable on systems which can't have docker installed for various reasons.</li> <li>And of course, it is fun to add a runtime which is not docker :D</li> </ol> <p>All in all, we encourage you to test podman runtime if it is of interest to you. Note that not every containerlab feature is supported with podman yet, but the basics are all there.</p> <p>When you install podman, make sure to enable the podman service for the API to work:</p> <pre><code>systemctl enable podman\nsystemctl start podman\n</code></pre> <p>and then you can use containerlab as per usual with just a flag enabling podman runtime:</p> <pre><code>containerlab --runtime podman deploy -t mytopo.clab.yml\n</code></pre>"},{"location":"rn/0.23/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>now whenever users decide to select an existing bridge as a backend for docker network, containerlab will check if the bridge has been already addressed and use the gateway information based on that. Read more about this new feature here.</li> <li>new community posts.</li> </ul>"},{"location":"rn/0.24/","title":"Release 0.24","text":"<p> 2022-02-22</p>"},{"location":"rn/0.24/#enabling-external-lab-access","title":"Enabling external lab access","text":"<p>As more users started playing with containerlab, more deployment use cases began to surface. One of the most common non-trivial deployment use cases was enabling external access from systems deployed outside of containerlab with the topology nodes.</p> <p>This use case was not that apparent because of the way Docker secures the host. By default, Docker doesn't allow external packets to reach containers, while containers can initiate connections to outside hosts. These security measures are carried out via iptables rules Docker maintains.</p> <p>In this release, containerlab will automatically create an allowing rule in the <code>DOCKER-USER</code> chain to allow external systems to reach containerlab nodes. Read more about this feature in our docs.</p>"},{"location":"rn/0.24/#docker-authentication","title":"Docker authentication","text":"<p>Containerlab pulls the images at deploy stage, if the images are not present. To support pulling the images from the repos which require authentication @lbaker-esnet in #755 added the logic to fetch the authentication data from local docker config store.</p> <p>Now if you have logged in to a certain repo with <code>docker login</code>, containerlab will be able to pull the images from this repo, using the credentials stored locally.</p>"},{"location":"rn/0.24/#binds-merge","title":"Binds merge","text":"<p>One of the most used containerlab options -- binds -- got even better. Now the bind paths will get merged should you define them on the defaults, kinds and nodes levels.</p> <p>This makes it possible to define some default binds and have node-specific binds to be added to them.</p>"},{"location":"rn/0.24/#restart-on-failure-for-linux-nodes","title":"Restart on failure for linux nodes","text":"<p>Now containerlab will automatically restart the nodes of <code>linux</code> kind if their main process exited with a non-0 return code. This will ensure longevity of the services such as Telemetry stacks or management software that may crash for various reasons.</p>"},{"location":"rn/0.24/#shared-network-namespaces","title":"Shared network namespaces","text":"<p>Our own @LimeHat added support for making containerlab nodes to join in Pods formation. That is when multiple nodes share the same network namespace.</p> <p>This is done with a new network-mode property value.</p>"},{"location":"rn/0.24/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>SR Linux breakout support has been fixed in #765</li> <li>It is now possible to create ansible inventory without auto-populated <code>ansible_host</code> variables. Thanks to @tobbbles for his work in #793</li> <li>A new flag has been added to deploy command to skip the post deploy actions. This may optimize the boot time if you have static configs provided. Thanks to @bjmeuer for adding this in #773</li> <li>podman \u03b2-support that we announced in <code>0.23.0</code> has fallen victim to a misconfigured build pipeline. Hopefully, in this release, you will be able to use it :D</li> <li>Linux nodes can now use ignite runtime #759</li> <li>SR Linux nodes now not only will get the authz keys from pub keys available at <code>~/.ssh</code> dir, but will also get everything from <code>~/.ssh/authorized_keys</code> file as well. Thanks to @hansthienpondt for adding this in #778</li> <li>destroy command will remove the lab dir even if no containers for that lab was found #753</li> <li>new community posts.</li> </ul>"},{"location":"rn/0.24/#patches","title":"Patches","text":""},{"location":"rn/0.24/#0241","title":"0.24.1","text":"<ul> <li>Do not stop containerlab deploy/destroy in case of a missing iptables DOCKER-USER chain. This is typical to docker installations older than 17.06 version. #797</li> <li>Fix directory removal flow during cleanup #799</li> </ul>"},{"location":"rn/0.25/","title":"Release 0.25","text":"<p> 2022-03-20</p>"},{"location":"rn/0.25/#graph-re-imagined","title":"Graph re-imagined","text":"<p>Since containerlab's inception, the focus has been on text files representing topology. While defining topologies in text format is a nice, git-friendly way of defining labs, we can't neglect the fact that having a nice visualisation of a topology is a compelling feature to have.</p> <p>With this release, we're adding a re-vamped graph engine based on the NeXt UI framework.</p> <p></p> <p>The new graph can be easily generated with the retrofitted <code>graph</code> command:</p> <pre><code>containerlab graph -t &lt;topology-file&gt;\n</code></pre>"},{"location":"rn/0.25/#ceos-interface-mapping-file","title":"cEOS interface mapping file","text":"<p>Starting with Arista cEOS 4.28 it is now possible to provide an interface mapping file that specifies which container interfaces are mapped to which ceos interfaces. This adds flexibility in interface mappings. #787</p>"},{"location":"rn/0.25/#ipinfusion-ocnos","title":"IPInfusion OcNOS","text":"<p>A new VM-based network OS has been added - IPInfusion OcNOS. An interesting fact about this image - it was built with boxen project.</p>"},{"location":"rn/0.25/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>numerous improvements to the resolve-path logic</li> <li>container's ulimits will be set to the host's values #825</li> <li>duplication of statically assigned management addresses will be detected</li> <li>added <code>ceos-copy-to-flash</code> extra node property to allow copying files to flash dir #819</li> <li>containerlab's topology backup file will now be created in the same dir as the original topo file. Before, it was created in the current working dir</li> <li>containerlab will stop if less than 2 vcpu has been detected and srlinux nodes were defined in a topology</li> <li>topology verification step has been enhanced to check if srlinux interface names comply with the expected structure of e1-1 or e1-1-1 pattern</li> <li>containerlab site moved to containerlab.dev domain</li> <li>new community posts</li> </ul>"},{"location":"rn/0.25/#patches","title":"Patches","text":""},{"location":"rn/0.25/#0251","title":"0.25.1","text":"<ul> <li>graph is now served on <code>0.0.0.0:50080</code> address instead of localhost. This enables reaching graphs on headless servers without any port forwarding.</li> </ul>"},{"location":"rn/0.26/","title":"Release 0.26","text":"<p> 2022-05-03</p>"},{"location":"rn/0.26/#nokia-sr-linux-banner","title":"Nokia SR Linux banner","text":"<p>Little things with big impact - this is what SR Linux banner addition is. Starting with 0.26 release, containerlab will add a banner message to all SR Linux nodes with all necessary pointers such as:</p> <ul> <li>docs site</li> <li>release notes PDF</li> <li>YANG browser URL</li> <li>repository where we keep container images</li> <li>links to our cozy discord server and coordinates of ISS our sales representatives</li> </ul> <pre><code>\u276f ssh admin@clab-srl-srl\nWarning: Permanently added 'clab-srl-srl,2001:172:20:20::f' (ECDSA) to the list of known hosts.\n................................................................\n:                  Welcome to Nokia SR Linux!                  :\n:              Open Network OS for the NetOps era.             :\n:                                                              :\n:    This is a freely distributed official container image.    :\n:                      Use it - Share it                       :\n:                                                              :\n: Get started: https://learn.srlinux.dev                       :\n: Container:   https://go.srlinux.dev/container-image          :\n: Docs:        https://doc.srlinux.dev/21-11                   :\n: Rel. notes:  https://doc.srlinux.dev/rn21-11-2               :\n: YANG:        https://yang.srlinux.dev/v21.11.2               :\n: Discord:     https://go.srlinux.dev/discord                  :\n: Contact:     https://go.srlinux.dev/contact-sales            :\n................................................................\n\nUsing configuration file(s): []\nWelcome to the srlinux CLI.\nType 'help' (and press &lt;ENTER&gt;) if you need any help using this.\n--{ running }--[  ]--                                               \n</code></pre> <p>Now everything you need to get started is just one click away.</p>"},{"location":"rn/0.26/#topology-export","title":"Topology export","text":"<p>Our partner in crime @bortok added a nice feature titled \"topology data auto export\" for his graphite project in #850. What now happens is that on each lab deployment you will find a file called <code>topology-data.json</code> inside the lab directory that contains most important topology information that can be consumed by external tools.</p> <p>Read more about what data is dumped here.</p> <p>In addition to the auto-export functionality, users are now able to provide their own Go template file describing the desired output format for topology export. This is possible with the newly added <code>export-template</code> flag.</p> <p>And even that is not all :D To be able to easily refer to the files located in a lab directory<sup>1</sup> a new magic variable has been added - <code>__clabDir__</code>.</p>"},{"location":"rn/0.26/#keysight-ixia-c","title":"Keysight IXIA-C","text":"<p>Keysight folks added support for their open source traffic generator appliance - ixia-c - to containerlab. Meet <code>keysight_ixia-c-one</code> kind!</p> <p>This addition allows you to write and run traffic-based tests using open-source production-grade traffic generators all inside the container runtime setting. There is a lot to be said about this integration, and we plan to record a few videos with Keysight; for now you can hack it on your own.</p>"},{"location":"rn/0.26/#env-files","title":"Env files","text":"<p>Our own @steiler is back and added support (#847) for env files that you can use with <code>env-files</code> node container.</p>"},{"location":"rn/0.26/#labels-as-env-vars","title":"Labels as env vars","text":"<p>Now every label that you (or containerlab) defined for a node gets promoted to an env var. With this it is possible to let scripts running inside the container access to metadata information we attach via clab files #841.</p>"},{"location":"rn/0.26/#sysctls","title":"Sysctls","text":"<p>And if you wanted to configure sysctls for a node - it is now possible with <code>sysctls</code> container #856.</p>"},{"location":"rn/0.26/#bridge-forwarding","title":"Bridge forwarding","text":"<p>When clabbers used linux bridges in their topologies it was common to see traffic to be dropped when entering a bridge. This was in line with the default iptables rules which prevent forwarding over the bridge by default.</p> <p>Now containerlab will add the relevant iptables rule to allow forwarding over the bridges #871.</p>"},{"location":"rn/0.26/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>next-ui-based graph works better with long image names in tooltips #834</li> <li>added json-rpc example for nokia sr linux</li> <li>better runtime mtu detection #840</li> <li>Arista cEOS nodes now will wait for all veth interfaces to first be present inside the container, before starting the main startup scripts #854</li> <li>added PauseContainer runtime method #843</li> <li>containerlab will not attempt to deploy labs with sr linux nodes if SSE3 cpu instruction set is not found #846</li> <li>better cleanup procedure for veth between container and host namespaces #862</li> <li>new community posts</li> </ul>"},{"location":"rn/0.26/#patches","title":"Patches","text":""},{"location":"rn/0.26/#0261","title":"0.26.1","text":"<ul> <li>scrapligo updated to mitigate long ceos boot times #879</li> <li>better handling of file paths #877</li> <li>support for ceos <code>et</code> interfaces in wait script #881</li> </ul>"},{"location":"rn/0.26/#0262","title":"0.26.2","text":"<ul> <li>fix concurrent calls to iptables #884</li> </ul> <ol> <li> <p>that is the one that is named as <code>clab-&lt;lab-name&gt;</code>.\u00a0\u21a9</p> </li> </ol>"},{"location":"rn/0.27/","title":"Release 0.27","text":"<p> 2022-07-06</p>"},{"location":"rn/0.27/#multiple-kind-names","title":"Multiple kind names","text":"<p>When we started containerlab, using short kind names was easy. Everybody knew that <code>srl</code> is SR Linux, and, say, <code>ceos</code> is cEOS. As we grew, more people started to use containerlab and not everyone knew the short names of NOSes.</p> <p>Therefore we added an ability to use long names for kinds, for example, <code>nokia_srlinux</code> or <code>arista_ceos</code>. These new \"long\" names have been added to kind docs.</p>"},{"location":"rn/0.27/#juniper-vqfx-and-juniper-vmx-startup-config","title":"Juniper vQFX and Juniper vMX startup config","text":"<p>We finally added support to provision vMX and vQFX with startup configuration. The same <code>startup-config</code> node property allows users to point to a file that contains configuration lines that will get applied to a network element when it starts.</p>"},{"location":"rn/0.27/#misc","title":"Misc","text":"<ul> <li>SR Linux nodes will be able to use the names of the other nodes of the same #891</li> </ul>"},{"location":"rn/0.27/#patches","title":"Patches","text":""},{"location":"rn/0.27/#0271","title":"0.27.1","text":"<ul> <li>adding missing config dir mount for vMX</li> </ul>"},{"location":"rn/0.28/","title":"Release 0.28","text":"<p> 2022-06-27</p>"},{"location":"rn/0.28/#topology-file-auto-detect","title":"Topology file auto-detect","text":"<p>With embedded shorthands <code>containerlab</code>-&gt;<code>clab</code>, <code>deploy</code>-&gt;<code>dep</code> and so on, we made using containerlab over the CLI a fast and pleasant experience. But even then, typing <code>clab dep -t mytopo.clab.yml</code> is one argument too long.</p> <p>Please welcome the topology file auto-detect feature which turns <code>clab dep -t mytopo.clab.yml</code> to just <code>clab dep</code> </p> <p>Note</p> <p>A single file matching <code>*.clab.y*ml</code> pattern must be present in the current working directory for auto-detect feature to work.</p> <p>Should you have multiple topology files, use the <code>--topo</code> flag as before.</p>"},{"location":"rn/0.28/#ignite-based-linux-containers","title":"Ignite-based linux containers","text":"<p>Weaveworks/ignite runtime integration that was added by @networkop for Cumulus VX nodes got a new application. With #910 merged, users now can run Linux VMs in a container packaging and leverage VM-like experience while running lightweight and fast.</p>"},{"location":"rn/0.28/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>added <code>-c</code> shorthand for <code>--reconfigure</code> as it is very handy to use <code>clab dep -c</code> to cleanly redeploy the lab during the iterations of lab build-out.</li> </ul>"},{"location":"rn/0.28/#patches","title":"Patches","text":""},{"location":"rn/0.28/#0281","title":"0.28.1","text":"<ul> <li>fixed crpd kind name #922</li> <li>added missing kinds to clab.schema.json</li> <li>ceos nodes now have default route pointing to management gw address #920</li> <li>iptables has been added to clab container #921</li> <li>diagrams rendering engine update to latest version</li> </ul>"},{"location":"rn/0.29/","title":"Release 0.29","text":"<p> 2022-07-11</p>"},{"location":"rn/0.29/#checkpoint-cloudguard","title":"Checkpoint Cloudguard","text":"<p>Our firewall-focused camp got a new member - Checkpoint Cloudguard platform. This platform is built with boxen project instead of vrnetlab. We are slowly building confidence in boxen and new platforms will likely be solely powered by boxen. PR #934</p>"},{"location":"rn/0.29/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>added <code>-c</code> shorthand for <code>--cleanup</code> flag of the <code>destroy</code> command. Now to remove a lab and get rid of the lab directory simply call <code>clab des -c</code>.</li> <li><code>inspect</code> command has been fixed to display IP information even for nodes connected to external networks.</li> <li>added srlinux 6e and 10e platforms</li> </ul>"},{"location":"rn/0.30/","title":"Release 0.30","text":"<p> 2022-07-28</p>"},{"location":"rn/0.30/#podman-v4","title":"Podman v4","text":"<p>Podman v4 delivers a new API to schedule containers within the Podman runtime. Version 4 allows us to deliver better support for this runtime in the context of containerlab #919.</p>"},{"location":"rn/0.30/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>ipv6 gateway support in arista ceos startup-config #940.</li> <li>management vrf support in ceos with the use of a <code>CLAB_MGMT_VRF</code> env var #941</li> <li>SR Linux mac address generation function allows for multi-node setups with random seed #942</li> <li>fixed Netconf save operations #953</li> </ul>"},{"location":"rn/0.31/","title":"Release 0.31","text":"<p> 2022-08-11</p>"},{"location":"rn/0.31/#external-containers","title":"External containers","text":"<p>As people of cloud started to use containerlab to test features on the intersection of networking and compute domains, it became evident that containerlab lacked an option to schedule a container in the network namespace of an external container. For example, the need was to start a containerlab lab and stitch it nicely to the k8s kind cluster nodes which run some CNIs that are willing to talk to the network nodes launched in containerlab.</p> <p>With #969 being merged we are adding this capability and making it easy to bridge emulated network nodes with k8s nodes running with kind.</p>"},{"location":"rn/0.31/#san-support-for-sr-linux-certificates","title":"SAN support for SR Linux certificates","text":"<p>With <code>SANs</code> node property users can set subject alternative names for certificates that get generated by containerlab. Currently, this is only implemented for SR Linux #968.</p>"},{"location":"rn/0.31/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>startup config template files can leverage gomplate functions #955</li> <li>Logs are now printed to <code>stderr</code>. This is mainly done to make sure that when we print any json, you can parse it with <code>jq</code> without logging info getting in the way #958</li> <li>containerlab version check can now be skipped if CLAB_VERSION_CHECK env var is set to <code>disable</code> #959</li> <li>mysocketio information will now be printed in json output #887</li> <li>gnmi unix socket is now enabled by default for SR Linux nodes #965</li> </ul>"},{"location":"rn/0.31/#patches","title":"Patches","text":""},{"location":"rn/0.31/#0311","title":"0.31.1","text":"<ul> <li>added support for saving configuration for numerous vrnetlab-based nodes #973</li> <li>updated lab examples to be sourced from the main branch instead of master #978</li> <li>enhancements to config templates loader #974</li> </ul>"},{"location":"rn/0.32/","title":"Release 0.32","text":"<p> 2022-09-22</p>"},{"location":"rn/0.32/#dependency-manager","title":"Dependency Manager","text":"<p>Our own @steiler has volunteered to tackle a problem of inter-node dependency management and produced three mighty PRs #1013, #1022, #1026. Dependency Manager (or DM for short) gives containerlab superpowers to decide the scheduling order of the lab nodes.</p> <p>Scheduling the order of the nodes is important. Nodes with dynamic management IPs should start after the nodes with static IPs, nodes with shared network namespace should start after the donor node is ready, and so on...</p> <p>These implicit inter-node dependencies are handled by containerlab in the background, so you don't have to worry. Sometimes, though, you may want to influence the order of nodes scheduling yourself. For example, you may want to start a telemetry collector after the network nodes are running. For that reason, a new node property <code>wait-for</code> has been introduced.</p> <p>In <code>wait-for</code> section, you can specify node names that this node will wait for before being allowed to get scheduled.</p>"},{"location":"rn/0.32/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>iptables interactions with external bridges have been fixed #982</li> <li>dependabot integration will keep watch on the deps used in containerlab and update to latest once available #984</li> <li>crpd startup config handling has been fixed in a scenario when some nodes have used startup config and some don't</li> </ul>"},{"location":"rn/0.32/#patches","title":"Patches","text":""},{"location":"rn/0.32/#0321","title":"0.32.1","text":"<ul> <li>fixed SR Linux IXR-D5 topology specification</li> </ul>"},{"location":"rn/0.32/#0322","title":"0.32.2","text":"<ul> <li>support for multi line card SR OS deployments</li> <li>fix for cvx nodes sequential boot #1037</li> <li>return of the ignite runtime for cvx kind #1063</li> <li>added <code>auto-remove</code> option for nodes #1056</li> </ul>"},{"location":"rn/0.32/#0323","title":"0.32.3","text":"<ul> <li>fix default bridge misuse #1071</li> <li>improved auto-remove setting propagation #1065</li> <li>added docs for SR OS multi line card configuration</li> </ul>"},{"location":"rn/0.32/#0324","title":"0.32.4","text":"<ul> <li>cleanup veth pairs connected to bridges #1090</li> <li>fix SSSE3 instruction check #1083</li> <li>change sr linux readiness check to use a file-based approach #1084</li> <li>display errors which may occur during sr linux startup config apply #1089</li> </ul>"},{"location":"rn/0.33/","title":"Release 0.33","text":"<p> 2022-11-28</p>"},{"location":"rn/0.33/#external-container-kind","title":"External container kind","text":"<p>With a new <code>ext-container</code> kind it is possible to enrich clab topology with nodes scheduled by other tools (e.g. <code>k8s-kind</code>). Thanks @steiler.</p>"},{"location":"rn/0.33/#graph-enhancements","title":"Graph enhancements","text":"<p>Thanks to @gamerslouis our graph feature got some important fixes like properly displaying bridge properties in #1097 and renders bridges in offline mode in #1098.</p>"},{"location":"rn/0.33/#sr-os-and-bof-persistency","title":"SR OS and BOF persistency","text":"<p>Making SR OS BOF persistent was not a trivial task, but @mabra94 added a nice doc section explaining how bind mounts and boot-good-exec SR OS option combined can achieve that. #1107</p>"},{"location":"rn/0.33/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>fixed ssse3 check #1092</li> <li>housekeeping item by @steiler - default node embedding #1099</li> <li>startup-config support for PAN OS nodes #1109</li> <li>save CPU from spinning when waiting for nodes to boot #1108</li> </ul>"},{"location":"rn/0.34/","title":"Release 0.34","text":"<p> 2022-12-23</p>"},{"location":"rn/0.34/#cisco-xrd-support","title":"Cisco XRd support","text":"<p>Thanks to @trustywolf we finally landed support for Cisco XRd! No more dealing with a 16GB mem-hungry VM monster when all you need is a control plane. #1144</p>"},{"location":"rn/0.34/#major-codebase-refactoring","title":"Major codebase refactoring","text":"<p>@steiler went into the berzerk mode and refactored half of the containerlab's internal code base to have a cleaner separation of packages, internal APIs and increased extensibility.</p> <p>This change was carried over in multiple PRs and touched a lot of files; while we did quite some testing and maintained the same user experience, there might be things that work differently, do let us know if there is something out of order.</p>"},{"location":"rn/0.34/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Containerlab can now be installed on Core OS #1115</li> <li>Fixed pull image function to support pulling images without explicit tags #1123</li> <li>Memory calculation function has been fixed to report on available memory, not just free one #1133</li> </ul>"},{"location":"rn/0.35/","title":"Release 0.35","text":"<p> 2023-01-17</p>"},{"location":"rn/0.35/#dns-options","title":"DNS options","text":"<p>A new node property - DNS - allows users to provide DNS options to the nodes.</p>"},{"location":"rn/0.35/#containerlab-exec-fixes-and-improvements","title":"<code>containerlab exec</code> fixes and improvements","text":"<p>Thanks to efforts by @steiler we have refactored <code>exec</code> command and fixed a few bugs along the way. The command now supports multiple <code>--cmd</code> arguments. #1161</p>"},{"location":"rn/0.35/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Internal refactoring of the node registry #1156</li> <li>fixed ovs-bridge init #1172</li> <li>interface name checks enabled for all VM-based nodes #1191</li> </ul>"},{"location":"rn/0.35/#patches","title":"Patches","text":""},{"location":"rn/0.35/#0351","title":"0.35.1","text":"<ul> <li>fixed the regexp used in the interface name check function #1201</li> </ul>"},{"location":"rn/0.35/#0352","title":"0.35.2","text":"<ul> <li>removed interface name checks for linux kinds.</li> <li>added a note on <code>et_X_Y</code> nameing of ceos.</li> <li>fixed <code>containerlab config</code> command for SR OS nodes.</li> </ul>"},{"location":"rn/0.36/","title":"Release 0.36","text":"<p> 2023-01-27 \u00b7  Full Changelog</p>"},{"location":"rn/0.36/#image-pull-policy","title":"Image Pull Policy","text":"<p>Now we give you control over the image pulling behavior by introducing <code>image-pull-policy</code> node property. You want your image to be always fetched from the remote registry? Now it is possible. #1223</p>"},{"location":"rn/0.36/#cisco-c8000","title":"Cisco c8000","text":"<p>A new kind - <code>cisco_8000</code> - has been added to the list of supported platforms thanks to an awesome contribution from @rskorka in #1216.</p>"},{"location":"rn/0.36/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>Fixed Cumulus interface name checks #1224</li> <li>Housekeeping items in #1208 #1209 and #1225 by @steiler</li> <li>Fixed credentials extraction for DockerHub images</li> </ul>"},{"location":"rn/0.36/#patches","title":"Patches","text":""},{"location":"rn/0.36/#0361","title":"0.36.1","text":"<ul> <li>fixed certificate creation for SR Linux nodes identified with <code>nokia_srlinux</code> kind.</li> </ul>"},{"location":"rn/0.37/","title":"Release 0.37","text":"<p> 2023-02-21 \u00b7  Full Changelog</p>"},{"location":"rn/0.37/#certificates-refactoring","title":"Certificates refactoring","text":"<p>Breaking change!</p> <p>Refactoring of the certificates handling in containerlab done in #1219 introduces a few breaking changes.</p> <p>First, the certificates are now stored in the <code>&lt;lab-dir&gt;/.tls</code> directory<sup>1</sup>.</p> <p>CA certificate and key are now placed under the <code>&lt;lab-dir&gt;/.tls/ca</code> directory<sup>2</sup>.</p> <p>Certificates are now generated for every node in the lab, but this may be configurable in the future.</p> <p>After upgrading to v0.37 make sure to deploy the labs using the <code>-c</code> flag to remove old lab directory and generate new certificates.</p>"},{"location":"rn/0.37/#custom-location-for-containerlab-lab-base-directory","title":"Custom location for containerlab lab base directory","text":"<p>By default, containerlab creates a lab directory (that is the one named <code>clab-xxx</code>) in the current working directory. This is not always convenient, especially when you want to run containerlab from a different directory than the one where you want to store the lab files. To address this issue, we have added a new environment variable <code>CLAB_LAB_DIR_BASE</code> that allows you to specify the location of the lab directory. #1248 #1250</p>"},{"location":"rn/0.37/#docker-v23-compatibility","title":"Docker v23 compatibility","text":"<p>In #1257 we have added support for Docker v23. This is a major release of Docker that brings some changes to the API specifically. We have tested containerlab with Docker v23 and it seems to work fine. However, we have not tested all the features of containerlab with Docker v23, so if you find any issues, please let us know.</p>"},{"location":"rn/0.37/#miscellaneous","title":"Miscellaneous","text":"<ul> <li>SR Linux gNMI/gNOI server rate-limit increased to <code>65000</code> #1239</li> <li>Big refactoring of the path handling #1238</li> </ul>"},{"location":"rn/0.37/#patches","title":"Patches","text":""},{"location":"rn/0.37/#0371","title":"0.37.1","text":"<ul> <li>removed compression with <code>upx</code> as it caused selinux violations on Rocky Linux v9+ #1264</li> </ul> <ol> <li> <p>Previous path was <code>&lt;lab-dir&gt;/ca</code> \u21a9</p> </li> <li> <p>Previous path was <code>&lt;lab-dir&gt;/ca/root</code> \u21a9</p> </li> </ol>"}]}